,id,category,paper,text
0,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-1,deeplearning,attention_is_all_you_need.pdf,"We used a beam size of 21and= 0:3
for both WSJ only and the semi-supervised setting."
1,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-2,deeplearning,attention_is_all_you_need.pdf,"The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.
For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based
on recurrent or convolutional layers."
2,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-3,deeplearning,attention_is_all_you_need.pdf,"[4]Jianpeng Cheng, Li Dong, and Mirella Lapata."
3,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-4,deeplearning,attention_is_all_you_need.pdf,"Attention Is All You Need
Ashish Vaswani
Google Brain
avaswani@google.comNoam Shazeer
Google Brain
noam@google.comNiki Parmar
Google Research
nikip@google.comJakob Uszkoreit
Google Research
usz@google.com
Llion Jones
Google Research
llion@google.comAidan N. Gomezy
University of Toronto
aidan@cs.toronto.eduŁukasz Kaiser
Google Brain
lukaszkaiser@google.com
Illia Polosukhinz
illia.polosukhin@gmail.com
Abstract
The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder.
3 Model Architecture
Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].
3.4 Embeddings and Softmax
Similarly to other sequence transduction models, we use learned embeddings to convert the input
tokens and output tokens to vectors of dimension dmodel."
4,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-5,deeplearning,attention_is_all_you_need.pdf,"At each step the model is auto-regressive
[10], consuming the previously generated symbols as additional input when generating the next.
The keys and values are also packed together into matrices KandV.
MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO
where head i= Attention( QWQ
i;KWK
i;VWV
i)
Where the projections are parameter matrices WQ
i2Rdmodeldk,WK
i2Rdmodeldk,WV
i2Rdmodeldv
andWO2Rhdvdmodel.
5 Training
This section describes the training regime for our models.
The conﬁguration of this model is
listed in the bottom line of Table 3.
These hyperparameters
were chosen after experimentation on the development set.
In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,
keeping the amount of computation constant, as described in Section 3.2.2.
During inference, we
increased the maximum output length to input length + 300."
5,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-6,deeplearning,attention_is_all_you_need.pdf,We present these results in Table 3.
6,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-7,deeplearning,attention_is_all_you_need.pdf,"Computational linguistics , 19(2):313–330, 1993.
In Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting of the ACL , pages 433–440.
Grammar as a foreign language."
7,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-8,deeplearning,attention_is_all_you_need.pdf,"13Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
14Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
."
8,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-9,deeplearning,attention_is_all_you_need.pdf,"Le.
of NAACL , 2016."
9,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-10,deeplearning,attention_is_all_you_need.pdf,"To this end, we add ""positional encodings"" to the input embeddings at the
bottoms of the encoder and decoder stacks.
The positional encodings have the same dimension dmodel
as the embeddings, so that the two can be summed.
In addition, we apply dropout to the sums of the embeddings and the
positional encodings in both the encoder and decoder stacks."
10,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-11,deeplearning,attention_is_all_you_need.pdf,"3.3 Position-wise Feed-Forward Networks
In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully
connected feed-forward network, which is applied to each position separately and identically."
11,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-12,deeplearning,attention_is_all_you_need.pdf,"3.2 Attention
An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors.
In practice, we compute the attention function on a set of queries simultaneously, packed together
into a matrix Q.
While for small values of dkthe two mechanisms perform similarly, additive attention outperforms
dot product attention without scaling for larger values of dk[3].
3.2.2 Multi-Head Attention
Instead of performing a single attention function with dmodel-dimensional keys, values and queries,
we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned
linear projections to dk,dkanddvdimensions, respectively.
On each of these projected versions of
queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional
output values.
We inspect attention distributions
from our models and present and discuss examples in the appendix.
(2016) [8] generative 93.3
In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality.
We are excited about the future of attention-based models and plan to apply them to other tasks.
Can active memory replace attention?
A decomposable attention
model."
12,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-13,deeplearning,attention_is_all_you_need.pdf,"ACL, August 2009.
ACL, June 2006.
ACL, July
2006.
ACL, August 2013."
13,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-14,deeplearning,attention_is_all_you_need.pdf,"[16] Łukasz Kaiser and Samy Bengio.
[17] Łukasz Kaiser and Ilya Sutskever.
[20] Diederik Kingma and Jimmy Ba.
[21] Oleksii Kuchaiev and Boris Ginsburg.
Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.
[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le."
14,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-15,deeplearning,attention_is_all_you_need.pdf,"For our base models using
the hyperparameters described throughout the paper, each training step took about 0.4 seconds.
We
trained the base models for a total of 100,000 steps or 12 hours.
For our big models,(described on the
bottom line of table 3), step time was 1.0 seconds.
The big models were trained for 300,000 steps
(3.5 days)."
15,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-16,deeplearning,attention_is_all_you_need.pdf,"We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the
Penn Treebank [ 25], about 40K training sentences.
We also trained it in a semi-supervised setting,
using the larger high-conﬁdence and BerkleyParser corpora from with approximately 17M sentences
[37].
In contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-
Parser [29] even when training only on the WSJ training set of 40K sentences.
Building a large annotated
corpus of english: The penn treebank.
Learning accurate, compact,
and interpretable tree annotation."
16,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-17,deeplearning,attention_is_all_you_need.pdf,"[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio.
[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith.
[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber.
[13] Sepp Hochreiter and Jürgen Schmidhuber."
17,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-18,deeplearning,attention_is_all_you_need.pdf,"We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely.
In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-
aligned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].
To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying
entirely on self-attention to compute representations of its input and output without using sequence-
aligned RNNs or convolution.
Additive attention computes the compatibility function using a feed-forward network with
a single hidden layer.
7 Conclusion
In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on
attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with
multi-headed self-attention.
We
plan to extend the Transformer to problems involving input and output modalities other than text and
to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs
such as images, audio and video.
End-to-end memory
networks."
18,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-19,deeplearning,attention_is_all_you_need.pdf,"<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
."
19,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-20,deeplearning,attention_is_all_you_need.pdf,"Aligning the positions to steps in computation time, they generate a sequence of hidden
statesht, as a function of the previous hidden state ht 1and the input for position t. This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples.
In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.
This
masking, combined with fact that the output embeddings are offset by one position, ensures that the
predictions for position ican depend only on the known outputs at positions less than i.
3.5 Positional Encoding
Since our model contains no recurrence and no convolution, in order for the model to make use of the
order of the sequence, we must inject some information about the relative or absolute position of the
5Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations
for different layer types.
There are many choices of positional encodings,
learned and ﬁxed [9]."
20,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-21,deeplearning,attention_is_all_you_need.pdf,"Gradient ﬂow in
recurrent nets: the difﬁculty of learning long-term dependencies, 2001.
Neural GPUs learn algorithms.
Factorization tricks for LSTM networks."
21,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-22,deeplearning,attention_is_all_you_need.pdf,"Neural machine translation by jointly
learning to align and translate.
Massive exploration of neural
machine translation architectures.
Neural machine translation in linear time.
Effective approaches to attention-
based neural machine translation.
Neural machine translation of rare words
with subword units."
22,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-23,deeplearning,attention_is_all_you_need.pdf,"Each layer has two
sub-layers.
That is, the output of each sub-layer is
LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer
itself.
FFN(x) = max(0;xW 1+b1)W2+b2 (2)
While the linear transformations are the same across different positions, they use different parameters
from layer to layer."
23,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-24,deeplearning,attention_is_all_you_need.pdf,"Recent work has achieved
signiﬁcant improvements in computational efﬁciency through factorization tricks [ 21] and conditional
computation [ 32], while also improving model performance in case of the latter.
ModelBLEU Training Cost (FLOPs)
EN-DE EN-FR EN-DE EN-FR
ByteNet [18] 23.75
Deep-Att + PosUnk [39] 39.2 1:01020
GNMT + RL [38] 24.6 39.92 2:310191:41020
ConvS2S [9] 25.16 40.46 9:610181:51020
MoE [32] 26.03 40.56 2:010191:21020
Deep-Att + PosUnk Ensemble [39] 40.4 8:01020
GNMT + RL Ensemble [38] 26.30 41.16 1:810201:11021
ConvS2S Ensemble [9] 26.36 41.29 7:710191:21021
Transformer (base model) 27.3 38.1 3:31018
Transformer (big) 28.4 41.8 2:31019
Label Smoothing During training, we employed label smoothing of value ls= 0:1[36].
Even our base model
surpasses all previously published models and ensembles, at a fraction of the training cost of any of
the competitive models.
(2013) [40] semi-supervised 91.3
Huang & Harper (2009) [14] semi-supervised 91.3
McClosky et al.
We further observe in rows (C) and (D) that, as expected,
bigger models are better, and dropout is very helpful in avoiding over-ﬁtting.
In the former task our best
model outperforms even all previously reported ensembles."
24,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-25,deeplearning,attention_is_all_you_need.pdf,"In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 770–778, 2016.
Neural computation ,
9(8):1735–1780, 1997.
In Advances in Neural
Information Processing Systems, (NIPS) , 2016.
In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,
Advances in Neural Information Processing Systems 28 , pages 2440–2448.
In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.
In
Advances in Neural Information Processing Systems , 2015."
25,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-26,deeplearning,attention_is_all_you_need.pdf,"[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V .
10[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio.
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
[14] Zhongqiang Huang and Mary Harper.
[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.
[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush.
[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen
Zhou, and Yoshua Bengio.
[23] Minh-Thang Luong, Quoc V .
[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.
[28] Romain Paulus, Caiming Xiong, and Richard Socher.
[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.
[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu."
26,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-27,deeplearning,attention_is_all_you_need.pdf,"In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.
The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-
wise fully connected feed-forward network.
(right) Multi-Head Attention consists of several
attention layers running in parallel.
4Multi-head attention allows the model to jointly attend to information from different representation
subspaces at different positions.
With a single attention head, averaging inhibits this.
In this work we employ h= 8 parallel attention layers, or heads.
Due to the reduced dimension of each head, the total computational cost
is similar to that of single-head attention with full dimensionality.
While single-head
attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads."
27,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-28,deeplearning,attention_is_all_you_need.pdf,"For the base model, we use a rate of
Pdrop= 0:1."
28,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-29,deeplearning,attention_is_all_you_need.pdf,"Deep residual learning for im-
age recognition."
29,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-30,deeplearning,attention_is_all_you_need.pdf,"We suspect that for large values of
dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has
extremely small gradients4.
To counteract this effect, we scale the dot products by1pdk."
30,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-31,deeplearning,attention_is_all_you_need.pdf,"CoRR , abs/1409.0473, 2014.
CoRR , abs/1703.03906, 2017.
CoRR , abs/1406.1078, 2014.
CoRR , abs/1412.3555, 2014.
Curran Associates,
Inc., 2015.
CoRR , abs/1512.00567, 2015.
CoRR , abs/1606.04199, 2016."
31,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-32,deeplearning,attention_is_all_you_need.pdf,"For each of these we use
dk=dv=dmodel=h= 64 .
5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.
This
suggests that determining compatibility is not easy and that a more sophisticated compatibility
function than dot product may be beneﬁcial."
32,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-33,deeplearning,attention_is_all_you_need.pdf,"Another way of describing this is as two convolutions with kernel size 1.
This would increase the maximum
path length to O(n=r).
A single convolutional layer with kernel width k<n does not connect all pairs of input and output
positions.
Doing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels,
orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths
between any two positions in the network."
33,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-34,deeplearning,attention_is_all_you_need.pdf,"31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v5  [cs.CL]  6 Dec 2017transduction problems such as language modeling and machine translation [ 35,2,5].
Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [38, 24, 15].
We also use the usual learned linear transfor-
mation and softmax function to convert the decoder output to predicted next-token probabilities.
Our results in Table 4 show that despite the lack of task-speciﬁc tuning our model performs sur-
prisingly well, yielding better results than all previously reported models with the exception of the
Recurrent Neural Network Grammar [8].
Learning phrase representations using rnn encoder-decoder for statistical
machine translation.
Google’s neural machine
translation system: Bridging the gap between human and machine translation.
Deep recurrent models with
fast-forward connections for neural machine translation."
34,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-35,deeplearning,attention_is_all_you_need.pdf,"The output is computed as a weighted sum
of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.
The input consists of
queries and keys of dimension dk, and values of dimension dv.
We compute the dot products of the
query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the
values."
35,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-36,deeplearning,attention_is_all_you_need.pdf,"7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the
English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
The Transformer (big) model trained for English-to-French used
dropout rate Pdrop= 0:1, instead of 0:3.
6.2 Model Variations
To evaluate the importance of different components of the Transformer, we varied our base model
in different ways, measuring the change in performance on English-to-German translation on the
development set, newstest2013."
36,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-37,deeplearning,attention_is_all_you_need.pdf,"4To illustrate why the dot products get large, assume that the components of qandkare independent random
variables with mean 0and variance 1.
Then their dot product, qk=Pdk
i=1qiki, has mean 0and variance dk."
37,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-38,deeplearning,attention_is_all_you_need.pdf,"Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful
comments, corrections and inspiration.
11[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.
[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean.
[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov.
[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al."
38,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-39,deeplearning,attention_is_all_you_need.pdf,"Many of the attention heads attend to a distant dependency of
the verb ‘making’, completing the phrase ‘making...more difﬁcult’.
Attentions here shown only for
the word ‘making’."
39,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-40,deeplearning,attention_is_all_you_need.pdf,"The best
performing models also connect the encoder and decoder through an attention
mechanism.
We also modify the self-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions.
3.2.3 Applications of Attention in our Model
The Transformer uses multi-head attention in three different ways:
In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,
and the memory keys and values come from the output of the encoder.
This allows every
position in the decoder to attend over all positions in the input sequence.
The encoder contains self-attention layers.
In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder.
Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position.
We need to prevent leftward
information ﬂow in the decoder to preserve the auto-regressive property.
<EOS>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
Figure 3: An example of the attention mechanism following long-distance dependencies in the
encoder self-attention in layer 5 of 6."
40,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-41,deeplearning,attention_is_all_you_need.pdf,"5.1 Training Data and Batching
We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million
sentence pairs.
Sentences were encoded using byte-pair encoding [ 3], which has a shared source-
target vocabulary of about 37000 tokens.
For English-French, we used the signiﬁcantly larger WMT
2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece
vocabulary [ 38].
Sentence pairs were batched together by approximate sequence length.
Each training
batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000
target tokens.
We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens
for the semi-supervised setting."
41,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-42,deeplearning,attention_is_all_you_need.pdf,"Ashish, with Illia, designed and implemented the ﬁrst Transformer models and
has been crucially involved in every aspect of this work.
In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].
2Figure 1: The Transformer - model architecture.
8Table 3: Variations on the Transformer architecture.
(2014) [37] semi-supervised 92.1
Transformer (4 layers) semi-supervised 92.7
Luong et al."
42,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-43,deeplearning,attention_is_all_you_need.pdf,"One is the total computational complexity per layer.
The third is the path length between long-range dependencies in the network.
Hence we also compare
the maximum path length between any two input and output positions in networks composed of the
different layer types."
43,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-44,deeplearning,attention_is_all_you_need.pdf,"(2014) [37] WSJ only, discriminative 88.3
Petrov et al.
(2006) [29] WSJ only, discriminative 90.4
Zhu et al.
(2013) [40] WSJ only, discriminative 90.4
Dyer et al.
(2016) [8] WSJ only, discriminative 91.7
Transformer (4 layers) WSJ only, discriminative 91.3
Zhu et al."
44,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-45,deeplearning,attention_is_all_you_need.pdf,"1 Introduction
Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks
in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and
Equal contribution.
Recurrent models typically factor computation along the symbol positions of the input and output
sequences.
Furthermore, RNN sequence-to-sequence
models have not been able to attain state-of-the-art results in small-data regimes [37].
Long short-term memory-networks for machine
reading.
Empirical evaluation
of gated recurrent neural networks on sequence modeling.
Recurrent neural
network grammars.
Generating sequences with recurrent neural networks.
Long short-term memory."
45,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-46,deeplearning,attention_is_all_you_need.pdf,"For the base models, we used a single model obtained by averaging the last 5 checkpoints, which
were written at 10-minute intervals.
For the big models, we averaged the last 20 checkpoints.
We used beam search as described in the previous section, but no
checkpoint averaging."
46,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-47,deeplearning,attention_is_all_you_need.pdf,"12Attention Visualizations
Input-Input Layer5
It
is
in
this
spirit
that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult
.
<EOS>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
It
is
in
this
spirit
that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult
."
47,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-48,deeplearning,attention_is_all_you_need.pdf,"[6]Francois Chollet.
[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin.
[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-
ray Kavukcuoglu.
[26] David McClosky, Eugene Charniak, and Mark Johnson.
[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.
[31] Rico Sennrich, Barry Haddow, and Alexandra Birch.
[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.
[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.
[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton."
48,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-49,deeplearning,attention_is_all_you_need.pdf,"To facilitate these residual connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension dmodel = 512 .
The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality
dff= 2048 ."
49,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-50,deeplearning,attention_is_all_you_need.pdf,"Noam proposed scaled dot-product attention, multi-head
attention and the parameter-free position representation and became the other person involved in nearly every
detail.
3Scaled Dot-Product Attention
 Multi-Head Attention
Figure 2: (left) Scaled Dot-Product Attention.
3.2.1 Scaled Dot-Product Attention
We call our particular attention ""Scaled Dot-Product Attention"" (Figure 2).
We compute
the matrix of outputs as:
Attention(Q;K;V ) = softmax(QKT
pdk)V (1)
The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-
plicative) attention.
Dot-product attention is identical to our algorithm, except for the scaling factor
of1pdk.
While the two are similar in theoretical complexity, dot-product attention is
much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized
matrix multiplication code.
We implement this
inside of scaled dot-product attention by masking out (setting to  1) all values in the input
of the softmax which correspond to illegal connections."
50,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-51,deeplearning,attention_is_all_you_need.pdf,Best viewed in color.
51,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-52,deeplearning,attention_is_all_you_need.pdf,"Structured attention networks.
A structured self-attentive sentence embedding.
A deep reinforced model for abstractive
summarization."
52,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-53,deeplearning,attention_is_all_you_need.pdf,"Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring signiﬁcantly
less time to train.
Our model achieves 28.4 BLEU on the WMT 2014 English-
to-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU.
On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature.
6 Results
6.1 Machine Translation
On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)
in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0
BLEU, establishing a new state-of-the-art BLEU score of 28:4.
On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0,
outperforming all of the previously published single models, at less than 1=4the training cost of the
previous state-of-the-art model.
Table 2 summarizes our results and compares our translation quality and training costs to other model
architectures from the literature.
All metrics are on the English-to-German translation development set, newstest2013.
We performed only a small number of experiments to select the dropout, both attention and residual
(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters
remained unchanged from the English-to-German base translation model.
On both WMT 2014 English-to-German and WMT 2014
English-to-French translation tasks, we achieve a new state of the art."
53,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-54,deeplearning,attention_is_all_you_need.pdf,"[10] Alex Graves.
[30] Oﬁr Press and Lior Wolf."
54,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-55,deeplearning,attention_is_all_you_need.pdf,"yWork performed while at Google Brain.
zWork performed while at Google Research."
55,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-56,deeplearning,attention_is_all_you_need.pdf,"In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax
linear transformation, similar to [ 30].
In the embedding layers, we multiply those weights bypdmodel."
56,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-57,deeplearning,attention_is_all_you_need.pdf,"In International Conference
on Learning Representations (ICLR) , 2016.
InInternational Conference on Learning Representations , 2017."
57,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-58,deeplearning,attention_is_all_you_need.pdf,"We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.
N d modeldffh d kdvPdroplstrain PPL BLEU params
steps (dev) (dev)106
base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65
(A)1 512 512 5.29 24.9
4 128 128 5.00 25.5
16 32 32 4.91 25.8
32 16 16 5.01 25.4
(B)16 5.16 25.1 58
32 5.01 25.4 60
(C)2 6.11 23.7 36
4 5.19 25.3 50
8 4.88 25.5 80
256 32 32 5.75 24.5 28
1024 128 128 4.66 26.0 168
1024 5.12 25.4 53
4096 4.75 26.2 90
(D)0.0 5.77 24.6
0.2 4.95 25.5
0.0 4.67 25.3
0.2 5.47 25.7
(E) positional embedding instead of sinusoids 4.92 25.7
big 6 1024 4096 16 0.3 300K 4.33 26.4 213
Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23
of WSJ)
Parser Training WSJ 23 F1
Vinyals & Kaiser el al.
6.3 English Constituency Parsing
To evaluate if the Transformer can generalize to other tasks we performed experiments on English
constituency parsing.
Self-training PCFG grammars with latent annotations
across languages.
Effective self-training for parsing.
Fast and accurate
shift-reduce constituent parsing."
58,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-59,deeplearning,attention_is_all_you_need.pdf,"In this work, we use sine and cosine functions of different frequencies:
PE(pos;2i)=sin(pos=100002i=d model)
PE(pos;2i+1)=cos(pos=100002i=d model)
whereposis the position and iis the dimension."
59,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-60,deeplearning,attention_is_all_you_need.pdf,"Llion also experimented with novel model variants, was responsible for our initial codebase, and
efﬁcient inference and visualizations."
60,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-61,deeplearning,attention_is_all_you_need.pdf,"Jakob proposed replacing RNNs with self-attention and started
the effort to evaluate this idea.
Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].
As side beneﬁt, self-attention could yield more interpretable models."
61,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-62,deeplearning,attention_is_all_you_need.pdf,"We set the maximum output length during
inference to input length + 50, but terminate early when possible [38].
Adam: A method for stochastic optimization.
Outrageously large neural networks: The sparsely-gated mixture-of-experts
layer.
Dropout: a simple way to prevent neural networks from overﬁtting.
Rethinking the inception architecture for computer vision."
62,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-63,deeplearning,attention_is_all_you_need.pdf,"arXiv preprint
arXiv:1607.06450 , 2016.
arXiv preprint arXiv:1601.06733 , 2016.
arXiv
preprint arXiv:1610.02357 , 2016.
arXiv preprint arXiv:1705.03122v2 , 2017.
arXiv preprint
arXiv:1308.0850 , 2013.
arXiv preprint arXiv:1602.02410 , 2016.
arXiv preprint arXiv:1610.10099v2 ,
2017.
arXiv preprint
arXiv:1703.10722 , 2017.
arXiv preprint
arXiv:1703.03130 , 2017.
arXiv preprint arXiv:1511.06114 , 2015.
arXiv preprint arXiv:1508.04025 , 2015.
arXiv preprint arXiv:1705.04304 , 2017.
arXiv
preprint arXiv:1608.05859 , 2016.
arXiv preprint arXiv:1508.07909 , 2015.
arXiv preprint arXiv:1701.06538 , 2017.
arXiv preprint
arXiv:1609.08144 , 2016."
63,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-64,deeplearning,attention_is_all_you_need.pdf,"We plan to investigate this approach further in future work.
Making generation less sequential is another research goals of ours."
64,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-65,deeplearning,attention_is_all_you_need.pdf,"We
used beam search with a beam size of 4and length penalty = 0:6[38].
This task presents speciﬁc challenges: the output is subject to strong structural
9constraints and is signiﬁcantly longer than the input."
65,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-66,deeplearning,attention_is_all_you_need.pdf,"Attention mechanisms have become an integral part of compelling sequence modeling and transduc-
tion models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [ 2,19].
In all but a few cases [ 27], however, such attention mechanisms
are used in conjunction with a recurrent network.
This mimics the
typical encoder-decoder attention mechanisms in sequence-to-sequence models such as
[38, 2, 9].
Learning long-range
dependencies is a key challenge in many sequence transduction tasks.
To improve computational performance for tasks involving
very long sequences, self-attention could be restricted to considering only a neighborhood of size rin
6the input sequence centered around the respective output position."
66,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-67,deeplearning,attention_is_all_you_need.pdf,"Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and
tensor2tensor.
Lukasz and Aidan spent countless long days designing various parts of and
implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating
our research.
The code we used to train and evaluate our models is available at https://github.com/
tensorflow/tensor2tensor ."
67,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-68,deeplearning,attention_is_all_you_need.pdf,Listing order is random.
68,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-69,deeplearning,attention_is_all_you_need.pdf,"Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence.
nis the sequence length, dis the representation dimension, kis the kernel
size of convolutions and rthe size of the neighborhood in restricted self-attention.
4 Why Self-Attention
In this section we compare various aspects of self-attention layers to the recurrent and convolu-
tional layers commonly used for mapping one variable-length sequence of symbol representations
(x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden
layer in a typical sequence transduction encoder or decoder.
As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially
executed operations, whereas a recurrent layer requires O(n)sequential operations.
In terms of
computational complexity, self-attention layers are faster than recurrent layers when the sequence
lengthnis smaller than the representation dimensionality d, which is most often the case with
sentence representations used by state-of-the-art models in machine translations, such as word-piece
[38] and byte-pair [ 31] representations."
69,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-70,deeplearning,attention_is_all_you_need.pdf,"5.3 Optimizer
We used the Adam optimizer [ 20] with1= 0:9,2= 0:98and= 10 9.
We varied the learning
rate over the course of training, according to the formula:
lrate =d 0:5
modelmin(step_num 0:5;step _numwarmup _steps 1:5) (3)
This corresponds to increasing the learning rate linearly for the ﬁrst warmup _steps training steps,
and decreasing it thereafter proportionally to the inverse square root of the step number.
We used
warmup _steps = 4000 ."
70,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-71,deeplearning,attention_is_all_you_need.pdf,"We employ a residual connection [ 11] around each of
the two sub-layers, followed by layer normalization [ 1].
Similar to the encoder, we employ residual connections
around each of the sub-layers, followed by layer normalization.
5.4 Regularization
We employ three types of regularization during training:
Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the
sub-layer input and normalized.
Layer normalization."
71,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-72,deeplearning,attention_is_all_you_need.pdf,"5.2 Hardware and Schedule
We trained our models on one machine with 8 NVIDIA P100 GPUs.
Training took 3:5days on 8P100 GPUs.
We estimate the number of ﬂoating point operations used to train a
model by multiplying the training time, the number of GPUs used, and an estimate of the sustained
single-precision ﬂoating-point capacity of each GPU5."
72,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-73,deeplearning,attention_is_all_you_need.pdf,"These are concatenated and once again projected, resulting in the ﬁnal values, as
depicted in Figure 2.
See Figure 2."
73,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-74,deeplearning,attention_is_all_you_need.pdf,"Unlisted values are identical to those of the base
model."
74,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-75,deeplearning,attention_is_all_you_need.pdf,"The fundamental
constraint of sequential computation, however, remains.
Another is the amount of computation that can
be parallelized, as measured by the minimum number of sequential operations required."
75,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-76,deeplearning,attention_is_all_you_need.pdf,The wavelengths form a geometric progression from 2to100002.
76,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-77,deeplearning,attention_is_all_you_need.pdf,15
77,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-78,deeplearning,attention_is_all_you_need.pdf,"In Proceedings of the 2009 Conference on Empirical Methods in Natural
Language Processing , pages 832–841.
In
Proceedings of the Human Language Technology Conference of the NAACL, Main Conference ,
pages 152–159.
In Empirical Methods in Natural Language Processing , 2016.
Journal of Machine
Learning Research , 15(1):1929–1958, 2014."
78,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-79,deeplearning,attention_is_all_you_need.pdf,"We
chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of
PEpos.
We chose the sinusoidal version
because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
during training."
79,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-80,deeplearning,attention_is_all_you_need.pdf,"In ICLR , 2015.
In Proceedings of the 51st Annual Meeting of the ACL (Volume
1: Long Papers) , pages 434–443."
80,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-81,deeplearning,attention_is_all_you_need.pdf,The heads clearly learned to perform different tasks.
81,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-82,deeplearning,attention_is_all_you_need.pdf,"The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.
3.1 Encoder and Decoder Stacks
Encoder: The encoder is composed of a stack of N= 6 identical layers.
Decoder: The decoder is also composed of a stack of N= 6identical layers.
In addition to the two
sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head
attention over the output of the encoder stack.
Each position in the encoder can attend to all positions in the previous layer of the
encoder.
We give two such examples above, from two different heads from the encoder self-attention
at layer 5 of 6."
82,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-83,deeplearning,attention_is_all_you_need.pdf,"(2006) [26] semi-supervised 92.1
Vinyals & Kaiser el al."
83,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-84,deeplearning,attention_is_all_you_need.pdf,"(2015) [23] multi-task 93.0
Dyer et al."
84,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-85,deeplearning,attention_is_all_you_need.pdf,"We also experimented with using learned positional embeddings [ 9] instead, and found that the two
versions produced nearly identical results (see Table 3 row (E)).
In row (E) we replace our
sinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical
results to the base model."
85,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-86,deeplearning,attention_is_all_you_need.pdf,"This
hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score."
86,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-87,deeplearning,attention_is_all_you_need.pdf,"Exploring
the limits of language modeling.
Using the output embedding to improve language models."
87,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-88,deeplearning,attention_is_all_you_need.pdf,"This makes
it more difﬁcult to learn dependencies between distant positions [ 12].
One key factor affecting the
ability to learn such dependencies is the length of the paths forward and backward signals have to
traverse in the network.
The shorter these paths between any combination of positions in the input
and output sequences, the easier it is to learn long-range dependencies [ 12]."
88,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-89,deeplearning,attention_is_all_you_need.pdf,"Listed
perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to
per-word perplexities."
89,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-90,deeplearning,attention_is_all_you_need.pdf,"References
[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton."
90,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-91,deeplearning,attention_is_all_you_need.pdf,"Convolu-
tional sequence to sequence learning.
Multi-task
sequence to sequence learning.
Sequence to sequence learning with neural
networks."
91,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-92,deeplearning,attention_is_all_you_need.pdf,Different colors represent different heads.
92,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-93,deeplearning,attention_is_all_you_need.pdf,"Here, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence
of continuous representations z= (z1;:::;z n).
Given z, the decoder then generates an output
sequence (y1;:::;y m)of symbols one element at a time.
This
consists of two linear transformations with a ReLU activation in between.
That is, each dimension of the positional encoding
corresponds to a sinusoid."
93,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-94,deeplearning,attention_is_all_you_need.pdf,In Proc.
94,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-95,deeplearning,attention_is_all_you_need.pdf,"Motivating our use of self-attention we
consider three desiderata.
Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.
<EOS>
<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution.
Top:
Full attentions for head 5.
Bottom: Isolated attentions from just the word ‘its’ for attention heads 5
and 6.
Note that the attentions are very sharp for this word.
<EOS>
<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the
sentence."
95,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-96,deeplearning,attention_is_all_you_need.pdf,"[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini."
96,e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-97,deeplearning,attention_is_all_you_need.pdf,"2 Background
The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU
[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions.
Layer Type Complexity per Layer Sequential Maximum Path Length
Operations
Self-Attention O(n2d) O(1) O(1)
Recurrent O(nd2) O(n) O(n)
Convolutional O(knd2)O(1) O(logk(n))
Self-Attention (restricted) O(rnd)O(1) O(n=r)
tokens in the sequence.
Convolutional layers are generally more expensive than
recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity
considerably, to O(knd+nd2).
Even with k=n, however, the complexity of a separable
convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,
the approach we take in our model.
Xception: Deep learning with depthwise separable convolutions."
97,0e743db25fb819961cd2f051fb88dd185c37e45bc1e11944edac8d96cb4a51e5-1,deeplearning,the_matrix_calculus_you_need_for_deeplearning.pdf,".
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
."
98,0e743db25fb819961cd2f051fb88dd185c37e45bc1e11944edac8d96cb4a51e5-2,deeplearning,the_matrix_calculus_you_need_for_deeplearning.pdf,"The Matrix Calculus You Need For Deep Learning
Terence Parr and Jeremy Howard
July 3, 2018
(We teach in University of San Francisco's MS in Data Science program and have other nefarious
projects underway.
You might know Terence as the creator of the ANTLR parser generator.
For
more material, see Jeremy's fast.ai courses and University of San Francisco's Data Institute in-
person version of the deep learning course.)
HTML version (The PDF and HTML were generated from markup using bookish)
Abstract
This paper is an attempt to explain all the matrix calculus you need in order to understand
the training of deep neural networks.
We assume no math knowledge beyond what you learned
in calculus 1, and provide links to help you refresh the necessary math where needed.
Note that
you do notneed to understand this material before you start learning to train and use deep
learning in practice; rather, this material is for those who are already familiar with the basics of
neural networks, and wish to deepen their understanding of the underlying math.
Don't worry
if you get stuck at some point along the way|just go back and reread the previous section, and
try writing down and working through some examples.
And if you're still stuck, we're happy
to answer your questions in the Theory category at forums.fast.ai.
Note : There is a reference
section at the end of the paper summarizing all the key matrix calculus rules and terminology
discussed here.
1arXiv:1802.01528v3  [cs.LG]  2 Jul 2018Contents
1 Introduction 3
2 Review: Scalar derivative rules 4
3 Introduction to vector calculus and partial derivatives 5
4 Matrix calculus 6
4.1 Generalization of the Jacobian .
7
4.2 Derivatives of vector element-wise binary operators .
9
4.3 Derivatives involving scalar expansion .
11
4.4 Vector sum reduction .
12
4.5 The Chain Rules .
14
4.5.1 Single-variable chain rule .
14
4.5.2 Single-variable total-derivative chain rule .
18
4.5.3 Vector chain rule .
21
5 The gradient of neuron activation 23
6 The gradient of the neural network loss function 25
6.1 The gradient with respect to the weights .
26
6.2 The derivative with respect to the bias .
27
7 Summary 29
8 Matrix Calculus Reference 29
8.1 Gradients and Jacobians .
29
8.2 Element-wise operations on vectors .
30
8.3 Scalar expansion .
30
8.4 Vector reductions .
31
8.5 Chain rules .
31
9 Notation 31
10 Resources 32
21 Introduction
Most of us last saw calculus in school, but derivatives are a critical part of machine learning,
particularly deep neural networks, which are trained by optimizing a loss function.
Pick up a
machine learning paper or the documentation of a library such as PyTorch and calculus comes
screeching back into your life like distant relatives around the holidays.
And it's not just any old
scalar calculus that pops up|you need dierential matrix calculus , the shotgun wedding of linear
algebra and multivariate calculus.
Well... maybe need isn't the right word; Jeremy's courses show how to become a world-class deep
learning practitioner with only a minimal level of scalar calculus, thanks to leveraging the automatic
dierentiation built in to modern deep learning libraries.
But if you really want to really understand
what's going on under the hood of these libraries, and grok academic papers discussing the latest
advances in model training techniques, you'll need to understand certain bits of the eld of matrix
calculus.
For example, the activation of a single computation unit in a neural network is typically calculated
using the dot product (from linear algebra) of an edge weight vector wwith an input vector xplus
a scalar bias (threshold): z(x) =Pn
iwixi+b=wx+b.
Function z(x) is called the unit's ane
function and is followed by a rectied linear unit, which clips negative values to zero: max(0;z(x)).
Such a computational unit is sometimes referred to as an \articial neuron"" and looks like:
Neural networks consist of many of these units, organized into multiple collections of neurons called
layers .
The activation of one layer's units become the input to the next layer's units.
The activation
of the unit or units in the nal layer is called the network output.
Training this neuron means choosing weights wand biasbso that we get the desired output for all N
inputs x.
To do that, we minimize a loss function that compares the network's nal activation (x)
with thetarget (x) (desired output of x) for all input xvectors.
To minimize the loss, we use
some variation on gradient descent, such as plain stochastic gradient descent (SGD), SGD with
momentum, or Adam.
All of those require the partial derivative (the gradient) of activation (x)
with respect to the model parameters wandb.
Our goal is to gradually tweak wandbso that the
overall loss function keeps getting smaller across all xinputs.
If we're careful, we can derive the gradient by dierentiating the scalar version of a common loss
3function (mean squared error):
1
NX
x(target (x) activation (x))2=1
NX
x(target (x) max(0;jxjX
iwixi+b))2
But this is just one neuron, and neural networks must train the weights and biases of all neurons
in all layers simultaneously.
Because there are multiple inputs and (potentially) multiple network
outputs, we really need general rules for the derivative of a function with respect to a vector and
even rules for the derivative of a vector-valued function with respect to a vector.
This article walks through the derivation of some important rules for computing partial derivatives
with respect to vectors, particularly those useful for training neural networks.
This eld is known as
matrix calculus , and the good news is, we only need a small subset of that eld, which we introduce
here.
While there is a lot of online material on multivariate calculus and linear algebra, they are
typically taught as two separate undergraduate courses so most material treats them in isolation.
The pages that do discuss matrix calculus often are really just lists of rules with minimal explanation
or are just pieces of the story.
They also tend to be quite obscure to all but a narrow audience
of mathematicians, thanks to their use of dense notation and minimal discussion of foundational
concepts.
(See the annotated list of resources at the end.)
In contrast, we're going to rederive and rediscover some key matrix calculus rules in an eort to
explain them.
It turns out that matrix calculus is really not that hard!
There aren't dozens of new
rules to learn; just a couple of key concepts.
Our hope is that this short paper will get you started
quickly in the world of matrix calculus as it relates to training neural networks.
We're assuming
you're already familiar with the basics of neural network architecture and training.
If you're not,
head over to Jeremy's course and complete part 1 of that, then we'll see you back here when you're
done.
(Note that, unlike many more academic approaches, we strongly suggest rst learning to
train and use neural networks in practice and then study the underlying math.
The math will be
much more understandable with the context in place; besides, it's not necessary to grok all this
calculus to become an eective practitioner.)
A note on notation : Jeremy's course exclusively uses code, instead of math notation, to explain
concepts since unfamiliar functions in code are easy to search for and experiment with.
In this
paper, we do the opposite: there is a lot of math notation because one of the goals of this paper is
to help you understand the notation that you'll see in deep learning papers and books.
At the end
of the paper, you'll nd a brief table of the notation used, including a word or phrase you can use
to search for more details.
2 Review: Scalar derivative rules
Hopefully you remember some of these main scalar derivative rules.
If your memory is a bit fuzzy
on this, have a look at Khan academy vid on scalar derivative rules.
4Rule f(x) Scalar derivative notation
with respect to xExample
Constant c 0d
dx99 = 0
Multiplication
by constantcf cd f
dxd
dx3x= 3
Power Rule xnnxn 1 d
dxx3= 3x2
Sum Rule f+gd f
dx+dg
dxd
dx(x2+ 3x) = 2x+ 3
Dierence Rule f gd f
dx dg
dxd
dx(x2 3x) = 2x 3
Product Rule fg fdg
dx+d f
dxgd
dxx2x=x2+x2x= 3x2
Chain Rule f(g(x))d f(u)
dudu
dx, letu=g(x)d
dxln(x2) =1
x22x=2
x
There are other rules for trigonometry, exponentials, etc., which you can nd at Khan Academy
dierential calculus course.
When a function has a single parameter, f(x), you'll often see f0andf0(x) used as shorthands for
d
dxf(x).
We recommend against this notation as it does not make clear the variable we're taking
the derivative with respect to.
You can think ofd
dxas an operator that maps a function of one parameter to another function.
That means thatd
dxf(x) mapsf(x) to its derivative with respect to x, which is the same thing
asd f(x)
dx.
Also, ify=f(x), thendy
dx=d f(x)
dx=d
dxf(x).
Thinking of the derivative as an operator
helps to simplify complicated derivatives because the operator is distributive and lets us pull out
constants.
For example, in the following equation, we can pull out the constant 9 and distribute
the derivative operator across the elements within the parentheses.
d
dx9(x+x2) = 9d
dx(x+x2) = 9(d
dxx+d
dxx2) = 9(1 + 2x) = 9 + 18x
That procedure reduced the derivative of 9( x+x2) to a bit of arithmetic and the derivatives of x
andx2, which are much easier to solve than the original derivative.
3 Introduction to vector calculus and partial derivatives
Neural network layers are not single functions of a single parameter, f(x).
So, let's move on to
functions of multiple parameters such as f(x;y).
For example, what is the derivative of xy(i.e.,
the multiplication of xandy)?
In other words, how does the product xychange when we wiggle
the variables?
Well, it depends on whether we are changing xory.
We compute derivatives with
respect to one variable (parameter) at a time, giving us two dierent partial derivatives for this two-
parameter function (one for xand one for y).
Instead of using operatord
dx, the partial derivative
operator is@
@x(a stylizeddand not the Greek letter ).
So,@
@xxyand@
@yxyare the partial derivatives
ofxy; often, these are just called the partials .
For functions of a single parameter, operator@
@xis
equivalent tod
dx(for suciently smooth functions).
However, it's better to used
dxto make it clear
you're referring to a scalar derivative.
The partial derivative with respect to xis just the usual scalar derivative, simply treating any other
variable in the equation as a constant.
Consider function f(x;y) = 3x2y.
The partial derivative
5with respect to xis written@
@x3x2y.
There are three constants from the perspective of@
@x: 3, 2,
andy.
Therefore,@
@x3yx2= 3y@
@xx2= 3y2x= 6yx.
The partial derivative with respect to ytreats
xlike a constant:@
@y3x2y= 3x2@
@yy= 3x2@y
@y= 3x21 = 3x2.
It's a good idea to derive these
yourself before continuing otherwise the rest of the article won't make sense.
Here's the Khan
Academy video on partials if you need help.
To make it clear we are doing vector calculus and not just multivariate calculus, let's consider what
we do with the partial derivatives@f(x;y)
@xand@f(x;y)
@y(another way to say@
@xf(x;y) and@
@yf(x;y))
that we computed for f(x;y) = 3x2y.
Instead of having them just oating around and not organized
in any way, let's organize them into a horizontal vector.
We call this vector the gradient off(x;y)
and write it as:
rf(x;y) = [@f(x;y)
@x;@f(x;y)
@y] = [6yx;3x2]
So the gradient of f(x;y) is simply a vector of its partials.
Gradients are part of the vector calculus
world, which deals with functions that map nscalar parameters to a single scalar.
Now, let's get
crazy and consider derivatives of multiple functions simultaneously.
4 Matrix calculus
When we move from derivatives of one function to derivatives of many functions, we move from
the world of vector calculus to matrix calculus.
Let's compute partial derivatives for two functions,
both of which take two parameters.
We can keep the same f(x;y) = 3x2yfrom the last section,
but let's also bring in g(x;y) = 2x+y8.
The gradient for ghas two entries, a partial derivative for
each parameter:
@g(x;y)
@x=@2x
@x+@y8
@x= 2@x
@x+ 0 = 21 = 2
and
@g(x;y)
@y=@2x
@y+@y8
@y= 0 + 8y7= 8y7
giving us gradient rg(x;y) = [2;8y7].
Gradient vectors organize all of the partial derivatives for a specic scalar function.
If we have two
functions, we can also organize their gradients into a matrix by stacking the gradients.
When we
do so, we get the Jacobian matrix (or just the Jacobian ) where the gradients are rows:
J=rf(x;y)
rg(x;y)
=""@f(x;y)
@x@f(x;y)
@y
@g(x;y)
@x@g(x;y)
@y#
=6yx3x2
2 8y7
Welcome to matrix calculus!
Note that there are multiple ways to represent the Jacobian.
We are using the so-called
numerator layout but many papers and software will use the denominator layout .
This is just
6transpose of the numerator layout Jacobian (ip it around its diagonal):
6yx 2
3x28y7
4.1 Generalization of the Jacobian
So far, we've looked at a specic example of a Jacobian matrix.
To dene the Jacobian matrix
more generally, let's combine multiple parameters into a single vector argument: f(x;y;z ))f(x).
(You will sometimes see notation ~ xfor vectors in the literature as well.)
Lowercase letters in bold
font such as xare vectors and those in italics font like xare scalars.
xiis theithelement of vector
xand is in italics because a single vector element is a scalar.
We also have to dene an orientation
for vector x.
We'll assume that all vectors are vertical by default of size n1:
x=2
6664x1
x2
...
xn3
7775
With multiple scalar-valued functions, we can combine them all into a vector just like we did with
the parameters.
Let y=f(x) be a vector of mscalar-valued functions that each take a vector x
of lengthn=jxjwherejxjis the cardinality (count) of elements in x. Eachfifunction within f
returns a scalar just as in the previous section:
y1=f1(x)
y2=f2(x)
...
ym=fm(x)
For instance, we'd represent f(x;y) = 3x2yandg(x;y) = 2x+y8from the last section as
y1=f1(x) = 3x2
1x2 (substituting x1forx;x 2fory)
y2=f2(x) = 2x1+x8
2
It's very often the case that m=nbecause we will have a scalar function result for each element
of the xvector.
For example, consider the identity function y=f(x) =x:
y1=f1(x) =x1
y2=f2(x) =x2
...
yn=fn(x) =xn
So we have m=nfunctions and parameters, in this case.
Generally speaking, though, the Jacobian
matrix is the collection of all mnpossible partial derivatives ( mrows andncolumns), which is
the stack of mgradients with respect to x:
@y
@x=2
664rf1(x)
rf2(x)
:::
rfm(x)3
775=2
664@
@xf1(x)
@
@xf2(x)
:::
@
@xfm(x)3
775=2
6664@
@x1f1(x)@
@x2f1(x):::@
@xnf1(x)
@
@x1f2(x)@
@x2f2(x):::@
@xnf2(x)
:::
@
@x1fm(x)@
@x2fm(x):::@
@xnfm(x)3
7775
7Each@
@xfi(x) is a horizontal n-vector because the partial derivative is with respect to a vector, x,
whose length is n=jxj.
The width of the Jacobian is nif we're taking the partial derivative with
respect to xbecause there are nparameters we can wiggle, each potentially changing the function's
value.
Therefore, the Jacobian is always mrows formequations.
It helps to think about the
possible Jacobian shapes visually:
scalar
xvector
x
scalar
f@f
@x@f
@x
vector
f@f
@x@f
@x
The Jacobian of the identity function f(x) =x, withfi(x) =xi, hasnfunctions and each function
hasnparameters held in a single vector x.
The Jacobian is, therefore, a square matrix since m=n:
@y
@x=2
664@
@xf1(x)
@
@xf2(x)
:::
@
@xfm(x)3
775=2
6664@
@x1f1(x)@
@x2f1(x):::@
@xnf1(x)
@
@x1f2(x)@
@x2f2(x):::@
@xnf2(x)
:::
@
@x1fm(x)@
@x2fm(x):::@
@xnfm(x)3
7775
=2
6664@
@x1x1@
@x2x1:::@
@xnx1
@
@x1x2@
@x2x2:::@
@xnx2
:::
@
@x1xn@
@x2xn:::@
@xnxn3
7775
(and since@
@xjxi= 0 forj6=i)
=2
6664@
@x1x1 0::: 0
0@
@x2x2::: 0
...
0 0 :::@
@xnxn3
7775
=2
66641 0:::0
0 1:::0
...
0 0:::13
7775
=I(Iis the identity matrix with ones down the diagonal)
8Make sure that you can derive each step above before moving on.
If you get stuck, just consider each
element of the matrix in isolation and apply the usual scalar derivative rules.
That is a generally
useful trick: Reduce vector expressions down to a set of scalar expressions and then take all of the
partials, combining the results appropriately into vectors and matrices at the end.
Also be careful to track whether a matrix is vertical, x, or horizontal, xTwhere xTmeans x
transpose.
Also make sure you pay attention to whether something is a scalar-valued function,
y=:::, or a vector of functions (or a vector-valued function), y=:::.
4.2 Derivatives of vector element-wise binary operators
Element-wise binary operations on vectors, such as vector addition w+x, are important because
we can express many common vector operations, such as the multiplication of a vector by a scalar,
as element-wise binary operations.
By \element-wise binary operations"" we simply mean applying
an operator to the rst item of each vector to get the rst item of the output, then to the second
items of the inputs for the second item of the output, and so forth.
This is how all the basic math
operators are applied by default in numpy or tensorow, for example.
Examples that often crop
up in deep learning are max(w;x) and w>x(returns a vector of ones and zeros).
We can generalize the element-wise binary operations with notation y=f(w)g(x) wherem=
n=jyj=jwj=jxj.
(Reminder:jxjis the number of items in x.)
Thesymbol represents
any element-wise operator (such as +) and not the function composition operator.
Here's what
equation y=f(w)g(x) looks like when we zoom in to examine the scalar equations:
2
6664y1
y2
...
yn3
7775=2
6664f1(w)g1(x)
f2(w)g2(x)
...
fn(w)gn(x)3
7775
where we write n(notm) equations vertically to emphasize the fact that the result of element-wise
operators give m=nsized vector results.
Using the ideas from the last section, we can see that the general case for the Jacobian with respect
towis the square matrix:
Jw=@y
@w=2
6664@
@w1(f1(w)g1(x))@
@w2(f1(w)g1(x)):::@
@wn(f1(w)g1(x))
@
@w1(f2(w)g2(x))@
@w2(f2(w)g2(x)):::@
@wn(f2(w)g2(x))
:::
@
@w1(fn(w)gn(x))@
@w2(fn(w)gn(x)):::@
@wn(fn(w)gn(x))3
7775
and the Jacobian with respect to xis:
Jx=@y
@x=2
6664@
@x1(f1(w)g1(x))@
@x2(f1(w)g1(x)):::@
@xn(f1(w)g1(x))
@
@x1(f2(w)g2(x))@
@x2(f2(w)g2(x)):::@
@xn(f2(w)g2(x))
:::
@
@x1(fn(w)gn(x))@
@x2(fn(w)gn(x)):::@
@xn(fn(w)gn(x))3
7775
9That's quite a furball, but fortunately the Jacobian is very often a diagonal matrix, a matrix that
is zero everywhere but the diagonal.
Because this greatly simplies the Jacobian, let's examine in
detail when the Jacobian reduces to a diagonal matrix for element-wise operations.
In a diagonal Jacobian, all elements o the diagonal are zero,@
@wj(fi(w)gi(x)) = 0 where j6=i.
(Notice that we are taking the partial derivative with respect to wjnotwi.)
Under what conditions
are those o-diagonal elements zero?
Precisely when fiandgiare contants with respect to wj,
@
@wjfi(w) =@
@wjgi(x) = 0.
Regardless of the operator, if those partial derivatives go to zero, the
operation goes to zero, 0 0 = 0 no matter what, and the partial derivative of a constant is zero.
Those partials go to zero when fiandgiare not functions of wj.
We know that element-wise
operations imply that fiis purely a function of wiandgiis purely a function of xi.
For example,
w+xsumswi+xi.
Consequently, fi(w)gi(x) reduces to fi(wi)gi(xi) and the goal becomes
@
@wjfi(wi) =@
@wjgi(xi) = 0.fi(wi) andgi(xi) look like constants to the partial dierentiation op-
erator with respect to wjwhenj6=iso the partials are zero o the diagonal.
(Notation fi(wi) is
technically an abuse of our notation because fiandgiare functions of vectors not individual ele-
ments.
We should really write something like ^fi(wi) =fi(w), but that would muddy the equations
further, and programmers are comfortable overloading functions, so we'll proceed with the notation
anyway.)
We'll take advantage of this simplication later and refer to the constraint that fi(w) andgi(x)
access at most wiandxi, respectively, as the element-wise diagonal condition .
Under this condition, the elements along the diagonal of the Jacobian are@
@wi(fi(wi)gi(xi)):
@y
@w=2
66664@
@w1(f1(w1)g1(x1))
@
@w2(f2(w2)g2(x2)) 0
:::
0@
@wn(fn(wn)gn(xn))3
77775
(The large \0""s are a shorthand indicating all of the o-diagonal are 0.)
More succinctly, we can write:
@y
@w=diag@
@w1(f1(w1)g1(x1));@
@w2(f2(w2)g2(x2)); :::;@
@wn(fn(wn)gn(xn))
and
@y
@x=diag@
@x1(f1(w1)g1(x1));@
@x2(f2(w2)g2(x2)); :::;@
@xn(fn(wn)gn(xn))
wherediag(x) constructs a matrix whose diagonal elements are taken from vector x.
Because we do lots of simple vector arithmetic, the general function f(w) in the binary element-
wise operation is often just the vector w. Any time the general function is a vector, we know
thatfi(w) reduces to fi(wi) =wi.
For example, vector addition w+xts our element-wise
10diagonal condition because f(w) +g(x) has scalar equations yi=fi(w) +gi(x) that reduce to just
yi=fi(wi) +gi(xi) =wi+xiwith partial derivatives:
@
@wi(fi(wi) +gi(xi)) =@
@wi(wi+xi) = 1 + 0 = 1
@
@xi(fi(wi) +gi(xi)) =@
@xi(wi+xi) = 0 + 1 = 1
That gives us@(w+x)
@w=@(w+x)
@x=I, the identity matrix, because every element along the diagonal
is 1.Irepresents the square identity matrix of appropriate dimensions that is zero everywhere but
the diagonal, which contains all ones.
Given the simplicity of this special case, fi(w) reducing to fi(wi), you should be able to derive the
Jacobians for the common element-wise binary operations on vectors:
Op Partial with respect to w
+@(w+x)
@w=diag(:::@(wi+xi)
@wi:::) =diag(~1) =I
 @(w x)
@w=diag(:::@(wi xi)
@wi:::) =diag(~1) =I

@(w
x)
@w=diag(:::@(wixi)
@wi:::) =diag(x)
@(wx)
@w=diag(:::@(wi=xi)
@wi:::) =diag(:::1
xi:::)
Op Partial with respect to x
+@(w+x)
@x=I
 @(w x)
@x=diag(:::@(wi xi)
@xi:::) =diag( ~1) = I

@(w
x)
@x=diag(w)
@(wx)
@x=diag(::: wi
x2
i:::)
The
andoperators are element-wise multiplication and division; 
is sometimes called the
Hadamard product .
There isn't a standard notation for element-wise multiplication and division so
we're using an approach consistent with our general binary operation notation.
4.3 Derivatives involving scalar expansion
When we multiply or add scalars to vectors, we're implicitly expanding the scalar to a vector
and then performing an element-wise binary operation.
For example, adding scalar zto vector x,
y=x+z, is really y=f(x) +g(z) where f(x) =xandg(z) =~1z.
(The notation ~1 represents
a vector of ones of appropriate length.)
zis any scalar that doesn't depend on x, which is useful
because then@z
@xi= 0 for any xiand that will simplify our partial derivative computations.
(It's
okay to think of variable zas a constant for our discussion here.)
Similarly, multiplying by a scalar,
11y=xz, is really y=f(x)
g(z) =x
~1zwhere
is the element-wise multiplication (Hadamard
product) of the two vectors.
The partial derivatives of vector-scalar addition and multiplication with respect to vector xuse our
element-wise rule:
@y
@x=diag
:::@
@xi(fi(xi)gi(z)):::
This follows because functions f(x) =xandg(z) =~1zclearly satisfy our element-wise diagonal
condition for the Jacobian (that fi(x) refer at most to xiandgi(z) refers to the ithvalue of the ~1z
vector).
Using the usual rules for scalar partial derivatives, we arrive at the following diagonal elements of
the Jacobian for vector-scalar addition:
@
@xi(fi(xi) +gi(z)) =@(xi+z)
@xi=@xi
@xi+@z
@xi= 1 + 0 = 1
So,@
@x(x+z) =diag(~1) =I.
Computing the partial derivative with respect to the scalar parameter z, however, results in a
vertical vector, not a diagonal matrix.
The elements of the vector are:
@
@z(fi(xi) +gi(z)) =@(xi+z)
@z=@xi
@z+@z
@z= 0 + 1 = 1
Therefore,@
@z(x+z) =~1.
The diagonal elements of the Jacobian for vector-scalar multiplication involve the product rule for
scalar derivatives:
@
@xi(fi(xi)
gi(z)) =xi@z
@xi+z@xi
@xi= 0 +z=z
So,@
@x(xz) =diag(~1z) =Iz.
The partial derivative with respect to scalar parameter zis a vertical vector whose elements are:
@
@z(fi(xi)
gi(z)) =xi@z
@z+z@xi
@z=xi+ 0 =xi
This gives us@
@z(xz) =x.
4.4 Vector sum reduction
Summing up the elements of a vector is an important operation in deep learning, such as the
network loss function, but we can also use it as a way to simplify computing the derivative of
vector dot product and other operations that reduce vectors to scalars.
12Lety=sum(f(x)) =Pn
i=1fi(x).
Notice we were careful here to leave the parameter as a vector x
because each function ficould use all values in the vector, not just xi.
The sum is over the results
of the function and not the parameter.
The gradient (1 nJacobian) of vector summation is:
@y
@x=h
@y
@x1;@y
@x2;:::;@y
@xni
=h
@
@x1P
ifi(x);@
@x2P
ifi(x); :::;@
@xnP
ifi(x)i
=hP
i@fi(x)
@x1;P
i@fi(x)
@x2; :::;P
i@fi(x)
@xni
(move derivative insideP)
(The summation inside the gradient elements can be tricky so make sure to keep your notation
consistent.)
Let's look at the gradient of the simple y=sum(x).
The function inside the summation is just
fi(x) =xiand the gradient is then:
ry=hP
i@fi(x)
@x1;P
i@fi(x)
@x2; :::;P
i@fi(x)
@xni
=hP
i@xi
@x1;P
i@xi
@x2; :::;P
i@xi
@xni
Because@
@xjxi= 0 forj6=i, we can simplify to:
ry=h
@x1
@x1;@x2
@x2; :::;@xn
@xni
=
1;1;:::; 1
=~1T
Notice that the result is a horizontal vector full of 1s, not a vertical vector, and so the gradient is
~1T.
(TheTexponent of ~1Trepresents the transpose of the indicated vector.
In this case, it ips a
vertical vector to a horizontal vector.)
It's very important to keep the shape of all of your vectors
and matrices in order otherwise it's impossible to compute the derivatives of complex functions.
As another example, let's sum the result of multiplying a vector by a constant scalar.
If y=sum(xz)
thenfi(x;z) =xiz.
The gradient is:
@y
@x=hP
i@
@x1xiz;P
i@
@x2xiz; :::;P
i@
@xnxizi
=h
@
@x1x1z;@
@x2x2z; :::;@
@xnxnzi
=
z;z;:::;z
The derivative with respect to scalar variable zis 11:
@y
@z=@
@zPn
i=1xiz
=P
i@
@zxiz
=P
ixi
=sum(x)
134.5 The Chain Rules
We can't compute partial derivatives of very complicated functions using just the basic matrix
calculus rules we've seen so far.
For example, we can't take the derivative of nested expressions like
sum(w+x) directly without reducing it to its scalar equivalent.
We need to be able to combine
our basic vector rules using what we can call the vector chain rule .
Unfortunately, there are a
number of rules for dierentiation that fall under the name \chain rule"" so we have to be careful
which chain rule we're talking about.
Part of our goal here is to clearly dene and name three
dierent chain rules and indicate in which situation they are appropriate.
To get warmed up, we'll
start with what we'll call the single-variable chain rule , where we want the derivative of a scalar
function with respect to a scalar.
Then we'll move on to an important concept called the total
derivative and use it to dene what we'll pedantically call the single-variable total-derivative chain
rule.
Then, we'll be ready for the vector chain rule in its full glory as needed for neural networks.
The chain rule is conceptually a divide and conquer strategy (like Quicksort) that breaks compli-
cated expressions into subexpressions whose derivatives are easier to compute.
Its power derives
from the fact that we can process each simple subexpression in isolation yet still combine the
intermediate results to get the correct overall result.
The chain rule comes into play when we need the derivative of an expression composed of nested
subexpressions.
For example, we need the chain rule when confronted with expressions liked
dxsin(x2).
The outermost expression takes the sinof an intermediate result, a nested subexpression that
squaresx.
Specically, we need the single-variable chain rule, so let's start by digging into that in
more detail.
4.5.1 Single-variable chain rule
Let's start with the solution to the derivative of our nested expression:d
dxsin(x2) = 2xcos(x2).
It
doesn't take a mathematical genius to recognize components of the solution that smack of scalar
dierentiation rules,d
dxx2= 2xandd
dusin(u) =cos(u).
It looks like the solution is to multiply
the derivative of the outer expression by the derivative of the inner expression or \chain the pieces
together,"" which is exactly right.
In this section, we'll explore the general principle at work and
provide a process that works for highly-nested expressions of a single variable.
Chain rules are typically dened in terms of nested functions, such as y=f(g(x)) for single-variable
chain rules.
(You will also see the chain rule dened using function composition ( fg)(x), which
is the same thing.)
Some sources write the derivative using shorthand notation y0=f0(g(x))g0(x),
but that hides the fact that we are introducing an intermediate variable: u=g(x), which we'll see
shortly.
It's better to dene the single-variable chain rule of f(g(x)) explicitly so we never take the
derivative with respect to the wrong variable.
Here is the formulation of the single-variable chain
rule we recommend:
dy
dx=dy
dudu
dx
To deploy the single-variable chain rule, follow these steps:
141.
Introduce intermediate variables for nested subexpressions and subexpressions for both binary
and unary operators; e.g., is binary,sin(x) and other trigonometric functions are usually
unary because there is a single operand.
This step normalizes all equations to single operators
or function applications.
2.
Compute derivatives of the intermediate variables with respect to their parameters.
3.
Combine all derivatives of intermediate variables by multiplying them together to get the
overall result.
4.
Substitute intermediate variables back in if any are referenced in the derivative equation.
The third step puts the \chain"" in \chain rule"" because it chains together intermediate results.
Multiplying the intermediate derivatives together is the common theme among all variations of the
chain rule.
Let's try this process on y=f(g(x)) =sin(x2):
1.
Introduce intermediate variables.
Let u=x2represent subexpression x2(shorthand for
u(x) =x2).
This gives us:
u=x2(relative to denition f(g(x));g(x) =x2)
y=sin(u) (y=f(u) =sin(u))
The order of these subexpressions does not aect the answer, but we recommend working in
the reverse order of operations dictated by the nesting (innermost to outermost).
That way,
expressions and derivatives are always functions of previously-computed elements.
2.
Compute derivatives.
du
dx= 2x (Take derivative with respect to x)
dy
du=cos(u) (Take derivative with respect to unotx)
3.
Combine.
dy
dx=dy
dudu
dx=cos(u)2x
4.
Substitute.
dy
dx=dy
dudu
dx=cos(x2)2x= 2xcos(x2)
Notice how easy it is to compute the derivatives of the intermediate variables in isolation!
The
chain rule says it's legal to do that and tells us how to combine the intermediate results to get
2xcos(x2).
You can think of the combining step of the chain rule in terms of units canceling.
If we let ybe miles,
xbe the gallons in a gas tank, and uas gallons we can interpretdy
dx=dy
dudu
dxasmiles
tank=miles
gallongallon
tank.
Thegallon denominator and numerator cancel.
15Another way to to think about the single-variable chain rule is to visualize the overall expression
as a dataow diagram or chain of operations (or abstract syntax tree for compiler people):
Changes to function parameter xbubble up through a squaring operation then through a sin
operation to change result y.
You can think ofdu
dxas \getting changes from xtou"" anddy
duas
\getting changes from utoy.""
Getting from xtoyrequires an intermediate hop.
The chain rule
is, by convention, usually written from the output variable down to the parameter(s),dy
dx=dy
dudu
dx.
But, thex-to-yperspective would be more clear if we reversed the ow and used the equivalent
dy
dx=du
dxdy
du.
Conditions under which the single-variable chain rule applies .
Notice that there is a single
dataow path from xto the rooty.
Changes in xcan inuence output yin only one way.
That is the
condition under which we can apply the single-variable chain rule.
An easier condition to remember,
though one that's a bit looser, is that none of the intermediate subexpression functions, u(x) and
y(u), have more than one parameter.
Consider y(x) =x+x2, which would become y(x;u) =x+u
after introducing intermediate variable u.
As we'll see in the next section, y(x;u) has multiple
paths from xtoy.
To handle that situation, we'll deploy the single-variable total-derivative chain
rule.
As an aside for those interested in automatic dierentiation, papers and library documentation use
terminology forward dierentiation and backward dierentiation (for use in the back-propagation
algorithm).
From a dataow perspective, we are computing a forward dierentiation because it
follows the normal data ow direction.
Backward dierentiation, naturally, goes the other direc-
tion and we're asking how a change in the output would aect function parameter x.
Because
backward dierentiation can determine changes in all function parameters at once, it turns out to
be much more ecient for computing the derivative of functions with lots of parameters.
Forward
dierentiation, on the other hand, must consider how a change in each parameter, in turn, aects
the function output y.
The following table emphasizes the order in which partial derivatives are
computed for the two techniques.
Forward dierentiation from xtoyBackward dierentiation from ytox
dy
dx=du
dxdy
dudy
dx=dy
dudu
dx
Automatic dierentiation is beyond the scope of this article, but we're setting the stage for a future
article.
Many readers can solved
dxsin(x2) in their heads, but our goal is a process that will work even for
very complicated expressions.
This process is also how automatic dierentiation works in libraries
like PyTorch.
So, by solving derivatives manually in this way, you're also learning how to dene
functions for custom neural networks in PyTorch.
16With deeply nested expressions, it helps to think about deploying the chain rule the way a compiler
unravels nested function calls like f4(f3(f2(f1(x)))) into a sequence (chain) of calls.
The result of
calling function fiis saved to a temporary variable called a register, which is then passed as a
parameter to fi+1.
Let's see how that looks in practice by using our process on a highly-nested
equation like y=f(x) =ln(sin(x3)2):
1.
Introduce intermediate variables.
u1=f1(x) =x3
u2=f2(u1) =sin(u1)
u3=f3(u2) =u2
2
u4=f4(u3) =ln(u3)(y=u4)
2.
Compute derivatives.
d
uxu1=d
xx3= 3x2
d
u1u2=d
u1sin(u1) =cos(u1)
d
u2u3=d
u2u2
2 = 2u2
d
u3u4=d
u3ln(u3) =1
u3
3.
Combine four intermediate values.
dy
dx=du4
dx=du4
du3du3
du2du2
du1du1
dx=1
u32u2cos(u1)3x2=6u2x2cos(u1)
u3
4.
Substitute.
dy
dx=6sin(u1)x2cos(x3)
u2
2=6sin(x3)x2cos(x3)
sin(u1)2=6sin(x3)x2cos(x3)
sin(x3)2=6x2cos(x3)
sin(x3)
Here is a visualization of the data ow through the chain of operations from xtoy:
At this point, we can handle derivatives of nested expressions of a single variable, x, using the
chain rule but only if xcan aectythrough a single data ow path.
To handle more complicated
expressions, we need to extend our technique, which we'll do next.
174.5.2 Single-variable total-derivative chain rule
Our single-variable chain rule has limited applicability because all intermediate variables must be
functions of single variables.
But, it demonstrates the core mechanism of the chain rule, that of
multiplying out all derivatives of intermediate subexpressions.
To handle more general expressions
such asy=f(x) =x+x2, however, we need to augment that basic chain rule.
Of course, we immediately seedy
dx=d
dxx+d
dxx2= 1 + 2x, but that is using the scalar addition
derivative rule, not the chain rule.
If we tried to apply the single-variable chain rule, we'd get
the wrong answer.
In fact, the previous chain rule is meaningless in this case because derivative
operatord
dxdoes not apply to multivariate functions, such as u2among our intermediate variables:
u1(x) =x2
u2(x;u 1) =x+u1 (y=f(x) =u2(x;u 1))
Let's try it anyway to see what happens.
If we pretend thatdu2
du1= 0 + 1 = 1 anddu1
dx= 2x, then
dy
dx=du2
dx=du2
du1du1
dx= 2xinstead of the right answer 1 + 2 x.
Becauseu2(x;u) =x+u1has multiple parameters, partial derivatives come into play.
Let's blindly
apply the partial derivative operator to all of our equations and see what we get:
@u1(x)
@x= 2x (same asdu1(x)
dx)
@u2(x;u1)
@u1=@
@u1(x+u1) = 0 + 1 = 1
@u2(x;u1)
@x
@
@x(x+u1) = 1 + 0 = 1 (something's not quite right here!)
Ooops!
The partial@u2(x;u1)
@xis wrong because it violates a key assumption for partial derivatives.
When taking the partial derivative with respect to x, the other variables must not vary as xvaries.
Otherwise, we could not act as if the other variables were constants.
Clearly, though, u1(x) =x2
is a function of xand therefore varies with x.
@u2(x;u1)
@x6= 1 + 0 because@u1(x)
@x6= 0.
A quick look
at the data ow diagram for y=u2(x;u 1) shows multiple paths from xtoy, thus, making it clear
we need to consider direct and indirect (through u1(x)) dependencies on x:
A change in xaectsyboth as an operand of the addition and as the operand of the square
operator.
Here's an equation that describes how tweaks to xaect the output:
^y= (x+ x) + (x+ x)2
Then, y= ^y y, which we can read as \the change in yis the dierence between the original y
andyat a tweaked x.""
18If we letx= 1, theny= 1+12= 2.
If we bump xby 1, x= 1, then ^y= (1+1)+(1+1)2= 2+4 = 6.
The change in yis not 1, as @u2=u1would lead us to believe, but 6  2 = 4!
Enter the \law"" of total derivatives, which basically says that to computedy
dx, we need to sum up
all possible contributions from changes in xto the change in y.
The total derivative with respect to
xassumes all variables, such as u1in this case, are functions of xand potentially vary as xvaries.
The total derivative of f(x) =u2(x;u 1) that depends on xdirectly and indirectly via intermediate
variableu1(x) is given by:
dy
dx=@f(x)
@x=@u2(x;u 1)
@x=@u2
@x@x
@x+@u2
@u1@u1
@x=@u2
@x+@u2
@u1@u1
@x
Using this formula, we get the proper answer:
dy
dx=@f(x)
@x=@u2
@x+@u2
@u1@u1
@x= 1 + 12x= 1 + 2x
That is an application of what we can call the single-variable total-derivative chain rule :
@f(x;u 1;:::;u n)
@x=@f
@x+@f
@u1@u1
@x+@f
@u2@u2
@x+:::+@f
@un@un
@x=@f
@x+nX
i=1@f
@ui@ui
@x
The total derivative assumes all variables are potentially codependent whereas the partial derivative
assumes all variables but xare constants.
There is something subtle going on here with the notation.
All of the derivatives are shown as
partial derivatives because fanduiare functions of multiple variables.
This notation mirrors
that of MathWorld's notation but diers from Wikipedia, which uses df(x;u 1;:::;u n)=dxinstead
(possibly to emphasize the total derivative nature of the equation).
We'll stick with the partial
derivative notation so that it's consistent with our discussion of the vector chain rule in the next
section.
In practice, just keep in mind that when you take the total derivative with respect to x, other
variables might also be functions of xso add in their contributions as well.
The left side of the
equation looks like a typical partial derivative but the right-hand side is actually the total derivative.
It's common, however, that many temporary variables are functions of a single parameter, which
means that the single-variable total-derivative chain rule degenerates to the single-variable chain
rule.
Let's look at a nested subexpression, such as f(x) =sin(x+x2).
We introduce three intermediate
variables:
u1(x) =x2
u2(x;u 1) =x+u1
u3(u2) =sin(u2) (y=f(x) =u3(u2))
and partials:
@u1
@x= 2x
@u2
@x=@x
@x+@u2
@u1@u1
@x= 1 + 12x = 1 + 2x
@f(x)
@x=@u3
@x+@u3
@u2@u2
@x= 0 +cos(u2)@u2
@x=cos(x+x2)(1 + 2x)
19where both@u2
@xand@f(x)
@xhave@ui
@xterms that take into account the total derivative.
Also notice that the total derivative formula always sums versus, say, multiplies terms@f
@ui@ui
@x.
It's tempting to think that summing up terms in the derivative makes sense because, for example,
y=x+x2adds two terms.
Nope.
The total derivative is adding terms because it represents a
weighted sum of all xcontributions to the change in y.
For example, given y=xx2instead
ofy=x+x2, the total-derivative chain rule formula still adds partial derivative terms.
( xx2
simplies to x3but for this demonstration, let's not combine the terms.)
Here are the intermediate
variables and partial derivatives:
u1(x) =x2
u2(x;u 1) =xu1 (y=f(x) =u2(x;u 1))
@u1
@x= 2x
@u2
@x=u1 (foru2=x+u1;@u2
@x= 1)
@u2
@u1=x (foru2=x+u1;@u2
@u1= 1)
The form of the total derivative remains the same, however:
dy
dx=@u2
@x+@u2
@u1du1
@x=u1+x2x=x2+ 2x2= 3x2
It's the partials (weights) that change, not the formula, when the intermediate variable operators
change.
Those readers with a strong calculus background might wonder why we aggressively introduce
intermediate variables even for the non-nested subexpressions such as x2inx+x2.
We use this
process for three reasons: (i) computing the derivatives for the simplied subexpressions is usually
trivial, (ii) we can simplify the chain rule, and (iii) the process mirrors how automatic dierentiation
works in neural network libraries.
Using the intermediate variables even more aggressively, let's see how we can simplify our single-
variable total-derivative chain rule to its nal form.
The goal is to get rid of the@f
@xsticking out on
the front like a sore thumb:
@f(x;u 1;:::;u n)
@x=@f
@x+nX
i=1@f
@ui@ui
@x
We can achieve that by simply introducing a new temporary variable as an alias for x:un+1=x.
Then, the formula reduces to our nal form:
@f(u1;:::;u n+1)
@x=n+1X
i=1@f
@ui@ui
@x
This chain rule that takes into consideration the total derivative degenerates to the single-variable
chain rule when all intermediate variables are functions of a single variable.
Consequently, you
can remember this more general formula to cover both cases.
As a bit of dramatic foreshadowing,
notice that the summation sure looks like a vector dot product,@f
@u@u
@x, or a vector multiply@f
@u@u
@x.
Before we move on, a word of caution about terminology on the web.
Unfortunately, the chain rule
given in this section, based upon the total derivative, is universally called \multivariable chain rule""
20in calculus discussions, which is highly misleading!
Only the intermediate variables are multivariate
functions.
The overall function, say, f(x) =x+x2, is a scalar function that accepts a single
parameterx.
The derivative and parameter are scalars, not vectors, as one would expect with a
so-called multivariate chain rule.
(Within the context of a non-matrix calculus class, \multivariate
chain rule"" is likely unambiguous.)
To reduce confusion, we use \single-variable total-derivative
chain rule"" to spell out the distinguishing feature between the simple single-variable chain rule,
dy
dx=dy
dudu
dx, and this one.
4.5.3 Vector chain rule
Now that we've got a good handle on the total-derivative chain rule, we're ready to tackle the
chain rule for vectors of functions and vector variables.
Surprisingly, this more general chain rule
is just as simple looking as the single-variable chain rule for scalars.
Rather than just presenting
the vector chain rule, let's rediscover it ourselves so we get a rm grip on it.
We can start by
computing the derivative of a sample vector function with respect to a scalar, y=f(x), to see if
we can abstract a general formula.
y1(x)
y2(x)
=f1(x)
f2(x)
=ln(x2)
sin(3x)
Let's introduce two intermediate variables, g1andg2, one for each fiso thatylooks more like
y=f(g(x)):
g1(x)
g2(x)
=x2
3x
f1(g)
f2(g)
=ln(g1)
sin(g2)
The derivative of vector ywith respect to scalar xis a vertical vector with elements computed
using the single-variable total-derivative chain rule:
@y
@x=""
@f1(g)
@x@f2(g)
@x#
=""@f1
@g1@g1
@x+@f1
@g2@g2
@x
@f2
@g1@g1
@x+@f2
@g2@g2
@x#
=1
g12x+ 0
0 +cos(g2)3
=2x
x2
3cos(3x)
=2
x
3cos(3x)
Ok, so now we have the answer using just the scalar rules, albeit with the derivatives grouped into
a vector.
Let's try to abstract from that result what it looks like in vector form.
The goal is to
convert the following vector of scalar operations to a vector operation.
""@f1
@g1@g1
@x+@f1
@g2@g2
@x
@f2
@g1@g1
@x+@f2
@g2@g2
@x#
If we split the@fi
@gj@gj
@xterms, isolating the@gj
@xterms into a vector, we get a matrix by vector
multiplication:
""@f1
@g1@f1
@g2@f2
@g1@f2
@g2#@g1
@x@g2
@x
=@f
@g@g
@x
21That means that the Jacobian is the multiplication of two other Jacobians, which is kinda cool.
Let's check our results:
@f
@g@g
@x=1
g10
0cos(g2)2x
3
=1
g12x+ 0
0 +cos(g2)3
=2
x
3cos(3x)
Whew!
We get the same answer as the scalar approach.
This vector chain rule for vectors of
functions and a single parameter appears to be correct and, indeed, mirrors the single-variable
chain rule.
Compare the vector rule:
@
@xf(g(x)) =@f
@g@g
@x
with the single-variable chain rule:
d
dxf(g(x)) =df
dgdg
dx
To make this formula work for multiple parameters or vector x, we just have to change xto vector
xin the equation.
The eect is that@g
@xand the resulting Jacobian,@f
@x, are now matrices instead
of vertical vectors.
Our complete vector chain rule is:
@
@xf(g(x)) =@f
@g@g
@x(Note: matrix multiply doesn't commute; order of@f
@g@g
@xmatters)
The beauty of the vector formula over the single-variable chain rule is that it automatically takes
into consideration the total derivative while maintaining the same notational simplicity.
The Ja-
cobian contains all possible combinations of fiwith respect to gjandgiwith respect to xj.
For
completeness, here are the two Jacobian components in their full glory:
@
@xf(g(x)) =2
6664@f1
@g1@f1
@g2:::@f1
@gk@f2
@g1@f2
@g2:::@f2
@gk
:::
@fm
@g1@fm
@g2:::@fm
@gk3
77752
6664@g1
@x1@g1
@x2:::@g1
@xn@g2
@x1@g2
@x2:::@g2
@xn
:::
@gk
@x1@gk
@x2:::@gk
@xn3
7775
wherem=jfj,n=jxj, andk=jgj.
The resulting Jacobian is mn(anmkmatrix multiplied
by aknmatrix).
Even within this@f
@g@g
@xformula, we can simplify further because, for many applications, the Jaco-
bians are square ( m=n) and the o-diagonal entries are zero.
It is the nature of neural networks
that the associated mathematics deals with functions of vectors not vectors of functions.
For ex-
ample, the neuron ane function has term sum(w
x) and the activation function is max(0;x);
we'll consider derivatives of these functions in the next section.
As we saw in a previous section, element-wise operations on vectors wandxyield diagonal matrices
with elements@wi
@xibecausewiis a function purely of xibut notxjforj6=i.
The same thing happens
here whenfiis purely a function of giandgiis purely a function of xi:
@f
@g=diag(@fi
@gi)
@g
@x=diag(@gi
@xi)
22In this situation, the vector chain rule simplies to:
@
@xf(g(x)) =diag(@fi
@gi)diag(@gi
@xi) =diag(@fi
@gi@gi
@xi)
Therefore, the Jacobian reduces to a diagonal matrix whose elements are the single-variable chain
rule values.
After slogging through all of that mathematics, here's the payo.
All you need is the vector chain
rule because the single-variable formulas are special cases of the vector chain rule.
The following
table summarizes the appropriate components to multiply in order to get the Jacobian.
scalar
xvector
x
@
@xf(g(x)) =@f
@g@g
@xscalar
uvector
uvector
u
scalar
f@f
@u@u
@x@f
@u@u
@x@f
@u@u
@x
vector
f@f
@u@u
@x@f
@u@u
@x@f
@u@u
@x
5 The gradient of neuron activation
We now have all of the pieces needed to compute the derivative of a typical neuron activation for
a single neural network computation unit with respect to the model parameters, wandb:
activation (x) =max(0;wx+b)
(This represents a neuron with fully connected weights and rectied linear unit activation.
There
are, however, other ane functions such as convolution and other activation functions, such as
exponential linear units, that follow similar logic.)
Let's worry about max later and focus on computing@
@w(wx+b) and@
@b(wx+b).
(Recall that
neural networks learn through optimization of their weights and biases.)
We haven't discussed the
derivative of the dot product yet, y=f(w)g(x), but we can use the chain rule to avoid having to
memorize yet another rule.
(Note notation ynotyas the result is a scalar not a vector.)
The dot product wxis just the summation of the element-wise multiplication of the elements:Pn
i(wixi) =sum(w
x).
(You might also nd it useful to remember the linear algebra notation
23wx=wTx.)
We know how to compute the partial derivatives of sum(x) and w
xbut haven't
looked at partial derivatives for sum(w
x).
We need the chain rule for that and so we can
introduce an intermediate vector variable ujust as we did using the single-variable chain rule:
u=w
x
y=sum(u)
Once we've rephrased y, we recognize two subexpressions for which we already know the partial
derivatives:
@u
@w=@
@w(w
x) =diag(x)
@y
@u=@
@usum(u) =~1T
The vector chain rule says to multiply the partials:
@y
@w=@y
@u@u
@w=~1Tdiag(x) =xT
To check our results, we can grind the dot product down into a pure scalar function:
y =wx =Pn
i(wixi)
@y
@wj=@
@wjP
i(wixi) =P
i@
@wj(wixi) =@
@wj(wjxj) =xj
Then:
@y
@w= [x1;:::;x n] =xT
Hooray!
Our scalar results match the vector chain rule results.
Now, lety=wx+b, the full expression within the max activation function call.
We have two
dierent partials to compute, but we don't need the chain rule:
@y
@w=@
@wwx+@
@wb=xT+~0T=xT
@y
@b=@
@bwx+@
@bb= 0 + 1 = 1
Let's tackle the partials of the neuron activation, max(0;wx+b).
The use of the max(0;z)
function call on scalar zjust says to treat all negative zvalues as 0.
The derivative of the max
function is a piecewise function.
When z0, the derivative is 0 because zis a constant.
When
z>0, the derivative of the max function is just the derivative of z, which is 1:
@
@zmax(0;z) =(
0z0
dz
dz= 1z>0
An aside on broadcasting functions across scalars.
When one or both of the max arguments are
vectors, such as max(0;x), we broadcast the single-variable function max across the elements.
This
is an example of an element-wise unary operator.
Just to be clear:
max(0;x) =2
664max(0;x1)
max(0;x2)
:::
max(0;xn)3
775
24For the derivative of the broadcast version then, we get a vector of zeros and ones where:
@
@ximax(0;xi) =(
0xi0
dxi
dxi= 1xi>0
@
@xmax(0;x) =2
6664@
@x1max(0;x1)
@
@x2max(0;x2)
:::
@
@xnmax(0;xn)3
7775
To get the derivative of the activation (x) function, we need the chain rule because of the nested
subexpression, wx+b.
Following our process, let's introduce intermediate scalar variable zto
represent the ane function giving:
z(w;b;x) =wx+b
activation (z) =max(0;z)
The vector chain rule tells us:
@activation
@w=@activation
@z@z
@w
which we can rewrite as follows:
@activation
@w=(
0@z
@w=~0Tz0
1@z
@w=@z
@w=xTz>0 (we computed@z
@w=xTpreviously)
and then substitute z=wx+bback in:
@activation
@w=(~0Twx+b0
xTwx+b>0
That equation matches our intuition.
When the activation function clips ane function output z
to 0, the derivative is zero with respect to any weight wi.
Whenz >0, it's as if the max function
disappears and we get just the derivative of zwith respect to the weights.
Turning now to the derivative of the neuron activation with respect to b, we get:
@activation
@b=(
0@z
@b= 0 wx+b0
1@z
@b= 1 wx+b>0
Let's use these partial derivatives now to handle the entire loss function.
6 The gradient of the neural network loss function
Training a neuron requires that we take the derivative of our loss or \cost"" function with respect to
the parameters of our model, wandb.
Because we train with multiple vector inputs (e.g., multiple
images) and scalar targets (e.g., one classication per image), we need some more notation.
Let
X= [x1;x2;:::;xN]T
25whereN=jXj, and then let
y= [target (x1);target (x2);:::;target (xN)]T
whereyiis a scalar.
Then the cost equation becomes:
C(w;b;X; y) =1
NNX
i=1(yi activation (xi))2=1
NNX
i=1(yi max(0;wxi+b))2
Following our chain rule process introduces these intermediate variables:
u(w;b;x) =max(0;wx+b)
v(y;u) =y u
C(v) =1
NPN
i=1v2
Let's compute the gradient with respect to wrst.
6.1 The gradient with respect to the weights
From before, we know:
@
@wu(w;b;x) =(~0Twx+b0
xTwx+b>0
and
@v(y;u)
@w=@
@w(y u) =~0T @u
@w= @u
@w=(~0Twx+b0
 xTwx+b>0
Then, for the overall gradient, we get:
@C(v)
@w=@
@w1
NNX
i=1v2
=1
NNX
i=1@
@wv2
=1
NNX
i=1@v2
@v@v
@w
=1
NNX
i=12v@v
@w
=1
NNX
i=1(
2v~0T=~0Twxi+b0
 2vxTwxi+b>0
26=1
NNX
i=1(~0Twxi+b0
 2(yi u)xT
iwxi+b>0
=1
NNX
i=1(~0Twxi+b0
 2(yi max(0;wxi+b))xT
iwxi+b>0
=1
NNX
i=1(~0Twxi+b0
 2(yi (wxi+b))xT
iwxi+b>0
=(~0Twxi+b0
 2
NPN
i=1(yi (wxi+b))xT
iwxi+b>0
=(~0Twxi+b0
2
NPN
i=1(wxi+b yi)xT
iwxi+b>0
To interpret that equation, we can substitute an error term ei=wxi+b yiyielding:
@C
@w=2
NNX
i=1eixT
i(for the nonzero activation case)
From there, notice that this computation is a weighted average across all xiinX.
The weights are
the error terms, the dierence between the target output and the actual neuron output for each xi
input.
The resulting gradient will, on average, point in the direction of higher cost or loss because
largeeiemphasize their associated xi.
Imagine we only had one input vector, N=jXj= 1, then
the gradient is just 2 e1xT
1.
If the error is 0, then the gradient is zero and we have arrived at the
minimum loss.
If e1is some small positive dierence, the gradient is a small step in the direction
ofx1.
Ife1is large, the gradient is a large step in that direction.
If e1is negative, the gradient is
reversed, meaning the highest cost is in the negative direction.
Of course, we want to reduce, not increase, the loss, which is why the gradient descent recurrence
relation takes the negative of the gradient to update the current position (for scalar learning rate
):
wt+1=wt @C
@w
Because the gradient indicates the direction of higher cost, we want to update xin the opposite
direction.
6.2 The derivative with respect to the bias
To optimize the bias, b, we also need the partial with respect to b.
Here are the intermediate
variables again:
u(w;b;x) =max(0;wx+b)
v(y;u) =y u
C(v) =1
NPN
i=1v2
27We computed the partial with respect to the bias for equation u(w;b;x) previously:
@u
@b=(
0wx+b0
1wx+b>0
Forv, the partial is:
@v(y;u)
@b=@
@b(y u) = 0 @u
@b= @u
@b=(
0wx+b0
 1wx+b>0
And for the partial of the cost function itself we get:
@C(v)
@b=@
@b1
NNX
i=1v2
=1
NNX
i=1@
@bv2
=1
NNX
i=1@v2
@v@v
@b
=1
NNX
i=12v@v
@b
=1
NNX
i=1(
0 wx+b0
 2vwx+b>0
=1
NNX
i=1(
0 wx+b0
 2(yi max(0;wxi+b))wx+b>0
=1
NNX
i=1(
0 wx+b0
2(wxi+b yi)wx+b>0
=(
0 wxi+b0
2
NPN
i=1(wxi+b yi)wxi+b>0
As before, we can substitute an error term:
@C
@b=2
NNX
i=1ei(for the nonzero activation case)
The partial derivative is then just the average error or zero, according to the activation level.
To
update the neuron bias, we nudge it in the opposite direction of increased cost:
bt+1=bt @C
@b
28In practice, it is convenient to combine wandbinto a single vector parameter rather than having
to deal with two dierent partials: ^w= [wT;b]T. This requires a tweak to the input vector xas
well but simplies the activation function.
By tacking a 1 onto the end of x,^x= [xT;1],wx+b
becomes ^w^x.
This nishes o the optimization of the neural network loss function because we have the two
partials necessary to perform a gradient descent.
7 Summary
Hopefully you've made it all the way through to this point.
You're well on your way to understand-
ing matrix calculus!
We've included a reference that summarizes all of the rules from this article
in the next section.
Also check out the annotated resource link below.
Your next step would be to learn about the partial derivatives of matrices not just vectors.
For
example, you can take a look at the matrix dierentiation section of Matrix calculus.
Acknowledgements .
We thank Yannet Interian (Faculty in MS data science program at Univer-
sity of San Francisco) and David Uminsky (Faculty/director of MS data science) for their help with
the notation presented here.
8 Matrix Calculus Reference
8.1 Gradients and Jacobians
The gradient of a function of two variables is a horizontal 2-vector:
rf(x;y) = [@f(x;y)
@x;@f(x;y)
@y]
The Jacobian of a vector-valued function that is a function of a vector is an mn(m=jfjand
n=jxj) matrix containing all possible scalar partial derivatives:
@y
@x=2
664rf1(x)
rf2(x)
:::
rfm(x)3
775=2
664@
@xf1(x)
@
@xf2(x)
:::
@
@xfm(x)3
775=2
6664@
@x1f1(x)@
@x2f1(x):::@
@xnf1(x)
@
@x1f2(x)@
@x2f2(x):::@
@xnf2(x)
:::
@
@x1fm(x)@
@x2fm(x):::@
@xnfm(x)3
7775
The Jacobian of the identity function f(x) =xisI.
298.2 Element-wise operations on vectors
Dene generic element-wise operations on vectors wandxusing operatorsuch as +:
2
6664y1
y2
...
yn3
7775=2
6664f1(w)g1(x)
fn(w)g2(x)
...
fn(w)gn(x)3
7775
The Jacobian with respect to w(similar for x) is:
Jw=@y
@w=2
6664@
@w1(f1(w)g1(x))@
@w2(f1(w)g1(x)):::@
@wn(f1(w)g1(x))
@
@w1(f2(w)g2(x))@
@w2(f2(w)g2(x)):::@
@wn(f2(w)g2(x))
:::
@
@w1(fn(w)gn(x))@
@w2(fn(w)gn(x)):::@
@wn(fn(w)gn(x))3
7775
Given the constraint ( element-wise diagonal condition ) thatfi(w) andgi(x) access at most wiand
xi, respectively, the Jacobian simplies to a diagonal matrix:
@y
@w=diag@
@w1(f1(w1)g1(x1));@
@w2(f2(w2)g2(x2)); :::;@
@wn(fn(wn)gn(xn))
Here are some sample element-wise operators:
Op Partial with respect to w Partial with respect to x
+@(w+x)
@w=I@(w+x)
@x=I
 @(w x)
@w=I@(w x)
@x= I

@(w
x)
@w=diag(x)@(w
x)
@x=diag(w)
@(wx)
@w=diag(:::1
xi:::)@(wx)
@x=diag(::: wi
x2
i:::)
8.3 Scalar expansion
Adding scalar zto vector x,y=x+z, is really y=f(x) +g(z) where f(x) =xandg(z) =~1z.
@
@x(x+z) =diag(~1) =I
@
@z(x+z) =~1
Scalar multiplication yields:
@
@x(xz) =Iz
@
@z(xz) =x
308.4 Vector reductions
The partial derivative of a vector sum with respect to one of the vectors is:
rxy=@y
@x=h
@y
@x1;@y
@x2;:::;@y
@xni
=hP
i@fi(x)
@x1;P
i@fi(x)
@x2; :::;P
i@fi(x)
@xni
Fory=sum(x):
rxy=~1T
Fory=sum(xz) andn=jxj, we get:
rxy= [z;z;:::;z ]
rzy=sum(x)
Vector dot product y=f(w)g(x) =Pn
i(wixi) =sum(w
x).
Substituting u=w
xand using
the vector chain rule, we get:
du
dx=d
dx(w
x) =diag(w)
dy
du=d
dusum(u) =~1T
dy
dx=dy
dudu
dx=~1Tdiag(w) =wT
Similarly,dy
dw=xT.
8.5 Chain rules
The vector chain rule is the general form as it degenerates to the others.
When fis a function
of a single variable xand all intermediate variables uare functions of a single variable, the single-
variable chain rule applies.
When some or all of the intermediate variables are functions of multiple
variables, the single-variable total-derivative chain rule applies.
In all other cases, the vector chain
rule applies.
Single-variable rule Single-variable total-derivative rule Vector rule
d f
dx=d f
dudu
dx@f(u1;:::;u n)
@x=@f
@u@u
@x@
@xf(g(x)) =@f
@g@g
@x
9 Notation
Lowercase letters in bold font such as xare vectors and those in italics font like xare scalars.
xi
is theithelement of vector xand is in italics because a single vector element is a scalar.
jxjmeans
\length of vector x.""
TheTexponent of xTrepresents the transpose of the indicated vector.
Pb
i=axiis just a for-loop that iterates ifromatob, summing all the xi.
31Notationf(x) refers to a function called fwith an argument of x.
Irepresents the square \identity matrix"" of appropriate dimensions that is zero everywhere but
the diagonal, which contains all ones.
diag(x) constructs a matrix whose diagonal elements are taken from vector x.
The dot product wxis the summation of the element-wise multiplication of the elements:Pn
i(wixi) =sum(w
x).
Or, you can look at it as wTx.
Dierentiationd
dxis an operator that maps a function of one parameter to another function.
That
means thatd
dxf(x) mapsf(x) to its derivative with respect to x, which is the same thing asd f(x)
dx.
Also, ify=f(x), thendy
dx=d f(x)
dx=d
dxf(x).
The partial derivative of the function with respect to x,@
@xf(x), performs the usual scalar derivative
holding all other variables constant.
The gradient of fwith respect to vector x,rf(x), organizes all of the partial derivatives for a
specic scalar function.
The Jacobian organizes the gradients of multiple functions into a matrix by stacking them:
J=rf1(x)
rf2(x)
The following notation means that yhas the value auponcondition 1and valuebuponcondition 2.
y=(
a condition 1
b condition 2
10 Resources
Wolfram Alpha can do symbolic matrix algebra and there is also a cool dedicated matrix calculus
dierentiator.
When looking for resources on the web, search for \matrix calculus"" not \vector calculus.""
Here
are some comments on the top links that come up from a Google search:
https://en.wikipedia.org/wiki/Matrix calculus The Wikipedia entry is actually quite good
and they have a good description of the dierent layout conventions.
Recall that we use the
numerator layout where the variables go horizontally and the functions go vertically in the
Jacobian.
Wikipedia also has a good description of total derivatives, but be careful that they
use slightly dierent notation than we do.
We always use the @xnotation not dx.
http://www.ee.ic.ac.uk/hp/sta/dmb/matrix/calculus.html This page has a section on ma-
trix dierentiation with some useful identities; this person uses numerator layout.
This might
32be a good place to start after reading this article to learn about matrix versus vector dier-
entiation.
https://www.colorado.edu/engineering/CAS/courses.d/IFEM.d/IFEM.AppC.d/IFEM.AppC.pdf
This is part of the course notes for \Introduction to Finite Element Methods"" I believe by Car-
los A. Felippa.
His Jacobians are transposed from our notation because he uses denominator
layout.
http://www.ee.ic.ac.uk/hp/sta/dmb/matrix/calculus.html This page has a huge number of
useful derivatives computed for a variety of vectors and matrices.
A great cheat sheet.
There
is no discussion to speak of, just a set of rules.
https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf Another cheat sheet that fo-
cuses on matrix operations in general with more discussion than the previous item.
https://www.comp.nus.edu.sg/~cs5240/lecture/matrix-dierentiation.pdf A useful set of slides.
To learn more about neural networks and the mathematics behind optimization and back propa-
gation, we highly recommend Michael Nielsen's book.
For those interested specically in convolutional neural networks, check out A guide to convolution
arithmetic for deep learning.
We reference the law of total derivative, which is an important concept that just means derivatives
with respect to xmust take into consideration the derivative with respect xof all variables that
are a function of x.
33"
99,95c543b8fe00c9b1adbed90ddbc9e22abacf34b8ecc804f58850ed67a54b4cc7-1,deeplearning,dense_x_retieval_what_retrieval_granularity_shoud_we_use.pdf,"DenseXRetrieval: What Retrieval Granularity Should We Use?
Tong Chen♣∗Hongwei Wang♢Sihao Chen♡Wenhao Yu♢
Kaixin Ma♢Xinran Zhao♠Hongming Zhang♢Dong Yu♢
♣University of Washington♢Tencent AI Lab
♡University of Pennsylvania♠Carnegie Mellon University
Abstract
Dense retrieval has become a prominent
method to obtain relevant context or world
knowledge in open-domain NLP tasks.
When
we use a learned dense retriever on a retrieval
corpus at inference time, an often-overlooked
design choice is the retrieval unit in which the
corpus is indexed, e.g.
document, passage, or
sentence.
We discover that the retrieval unit
choice significantly impacts the performance
of both retrieval and downstream tasks.
Dis-
tinct from the typical approach of using pas-
sages or sentences, we introduce a novel re-
trieval unit, proposition , for dense retrieval.
Propositions are defined as atomic expressions
within text, each encapsulating a distinct fac-
toid and presented in a concise, self-contained
natural language format.
We conduct an empir-
ical comparison of different retrieval granular-
ity.
Our results reveal that proposition-based
retrieval significantly outperforms traditional
passage or sentence-based methods in dense
retrieval.
Moreover, retrieval by proposition
also enhances the performance of downstream
QA tasks, since the retrieved texts are more
condensed with question-relevant information,
reducing the need for lengthy input tokens and
minimizing the inclusion of extraneous, irrele-
vant information.
1 Introduction
Dense retrievers are a popular class of techniques
for accessing external information sources for
knowledge-intensive tasks (Karpukhin et al., 2020).
Before we use a learned dense retriever to retrieve
from a corpus, an imperative design decision we
have to make is the retrieval unit – i.e.
the granu-
larity at which we segment and index the retrieval
∗Work was done during internship at Tencent AI Lab,
Bellevue.
Sentence
RetrievalPrior to restoration work performed be-
tween 1990 and 2001, the tower leaned at
an angle of 5.5 degrees, but thetower now
leans atabout 3.99 degrees.
Proposition
RetrievalThe Lean ingTower ofPisa now leans at
about 3.99 degrees.
Contriever GTR3040506070Recall@5 (%)Passage Retrieval
Contriever GTR010203040EM@100 (%)Question AnsweringPassage Sentence Proposition
Figure 1: ( Top) An example of three granularities of
retrieval units of Wikipedia text when using dense re-
trieval.
( Bottom ) We observe that retrieving by proposi-
tions yields the best retrieval performance in both pas-
sage retrieval task and downstream open-domain QA
task, e.g.
with Contriever (Izacard et al., 2022) or GTR
(Ni et al., 2022) as the backbone retriever.
High light
indicates the part that contains answer to the question.
corpus for inference.
In practice, the choice of re-
trieval unit, e.g.
documents, fixed-length passage
chunks or sentences, etc, is usually pre-determined
based on how the dense retrieval model is instanti-
ated or trained (Lewis et al., 2020; Lee et al., 2021a;
Santhanam et al., 2022; Ni et al., 2022).
In this paper, we investigate an overlooked re-
search question with dense retrieval inference – at
what retrieval granularity should we segment and
index the retrieval corpus?
We discover that se-
lecting the proper retrieval granularity at inference
time can be a simple yet effective strategy for im-arXiv:2312.06648v2  [cs.CL]  12 Dec 20231.
✓AnswerQueryQA ModelRetrieval Units
Retrieval Units
Passage RetrievalBC
DProposition-izer
?QueryCorpus
PassagesSentencesPropositionsRetrieval UnitsRetrieverAFigure 2: We discover that segmenting and indexing a retrieval corpus on the proposition level can be a simple yet
effective strategy to increase dense retrievers’ generalization performance at inference time (A, B) .
We empirically
compare the retrieval and downstream open-domain QA tasks performance when dense retrievers work with
Wikipedia indexed at the level of 100-word passage, sentence or proposition (C, D) .
proving dense retrievers’ retrieval and downstream
task performance.
We illustrate our intuition with
an example of open-domain question-answering
(QA) in Table 1.
The example shows retrieved text
by the same model at three different granularities.
Thepassage , which represents a coarser retrieval
unit with a longer context, is theoretically able to
provide more relevant information for the ques-
tion.
However, a passage often includes extraneous
details (e.g., restoration period and horizontal dis-
placement in the example of Table 1) that could po-
tentially distract both the retriever and the language
model in downstream tasks (Shi et al., 2023; Yu
et al., 2023b).
On the other hand, sentence -level in-
dexing provides a finer-grained approach but does
not entirely address the issue (Akkalyoncu Yilmaz
et al., 2019; Yang et al., 2020).
This is because sen-
tences can still be complex and compounded, and
they are often not self-contained, lacking necessary
contextual information (e.g., in the example of Ta-
ble 1, “the tower” is coreference of “Pisa Tower”)
for judging the query-document relevance.
To address these shortcomings of typical re-
trieval units such as passages or sentences, we
propose using proposition as a novel retrieval unit
for dense retrieval.
Propositions are defined as
atomic expressions within text, each encapsulat-
ing a distinct factoid and presented in a concise,
self-contained natural language format.
We show
an example proposition in Table 1.
We provide a more detailed definition
and description of proposition in §2.To validate the efficacy of using proposition as
a retrieval unit for dense retrievers inference, we
first process and index an English Wikipedia dump
with all documents segmented into propositions,
which we refer to as FACTOID WIKI.
Then we con-
duct experiments on five different open-domain QA
datasets and empirically compare the performance
of six dual-encoder retrievers when Wikipedia is
indexed by passage, sentence, and our proposed
proposition.
Our evaluation is twofold: we exam-
ine both the retrieval performance and the impact
on downstream QA tasks.
Notably, our findings in-
dicate that proposition-based retrieval outperforms
sentence and passage-based methods, especially in
terms of generalization, as discussed in §5.
This
suggests that propositions, being both compact and
rich in context, enable dense retrievers to access
precise information while maintaining adequate
context.
The average improvement over passage-
based retrieval of Recall@20 is +10.1 on unsu-
pervised dense retrievers and +2.2 on supervised
retrievers.
Furthermore, we observe a distinct ad-
vantage in downstream QA performance when us-
ing proposition-based retrieval, as elaborated in §6.
Given the often limited input token length in lan-
guage models, propositions inherently provide a
higher density of question-relevant information.
Our main contributions in the paper are:
•We propose using propositions as retrieval units
when indexing a retrieval corpus to improve the
dense retrieval performance.
•We introduce FACTOID WIKI, a processed En-
glish Wikipedia dump, where each page is seg-
mented into multiple granularities: 100-word pas-
sages, sentences and propositions.•We discover that retrieval by proposition outper-
forms passage or sentence retrieval in terms of
generalization for passage retrieval and accuracy
for downstream question-answering given the
same input token limit.
2 Proposition as a Retrieval Unit
The goal of our study is to understand how the gran-
ularity of a retrieval corpus influences the dense
retrieval models’ performance empirically.
Aside
from commonly-used retrieval units such as 100-
word passage (Karpukhin et al., 2020) or sentence,
we propose using proposition as an alternative re-
trieval unit choice.
Here, propositions represent
atomic expressions of meanings in text (Min et al.,
2023) that are defined by the three principles below.
1.Each proposition should correspond to a distinct
piece of meaning in text, where the composition
of all propositions would represent the seman-
tics of the entire text.
2.A proposition should be minimal , i.e.
it cannot
be further split into separate propositions.
3.A proposition should be contextualized and self-
contained (Choi et al., 2021).
A proposition
should include all the necessary context from the
text (e.g.
coreference) to interpret its meaning.
The use of proposition as a retrieval unit is inspired
by a recent line of work (Min et al., 2023; Kamoi
et al., 2023; Chen et al., 2023a,b), which finds suc-
cess in representing and evaluating text semantics
at the level of propositions.
We demonstrate the
concept of proposition and how a passage can be
split into its set of propositions by an example on
the left side of Figure 2.
The passage contains
three propositions, each of which corresponds to
a distinct factoid about the Leaning Tower of Pisa :
the angle before the restoration, the current an-
gle, and the horizontal displacement.
Within each
proposition, necessary context from the passage is
incorporated so that the meaning of the proposition
can be interpreted independently of the original
text, e.g.
the reference of the tower is resolved into
its full mention, the Leaning Tower of Pisa , in the
first proposition.
We expect each proposition to de-
scribe exactly one contextualized atomic fact, and
so our intuition is that propositions would suitably
work as a retrieval unit for information-seeking
questions.3 F ACTOID WIKI: Proposition-Level
Index and Retrieval for Wikipedia
We empirically compare the use of 100-word pas-
sages, sentences, and propositions as retrieval units
on Wikipedia, a commonly-used retrieval source
for knowledge-intensive NLP tasks (Petroni et al.,
2021).
To allow for a fair comparison across gran-
ularities, we process an English Wikipedia dump
from 2021-10-13, as used by Bohnet et al.
We segment each document text into three different
granularities: 100-word passages, sentences, and
propositions.
We include the details on passage-
and sentence-level segmentation of the corpus in
Appendix A.
Parsing Passage to Propositions.
To segment
the Wikipedia pages into propositions, we finetune
a text generation model, which we refer to as the
Propositionizer .
The Propositionizer takes a pas-
sage as input and generates the list of propositions
within the passage.
(2023b),
we train the Propositionizer with a two-step distil-
lation process.
We first prompt GPT-4 (OpenAI,
2023) with an instruction containing the propo-
sition definition and 1-shot demonstrations.
We
include the details of the prompt in Figure 8.
We
start with a set of 42k passages and use GPT-4 to
generate the seed set of paragraph-to-propositions
pairs.
We refer to the processed corpus as FACTOID -
WIKI.
The resulting statistics of FACTOID WIKIare
shown in Table 1.
# words
Passage 41,393,528 58.5
Sentence 114,219,127 21.0
Proposition 256,885,003 11.2
Table 1: Statistics of text units in the English Wikipedia
dump from 2021-10-13.
4 Experimental Settings
To evaluate the impact of the three retrieval unit
choices, we conduct experiments on five differ-
ent open-domain QA datasets with FACTOID WIKI.
With each dataset, we evaluate both passage re-
trieval and downstream QA performance when
dense retrievers work with Wikipedia indexed in
different granularities.4.1 Open-Domain QA Datasets
We evaluate on five different open-domain QA
datasets with Wikipedia as the retrieval source:
Natural Questions (NQ) (Kwiatkowski et al.,
2019), TriviaQA (TQA) (Joshi et al., 2017),
Web Questions (WebQ) (Berant et al., 2013),
SQuAD (Rajpurkar et al., 2016), and Entity Ques-
tions (EQ) (Sciavolino et al., 2021).
4.2 Dense Retrieval Models
We compare the performance of the six following
supervised or unsupervised dense retriever mod-
els.
Here, supervised models refer to ones that
have used human-labeled query-passage pairs as
supervision during training, and vice versa.
•SimCSE (Gao et al., 2021) is a BERT-base (De-
vlin et al., 2019) encoder trained on unlabeled
sentence randomly sampled from Wikipedia.
•Contriever (Izacard et al., 2022) is an unsuper-
vised retriever, instantiated with a BERT-base
encoder.
Contriever is contrastively trained by
segment pairs constructed from unlabeled docu-
ments from Wikipedia and web crawl data.
•DPR (Karpukhin et al., 2020) is a dual-encoder
BERT-base model finetuned on five open-domain
QA datasets, which includes four of the datasets
(NQ, TQA, WebQ and SQuAD) in our evalua-
tion.
•ANCE (Xiong et al., 2020) used the same setting
from DPR and trained the model with Approxi-
mate nearest neighbor Negative Contrastive Esti-
mation (ANCE), a hard negatives-based training
approach.
•TAS-B (Hofstätter et al., 2021) is a dual-
encoder BERT-base model distilled from a
teacher model with cross-attention trained on MS
MARCO (Nguyen et al., 2016).
•GTR (Hofstätter et al., 2021) is a T5-base en-
coder (Raffel et al., 2020) pretrained on unla-
beled pairs of online forum QA data, and fine-
tuned on MS MARCO and Natural Question.
4.3 Passage Retrieval Evaluation
We evaluate retrieval performance at the passage
level when the corpus is indexed at the passage,
sentence, or proposition level respectively.
For sen-
tence and proposition level retrieval, we follow the
setting introduced in Lee et al.
(2021b), where the
score of the passage is based on the maximum sim-
ilarity score between the query and all sentencesor propositions in a passage.
In practice, we first
retrieve a slightly larger number of text units, map
each unit to the source passage, and return the top- k
unique passages.
We use Recall@ kas our evalu-
ation metric, which is defined as the percentage
of questions for which the correct answer is found
within the top- kretrieved passages.
4.4 Downstream QA Evaluation
To understand the implications of using different
retrieval units on the downstream open-domain
QA tasks, we evaluate the use of retrieval mod-
els in retrieve-then-read setup (Izacard and Grave,
2021).
With the retrieve-then-read setting, a re-
trieval model first retrieves ktext units given the
query.
The kretrieved text units are then used as
input along with the query to a reader model to
derive the final answer.
Typically, the choice of
kis subject to the reader model’s maximum input
length constraint, or the limit of compute budget,
which scales with the number of input tokens.
For this reason, we follow an evaluation setup
where the maximum number of retrieved words
is capped at l= 100 or500, i.e.
only the top l
words from passage, sentence, or proposition level
retrieval are feed into the reader model as input.
We
evaluate the percentage of questions for which the
predicted answer exactly matches (EM) the ground
truth.
We denote our metric as EM @ l. For our
evaluation, we use T5-large size UnifiedQA-v2 as
the reader model (Khashabi et al., 2022).
5 Results: Passage Retrieval
In this section, we report and discuss the retrieval
tasks performance.
Our results show that despite
none of the models being trained with proposition-
level data, all the retrieval models demonstrated
on-par or superior performance when the corpus is
indexed at the proposition level.
5.1 Passage Retrieval Performance
We report our evaluation results in Table 2.
We
observe that retrieval by propositions outperforms
retrieval by sentence or passage on most tasks for
both unsupervised and supervised retrievers.
With all dense retrievers tested, proposition-
level retrieval consistently outperforms sentence
and passage-level retrieval on average across the
five datasets.
With the unsupervised retrievers, i.e.
SimCSE and Contriever, we see an averaged Re-
call@5 improvement of +12.0and+9.3(35.0%Retriever GranularityNQ TQA WebQ SQuAD EQ Avg.
R@5 R@20 R@5 R@20 R@5 R@20 R@5 R@20 R@5 R@20 R@5 R@20
Unsupervised Dense Retrievers
SimCSE Passage 28.8 44.3 44.9 59.4 39.8 56.0 29.5 45.5 28.4 40.3 34.3 49.1
Sentence 35.5 53.1 50.5 64.3 45.3 64.1 37.1 52.3 36.3 50.1 40.9 56.8
Proposition 41.1 58.9 52.4 66.5 50.0 66.8 38.7 53.9 49.5 62.2 46.3 61.7
Contriever Passage 42.5 63.8 58.1 73.7 37.1 60.6 40.8 59.8 36.3 56.3 43.0 62.8
Sentence 46.4 66.8 60.6 75.7 41.7 63.1 45.1 63.5 42.7 61.3 47.3 66.1
Proposition 50.1 70.0 65.1 77.9 45.9 66.8 50.7 67.7 51.7 70.1 52.7 70.5
Supervised Dense Retrievers
DPR Passage 66.0 78.0 71.6 80.2 62.9 74.9 38.3 53.9 47.5 60.4 57.3 69.5
Sentence 66.0 78.0 71.8 80.5 64.1 74.4 40.3 55.9 53.7 66.0 59.2 71.0
Proposition 65.4 77.7 70.7 79.6 62.8 75.1 41.4 57.2 59.4 71.3 59.9 72.2
ANCE Passage 70.7 81.4 73.9 81.4 65.7 77.2 43.3 58.6 57.0 69.1 62.1 73.5
Sentence 70.3 81.6 73.9 81.5 65.2 77.4 45.8 60.7 61.4 72.8 63.3 74.8
Proposition 69.9 81.1 72.8 80.6 65.1 77.1 46.2 61.9 66.7 76.6 64.1 75.5
TAS-B Passage 64.2 77.9 70.4 79.3 65.1 77.0 54.3 69.2 72.2 81.3 65.2 76.9
Sentence 64.0 78.4 71.4 80.2 63.9 76.7 58.9 72.3 72.7 82.0 66.2 77.9
Proposition 63.8 78.6 71.4 80.0 63.8 76.8 59.8 73.4 75.1 83.3 66.8 78.4
GTR Passage 66.3 78.4 70.1 79.4 63.3 76.5 54.4 68.1 71.7 80.5 65.2 76.6
Sentence 66.4 79.4 71.6 80.9 62.2 76.8 60.9 73.4 72.5 81.3 66.7 78.4
Proposition 66.5 79.6 72.2 80.9 63.2 77.4 63.3 75.0 74.9 83.0 68.0 79.2
Table 2: Passage retrieval performance (Recall@ k= 5, 20) on five different open-domain QA datasets when
pre-trained dense retrievers work with the three different granularity from the retrieval corpus.
Underline denotes
cases where the training split of the target dataset was included in the training data of the dense retriever.
101102103
Popularity204060Recall@5
SimCSE
101102103
Popularity204060Recall@5
Contriever
101102103
Popularity405060Recall@5
DPR
101102103
Popularity40506070Recall@5
ANCE
101102103
Popularity50607080Recall@5
TAS-B
101102103
Popularity50607080Recall@5
GTRPassage Sentence Proposition
Figure 3: Document retrieval recall vs. the popularity of the target entity in each question from the EntityQuestions
dataset.
The popularity of each entity (i.e.
smaller value ⇒less common entities, and vice versa) is estimated by
the occurrence of the entity in its top-1000 passage retrieved by BM25.
On queries with less common entities, we
observe that retrieving by proposition shows a larger advantage over retrieval by proposition.
and 22.5% relative improvement) respectively over
five datasets.
With the supervised retrievers, proposition-level
retrieval still shows an advantage on average, yet
the sizes of improvements are smaller.
We hypothe-
size that this is due to these retrievers being trained
on query-passage pairs.
For instance, with DPR
and ANCE, which have been trained on NQ, TQA,
WebQ, and SQuAD, we observe that proposition
and sentence level retrieval perform slightly worse
compared to passage level on three out of the four
datasets, with the exception of SQuAD.
As shown
in Table 2, all supervised retrievers demonstrate
comparable performance across three levels of re-
trieval granularity in NQ, TQA, and WebQ.
However, on datasets that the retriever model has
notseen during training, we observe that retrievalby proposition demonstrates a clear advantage.
For
instance, most notably on SQuAD or EntityQues-
tions, we observe that proposition-based retrieval
significantly outperforms the other two granulari-
ties.
We see 17-25% Recall@5 relative improve-
ment on EntityQuestions with relatively weak re-
trievers like DPR and ANCE.
Furthermore, the
Recall@5 of retrieval by proposition on SQuAD
improved most on TAS-B and GTR, with 10-16%
relative improvements.
5.2 Retrieval by Proposition ⇒Better
Cross-Task Generalization
Our results indicate that the advantage of retrieval
by proposition becomes most visible in cross-
task generalization settings.
We observe that on
SQuAD and EntityQuestions, retrieval by proposi-Retriever GranularityNQ TQA WebQ SQuAD EQ Avg.
EM EM EM EM EM EM
@100 @500 @100 @500 @100 @500 @100 @500 @100 @500 @100 @500
Unsupervised Dense Retrievers
SimCSE Passage 8.1 16.3 22.6 33.7 7.7 14.9 9.8 17.8 10.9 17.5 11.8 20.0
Sentence 10.1 18.0 27.2 37.2 9.6 15.6 17.3 24.8 13.0 19.8 15.4 23.1
Proposition 12.7 20.2 28.4 37.7 11.2 17.2 18.0 25.1 18.3 25.0 17.7 25.0
Contriever Passage 11.1 22.4 25.7 41.4 6.8 14.9 15.6 27.7 10.9 21.5 14.0 25.6
Sentence 13.8 23.9 30.5 44.2 9.1 17.2 22.6 32.8 12.2 22.2 17.6 28.1
Proposition 16.5 26.1 37.7 48.7 13.3 19.9 25.6 34.4 16.1 27.3 21.8 31.3
Supervised Dense Retrievers
DPR Passage 24.8 36.1 40.3 51.0 14.0 22.2 12.4 21.7 18.6 25.9 22.0 31.4
Sentence 27.6 35.9 44.6 52.8 16.3 23.7 18.6 26.1 21.8 28.2 25.8 33.3
Proposition 28.3 34.3 45.7 51.9 19.0 23.8 19.8 26.3 26.3 31.9 27.8 33.6
ANCE Passage 27.1 38.3 43.1 53.1 15.2 23.0 15.3 26.0 23.4 31.1 24.8 34.3
Sentence 30.1 37.3 47.0 54.7 16.6 23.8 22.9 30.5 25.9 32.0 28.5 35.7
Proposition 29.8 37.0 47.4 53.5 19.3 24.1 22.9 30.1 29.1 33.7 29.7 35.7
TAS-B Passage 21.1 33.9 39.3 50.5 13.1 20.7 23.9 34.6 30.9 37.3 25.7 35.4
Sentence 24.6 33.9 43.6 52.3 14.4 21.4 33.8 40.5 31.4 36.1 29.6 36.8
Proposition 26.6 34.0 44.9 51.8 18.1 23.7 34.2 38.9 34.2 37.8 31.6 37.2
GTR Passage 23.4 34.5 38.7 49.3 13.1 20.1 23.9 33.8 31.3 36.7 26.1 34.9
Sentence 26.8 35.1 43.9 52.2 15.9 21.6 35.6 41.3 31.3 35.1 30.7 37.1
Proposition 29.5 34.4 45.9 52.6 18.7 23.8 37.0 40.4 34.1 37.1 33.0 37.7
Table 3: Open-domain QA performance (EM = Exact Match) under retrieve-then-read setting where the number of
retrieved words to the reader QA model is limited at l= 100 or500.
We use UnifedQA V2 (Khashabi et al., 2022)
as the reader model.
The first lwords from the concatenated top retrieved text unit are feed as input to the reader
model.
Underline denotes cases where the training split of the target dataset was included in the training data of the
dense retriever.
In most cases, we see better QA performance with smaller retrieval units.
tion brings more performance gain over retrieval
by passage.
To better understand where the improvements
can be attributed, we conduct an additional analysis
on EntityQuestions.
As EntityQuestions features
questions targeting the properties of longer-tail enti-
ties, we study how the retrieval performance under
three different granularities is affected by the popu-
larity of the target entity in question, i.e.
whether
the entity appears frequently in Wikipedia or not.
We estimate the popularity of each entity with the
following method.
Given the surface form of an en-
tity, we use BM25 to retrieve the top-1000 relevant
passages from Wikipedia.
We use the number of
occurrences of the entity in its top-1000 passages
as an estimate of its popularity.
With the 20,000
test queries in EntityQuestion, around 25% of the
target entities have a popularity value of less or
equal to 3.
Figure 3 shows the passage retrieval perfor-
mance vs. the popularity of the target entity in
each question.
Across all 6 dense retrievers, we ob-
serve that retrieving by proposition shows a much
larger advantage over retrieving by passage with
questions targeting less common entities.
As the
popularity of entities increases, the performancegap decreases.
Our findings indicate that the per-
formance gain from retrieval by proposition can
mostly be attributed to queries for long-tailed infor-
mation.
This echoes our observation that retrieval
by proposition improves the cross-task generaliza-
tion performance of dense retrievers.
6 Results: Open-Domain QA
In this section, we study how the choice of retrieval
granularity affects downstream open-domain QA
tasks.
We show that retrieval by proposition leads
to strong downstream QA performance in the
retrieve-then-read setting, where the number of re-
trieved tokens for input to the reader QA model is
capped at l= 100 or500words.
Across dif-
ferent retrievers, we observe higher QA perfor-
mance in terms of the EM@ lmetric on average
when using propositions as the retrieval unit.
The
unsupervised retrievers, SimCSE and Contriever,
demonstrate improvements of +5.9 and +7.8 in
the EM@100 score (50% and 55% relative im-
provement), respectively.
The supervised retriev-
ers, DPR, ANCE, TAS-B, and GTR, improve +5.8,0 200 400
#Words506070Recall (%)GTR / NQ
0 200 400
#Words6070Recall (%)GTR / TQA
0 200 400
#Words506070Recall (%)GTR / WebQ
0 200 400
#Words405060Recall (%)GTR / SQuAD
0 200 400
#Words607080Recall (%)GTR / EQPassage Sentence PropositionFigure 4: Recall of the gold answer in the retrieved text limited to first kwords for the GTR retriever.
Finer-grained
retrieval has a higher recall across all numbers of words.
Similar
to our observations from passage retrieval evalu-
ations, we find retrieval by proposition becomes
more beneficial to downstream QA performance
when the retriever has not been trained on the target
dataset.
In other cases, retrieval by proposition still
holds an advantage, but with a smaller margin on
average.
6.2 Propositions ⇒Higher Density of
Question-Related Information
Intuitively, compared to sentences or passages as
retrieval units, the advantage of propositions is that
the retrieved propositions have a higher density
of relevant information to the query.
With finer-
grained retrieval units, the correct answer to the
query would more likely appear in the top- lre-
trieved words by a dense retriever.
Here, we investigate the posi-
tion at which the ground truth answer appears in
the top- lretrieved words.
Specifically, we calcu-
late the recall of the gold answer within the initial l
retrieved words with GTR working with Wikipedia
indexed in three different granularities.
We show the results in Figure 4 and Figure 7 with
lranging from 0 to 500 across all five datasets.
For
a fixed retrieved word budget, proposition retrieval
demonstrates a higher success rate compared to
sentence and passage retrieval methods.
The most
significant improvement of proposition retrieval
over passage retrieval occurs within the range of
100-200 words, which corresponds to roughly 10
propositions, 5 sentences, or 2 passages.
As the
word count further increases, the recall rates of
the three granularity converge since all question-
relevant information is included in the retrieved
text.6.3 Error Case Study
To understand the source of errors from each type
of retrieval granularity, we present and discuss four
typical examples of mistakes in Table 4.
With each
example, we show the question and its correspond-
ing top-1 retrieved text unit by the GTR retriever
across the three granularities.
We observe that with passage-level retrieval, the
ambiguity of an entity or its references presents a
challenge for dense retrievers, which echoes find-
ings from (Min et al., 2020).
For instance, in exam-
ple Q1, the question asks for “ Super Bowl 50 ”, but
the retrieved passage and sentence refers to “ Super
Bowl 5 ”.
In Example Q2, passage retrieval fails
to identify the part referring to the correct “ atomic
number ”.
Instead, the top-1 retrieved passage men-
tions “ atomic number ” in a different and irrelevant
context to the question.
Retrieval by sentences can
also have a similar problem as retrieval by passages
like Example Q1.
Also, retrieval by sentences faces
another challenge of lacking context.
In Example
Q3, sentence-based retrieval fails as the correct sen-
tence in the retrieved passage uses “ it” to refer to
the pericardial sac.
Retrieval by propositions tackles the aforemen-
tioned problems by ensuring each retrieval unit
contains one piece of fact only and necessary con-
text is incorporated in the propositions.
However,
proposition-based retrieval faces challenges with
questions that involve multi-hop reasoning over
long-range textual analysis.
In Example Q4, the
retrieved passage separately describes the actor’s
name and the character they portray.
There is not
a single proposition that entails both the question
and the answer.
7 Related Work
Recent works on dense retrievers typically adopt
a dual-encoder architecture (Yih et al., 2011;
Reimers and Gurevych, 2019; Karpukhin et al.,
2020; Ni et al., 2022).
With dual-encoders,Passage Retrieval Sentence Retrieval Proposition Retrieval
Q1: What was the theme of Super Bowl 50?
Title: Super Bowl X ✗
The overall theme of the Super Bowl enter-
tainment was to celebrate the United States
Bicentennial.
Table 4: Example cases where top-1 retrieved text unit of each retrieval granularity fails to provide the correct
answer.
The gray text is the context of propositions, but it is for illustration
purpose only and not provided to the retrievers and downstream QA models.
each query and document is encoded into a low-
dimensional feature vector respectively, and their
relevance is measured by a non-parametric similar-
ity function between the embedding vectors (Muss-
mann and Ermon, 2016).
Due to the limited expres-
sivity from the similarity function, dual encoder
models often generalize poorly to new tasks with
scarce training data (Thakur et al., 2021).
To this
end, previous studies use techniques such as data
augmentation (Wang et al., 2022; Yu et al., 2023a;
Izacard et al., 2022; Gao and Callan, 2022; Lin
et al., 2023; Dai et al., 2023), continual pre-training
(Chang et al., 2020; Sachan et al., 2021; Oguz et al.,
2022), task-aware training (Xin et al., 2022; Cheng
et al., 2023), hybrid sparse-dense retrieval (Luan
et al., 2021; Chen et al., 2022) or mixed strategy re-
trieval (Ma et al., 2022, 2023) and so on to improve
cross-task generalization performance of dense re-
trievers.
The motivation of our work echoes in part with
multi-vector retrieval, e.g.
ColBERT (Khattab and
Zaharia, 2020), DensePhrase (Lee et al., 2021a,b),
ME-BERT (Luan et al., 2021), MVR (Zhang et al.,
2022), where the retrieval model learns to encodea candidate retrieval unit into multiple vectors to
increase model expressivity and improve retrieval
granularity (Seo et al., 2019; Humeau et al., 2019).
Our work instead focuses on the setting where we
do not update the dense retriever model or its pa-
rameters.
We show that segmenting the retrieval
corpus into finer-grained units of proposition can
be a simple and orthogonal strategy for improving
the generalization of dual encoders dense retrievers
at inference time.
The idea of using propositions as a unit of text
representation dates back to the Pyramid method
in summarization evaluation (Nenkova and Passon-
neau, 2004), where model generated summary is
evaluated by each proposition.
Proposition extrac-
tion from text has been a long-standing task in NLP,
with earlier formulations focusing on a structured
representation of propositions, e.g.
Open Infor-
mation Extraction (Etzioni et al., 2008), Semantic
Role Labeling (Gildea and Jurafsky, 2000).
More
recent studies have found success in extracting
propositions in natural language form via few-shot
prompting with large language models (Min et al.,
2023; Kamoi et al., 2023), or finetuning smallercompact-sized models (Chen et al., 2023b).
Retrieve-then-read , or more broadly – retrieval
augmented generation, has recently merged as a
popular paradigm for open-domain question an-
swering (Lewis et al., 2021; Jiang et al., 2023;
Asai et al., 2023).
While earlier works provide
up to the top 100 retrieved passages for the down-
stream reader (Izacard and Grave, 2021; Kedia
et al., 2022), the amount of allowed context is
significantly reduced when using recent large lan-
guage models (e.g.
top 10) (Touvron et al., 2023;
Yu et al., 2023b), due to their limited context win-
dow length and inability to reason over long con-
text (Liu et al., 2023).
Recent efforts have tried to
improve the quality of the reader context by filter-
ing or compressing the retrieved documents (Wang
et al., 2023; Xu et al., 2023).
Our work offers a
new perspective by leveraging a new retrieval unit,
the proposition that not only reduces the context
length but also offers greater information density,
effectively addressing the issue.
8 Conclusion
We propose the use of propositions as retrieval units
for indexing corpus to improve dense retrieval per-
formance at inference time.
Through our experi-
ments on five open-domain QA datasets with six
different dense retrievers, we discovered that re-
trieval by proposition outperforms passage or sen-
tence in both passage retrieval accuracy and down-
stream QA performance with a fixed retrieved word
budget.
We introduce FACTOID WIKI, an indexed
version of the English Wikipedia dump, where text
from 6 million pages is segmented into 250 million
propositions.
We hope that FACTOID WIKI, along
with our findings in the paper, will facilitate future
research on information retrieval.
Limitations
The scope of our current study on the granular-
ity of retrieval corpus has the following limita-
tions.
(1) Retrieval Corpus – Our study only focus
on Wikipedia as the retrieval corpus, due to the
fact that most open-domain QA datasets adopts
Wikipedia as the retrieval corpus.
(2) Types of
dense retrievers evaluated – In the current version
of the paper, we only evaluate on 6 types of popular
dense retrievers, most of which follow bi- or dual-
encoder architecture.
In future versions, we will
include and discuss results on a broader range of
dense retrievers.
(3) Language – Our current studyis limited to English Wikipedia only.
We leave the
exploration on other languages to future work.
Cross-domain modeling of
sentence-level evidence for document retrieval.
In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 3490–
3496, Hong Kong, China.
Association for Computa-
tional Linguistics.
Self-rag: Learning to re-
trieve, generate, and critique through self-reflection.
Semantic parsing on freebase from
question-answer pairs.
In Proceedings of the 2013
conference on empirical methods in natural language
processing , pages 1533–1544.
Attributed question answering: Evaluation and
modeling for attributed large language models.
Pre-training tasks
for embedding-based large-scale retrieval.
In Inter-
national Conference on Learning Representations .
PropSegmEnt: A
large-scale corpus for proposition-level segmentation
and entailment recognition.
In Findings of the As-
sociation for Computational Linguistics: ACL 2023 ,
pages 8874–8893, Toronto, Canada.
Association for
Computational Linguistics.
Sub-sentence en-
coder: Contrastive learning of propositional semantic
representations.
Salient phrase aware dense retrieval: Can a dense
retriever imitate a sparse one?
In Findings of the
Association for Computational Linguistics: EMNLP
2022 , pages 250–262, Abu Dhabi, United Arab Emi-
rates.
Association for Computational Linguistics.
Task-aware specialization for efficient
and robust dense retrieval for open-domain questionanswering.
In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers) , pages 1864–1875, Toronto,
Canada.
Association for Computational Linguistics.
Decontextualization: Making sen-
tences stand-alone.
Transactions of the Association
for Computational Linguistics , 9:447–461.
Scaling instruction-finetuned language models.
Promptagator: Few-
shot dense retrieval from 8 examples.
In The Eleventh
International Conference on Learning Representa-
tions .
BERT: Pre-training of
deep bidirectional transformers for language under-
standing.
In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota.
Association for
Computational Linguistics.
Open information extrac-
tion from the web.
Unsupervised cor-
pus aware language model pre-training for dense pas-
sage retrieval.
In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 2843–2853,
Dublin, Ireland.
Association for Computational Lin-
guistics.
Simcse: Simple contrastive learning of sentence em-
beddings.
Automatic
labeling of semantic roles.
In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics , pages 512–520, Hong Kong.
Association
for Computational Linguistics.
Ef-
ficiently teaching an effective dense retriever with
balanced topic aware sampling.
In Proceedings of
the 44th International ACM SIGIR Conference on
Research and Development in Information Retrieval ,
pages 113–122.Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,
and Jason Weston.
Poly-encoders: Trans-
former architectures and pre-training strategies for
fast and accurate multi-sentence scoring.
Unsupervised dense informa-
tion retrieval with contrastive learning.
Transactions
on Machine Learning Research .
Leveraging
passage retrieval with generative models for open do-
main question answering.
In Proceedings of the 16th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics: Main Volume ,
pages 874–880, Online.
Association for Computa-
tional Linguistics.
Active retrieval
augmented generation.
Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion.
Wice: Real-world entailment for
claims in wikipedia.
In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing .
Dense passage retrieval for open-
domain question answering.
In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 6769–6781,
Online.
Association for Computational Linguistics.
FiE: Building a global probability space by leverag-
ing early fusion in encoder for open-domain question
answering.
In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Process-
ing, pages 4246–4260, Abu Dhabi, United Arab Emi-
rates.
Association for Computational Linguistics.
Unifiedqa-v2: Stronger generalization
via broader cross-format training.
Colbert: Effi-
cient and effective passage search via contextualized
late interaction over bert.
In Proceedings of the 43rd
International ACM SIGIR conference on research
and development in Information Retrieval , pages 39–
48.
Natural questions: a benchmark
for question answering research.
Transactions of the
Association for Computational Linguistics , 7:453–
466.
Learning to select question-
relevant relations for visual question answering.
In
Proceedings of the Third Workshop on Multimodal
Artificial Intelligence , pages 87–96, Mexico City,
Mexico.
Association for Computational Linguistics.
Phrase retrieval learns passage retrieval, too.
In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing , pages 3661–
3672, Online and Punta Cana, Dominican Republic.
Association for Computational Linguistics.
Retrieval-augmented generation
for knowledge-intensive nlp tasks.
Advances in Neu-
ral Information Processing Systems , 33:9459–9474.
Retrieval-augmented generation for knowledge-
intensive nlp tasks.
How to Train Your DRAGON: Di-
verse Augmentation Towards Generalizable Dense
Retrieval.
In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing .
Lost in the middle: How language
models use long contexts.
Sparse, dense, and attentional
representations for text retrieval.
Transactions of the
Association for Computational Linguistics , 9:329–
345.
Open-domain question an-
swering via chain of reasoning over heterogeneous
knowledge.
In Findings of the Association for Com-
putational Linguistics: EMNLP 2022 , pages 5360–
5374, Abu Dhabi, United Arab Emirates.
Association
for Computational Linguistics.
Chain-of-skills:
A configurable model for open-domain question an-
swering.
In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 1599–1618, Toronto,
Canada.
Association for Computational Linguistics.Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike
Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,
Luke Zettlemoyer, and Hannaneh Hajishirzi.
FActScore: Fine-grained atomic evaluation of factual
precision in long form text generation.
In Proceed-
ings of the 2023 Conference on Empirical Methods
in Natural Language Processing .
AmbigQA: Answering am-
biguous open-domain questions.
In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP) , pages 5783–
5797, Online.
Association for Computational Lin-
guistics.
Learning
and inference via maximum inner product search.
InInternational Conference on Machine Learning ,
pages 2587–2596.
Evaluat-
ing content selection in summarization: The pyramid
method.
In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics:
HLT-NAACL 2004 , pages 145–152, Boston, Mas-
sachusetts, USA.
Association for Computational Lin-
guistics.
MS MARCO: A human gener-
ated machine reading comprehension dataset.
Large dual encoders are generalizable retrievers.
In
Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing , pages
9844–9855, Abu Dhabi, United Arab Emirates.
As-
sociation for Computational Linguistics.
Domain-matched
pre-training tasks for dense retrieval.
In Findings
of the Association for Computational Linguistics:
NAACL 2022 , pages 1524–1534, Seattle, United
States.
Association for Computational Linguistics.
OpenAI.
KILT: a benchmark for knowledge
intensive language tasks.
In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: HumanLanguage Technologies , pages 2523–2544, Online.
Association for Computational Linguistics.
Exploring the limits
of transfer learning with a unified text-to-text trans-
former.
The Journal of Machine Learning Research ,
21(1):5485–5551.
Squad: 100,000+ questions
for machine comprehension of text.
Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks.
In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
3982–3992, Hong Kong, China.
Association for Com-
putational Linguistics.
End-to-end training
of neural retrievers for open-domain question answer-
ing.
In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers) , pages
6648–6662, Online.
Association for Computational
Linguistics.
Col-
BERTv2: Effective and efficient retrieval via
lightweight late interaction.
In Proceedings of the
2022 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 3715–3734, Seat-
tle, United States.
Association for Computational
Linguistics.
Simple entity-centric ques-
tions challenge dense retrievers.
Real-time open-domain question answering with
dense-sparse phrase index.
In Proceedings of the
57th Annual Meeting of the Association for Computa-
tional Linguistics , pages 4430–4441, Florence, Italy.
Association for Computational Linguistics.
Large language models can
be easily distracted by irrelevant context.
In Inter-
national Conference on Machine Learning , pages
31210–31227.
Beir:
A heterogeneous benchmark for zero-shot evaluation
of information retrieval models.
In Thirty-fifth Con-
ference on Neural Information Processing Systems
Datasets and Benchmarks Track (Round 2) .
Llama 2: Open foundation and fine-
tuned chat models.
GPL: Generative pseudo labeling
for unsupervised domain adaptation of dense retrieval.
InProceedings of the 2022 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 2345–2360, Seattle, United States.
Association
for Computational Linguistics.
Learning to filter
context for retrieval-augmented generation.
Zero-
shot dense retrieval with momentum adversarial do-
main invariant representations.
In Findings of the As-
sociation for Computational Linguistics: ACL 2022 ,
pages 4008–4020, Dublin, Ireland.
Association for
Computational Linguistics.
Approximate nearest neighbor neg-
ative contrastive learning for dense text retrieval.
Re-
comp: Improving retrieval-augmented lms with com-
pression and selective augmentation.
2020.Multilingual universal sentence encoder for semantic
retrieval.
In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations , pages 87–94.
Learning discriminative
projections for text similarity measures.
In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning , pages 247–256, Port-
land, Oregon, USA.
Association for Computational
Linguistics.
Generate
rather than retrieve: Large language models are
strong context generators.
In The Eleventh Inter-
national Conference on Learning Representations .
Chain-of-
note: Enhancing robustness in retrieval-augmented
language models.
Multi-view document repre-
sentation learning for open-domain dense retrieval.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 5990–6000, Dublin, Ireland.
Association for Computational Linguistics.A Retrieval Corpus Processing
The English Wikipedia dump used in this study,
released by Bohnet et al., 2022, was selected be-
cause it has been filtered to remove figures, tables,
and lists, and is organized into paragraphs.
We have
segmented Wikipedia into three retrieval units for
this study: 100-word passage chunks, sentences,
and propositions.
Paragraphs are divided into 100-
word passage chunks using a greedy method.
We
divide only at the end of sentences to ensure each
passage chunk contains complete sentences.
As
we process the paragraph, we add sentences one
by one.
If including the next sentence causes the
passage chunk to exceed 100 words, we start a
new passage chunk with that sentence.
However,
if the final passage chunk is shorter than 50 words,
we merge it with the previous one to avoid overly
small segments.
Each passage is further segmented
into sentences using the widely used Python SpaCy
en_core_web_lg model.
Additionally, each
passage is decomposed into propositions by our
Propositionizer model.
We decomposed 6 million
pages into 41 million passages, 114 million sen-
tences, and 257 million propositions.
On average,
a passage contains 6.3 propositions, and a sentence
contains 2.3 propositions.
B Training the Propositionizer
We generated a list of propositions from a given
paragraph using GPT-4 with a prompt, as shown in
Figure 8.
We named the
model Propositionizer.
The AdamW optimizer was
used with a batch size of 64, learning rate of 1e-4,
weight decay of 1e-4, and 3 epochs.
To compare the proposition generation perfor-
mance of different models, we set up a development
set and an evaluation metric.
The development set
contains an additional 1,000 pairs collected by GPT-
4 using the same approach as the training set.
We
evaluated the quality of the predicted propositions
by the F1 score of two sets of propositions.
Mo-
tivated by the F1 score of two sets of tokens in
BertScore, we designed the F1 score for two sets of
propositions.
Let P={p1, ..., p n}denote the set
of labeled propositions and ˆP={ˆp1, ...,ˆpm}the
set of predicted propositions.
We use sim(pi,ˆpj)
to represent the similarity between two proposi-
tions.
Theoretically, any text similarity metric can
be used.
We chose BertScore with roberta-largeconfiguration as our simfunction since we wanted
our metric to reflect the semantic difference be-
tween propositions.
We define
Recall =1
|P|X
pi∈Pmax
ˆpj∈ˆPsim(pi,ˆpj)
Precision =1
|ˆP|X
ˆpj∈ˆPmax
pi∈Psim(pi,ˆpj)
F1 = 2 ·Precision ·Recall
Precision + Recall
Here is a figurative explanation of the F1 score:
Recall represents the percentage of propositions
in the labeled set that are similar to those in the
generated set, Precision represents the percentage
of propositions in the generated set that are similar
to the labeled set, and F1is the harmonic mean of
Recall andPrecision .F1is 1 if the two sets are
exactly the same, and 0 if any two propositions are
semantically different.
We conducted a comparative analysis of base-
size and large-size Flan-T5 models, which were
trained using varying amounts of data (shown in
Figure 5).
Our findings suggest that larger models,
coupled with extensive training data, yield better
results.
The Propositionizer presented in this paper
attained an F1 score of 0.822.
Upon manually
reviewing the generated propositions, we found
them to be satisfactory.
5000 7500 10000 12500 15000 17500
Number of training samples767778798081F1
flan-t5-base flan-t5-large
Figure 5: Performance of proposition-level decompo-
sition by models with different sizes and number of
training data.
C Offline Indexing
We used the pyserini andfaiss packages
to encode retrieval units into embeddings.
We
exploited multiple GPUs to encode each text
unit in groups of 1M units with a batch size
of 64.
After preprocessing the embeddings,
we used an exact search for the inner product(faiss.IndexFlatIP ) in all experiments.
The
plain index of FACTOID WIKIis approximately
768GB in size.
To reduce memory pressure, the
embeddings are split into 8 shards.
An approximate
nearest neighbor search is conducted per shard be-
fore aggregating all results.
Although the number of propositions is six times
that of passages, using efficient indexing tech-
niques can enable sub-linear search times relative
to the total count of vectors.
Moreover, utilizing
GPU parallelism and distributed indexes signifi-
cantly decreases the online search time.
As a result,
with proper implementation, we can make propo-
sition retrieval a practically viable and efficient
option.
D Retrievers Models
We used transformers andsentence-tra
nsformers packages for the model implementa-
tion.
We used the following checkpoints released
on HuggingFace: SimCSE ( princeton-nlp/u
nsup-simcse-bert-base-uncased ), Con-
triever ( facebook/contriever ), DPR ( fac
ebook/dpr-ctx_encoder-multiset-ba
se,facebook/dpr-question_encoder-
multiset-base ), ANCE ( castorini/anc
e-dpr-context-multi ,castorini/anc
e-dpr-question-multi , ), TAS-B ( sente
nce-transformers/msmarco-distilbe
rt-base-tas-b ), and GTR ( sentence-tra
nsformers/gtr-t5-base ).
E Additional Results
In Section 5.2, we demonstrated the advantage
of retrieval by proposition over retrieval by sen-
tence, particularly as the population of the entity
decreases in EQ.
We used the occurrence in the
top-1000 paragraphs retrieved by BM25 as a proxy
for popularity, rather than counting the number of
hyperlinks to the entity used in Sciavolino et al.,
2021.
Therefore, the trend in the performance ver-
sus popularity plot shows some differences (Fig-
ure 6) between our results and those in Sciavolino
et al., 2021.
For example, some entities are am-
biguous (e.g., 1992 , a TV series).
In such cases,
the occurrence of the surface form of the entity is
large.
Simultaneously, questions related to ambigu-
ous entities are challenging to answer, leading to
lower recall.
In Section 6.2, we discussed the recall of an-
swers in the retrieved text with respect to the con-text length.
We further illustrate the performance
trends of six dense retrievers, as detailed in Fig-
ure 7.
The results indicate that the recall rate of
propositions consistently outperforms that of sen-
tences and passages.
Our findings lead to the con-
clusion that question-related density is greater in
proposition units compared to sentences and pas-
sages.101102
Popularity204060Recall@5
SimCSE
101102
Popularity30405060Recall@5
Contriever
101102
Popularity4060Recall@5
DPR
101102
Popularity506070Recall@5
ANCE
101102
Popularity7080Recall@5
TAS-B
101102
Popularity7080Recall@5
GTRPassage Sentence Proposition(a) Where was [X] born?
101102
Popularity204060Recall@5
SimCSE
101102
Popularity204060Recall@5
Contriever
101102
Popularity406080Recall@5
DPR
101102
Popularity6080Recall@5
ANCE
101102
Popularity708090Recall@5
TAS-B
101102
Popularity708090Recall@5
GTRPassage Sentence Proposition
(b) Who was [X] created by?
Figure 6: Document retrieval recall vs. the popularity of the target entity in each question from the EntityQuestions
dataset.
We display the performance of two relations.0 200 400
#Words203040Recall (%)SimCSE / NQ
0 200 400
#Words304050Recall (%)SimCSE / TQA
0 200 400
#Words304050Recall (%)SimCSE / WebQ
0 200 400
#Words203040Recall (%)SimCSE / SQuAD
0 200 400
#Words304050Recall (%)SimCSE / EQPassage Sentence Proposition
0 200 400
#Words20304050Recall (%)Contriever / NQ
0 200 400
#Words40506070Recall (%)Contriever / TQA
0 200 400
#Words20304050Recall (%)Contriever / WebQ
0 200 400
#Words304050Recall (%)Contriever / SQuAD
0 200 400
#Words20304050Recall (%)Contriever / EQPassage Sentence Proposition
0 200 400
#Words506070Recall (%)DPR / NQ
0 200 400
#Words6070Recall (%)DPR / TQA
0 200 400
#Words506070Recall (%)DPR / WebQ
0 200 400
#Words3040Recall (%)DPR / SQuAD
0 200 400
#Words405060Recall (%)DPR / EQPassage Sentence Proposition
0 200 400
#Words506070Recall (%)ANCE / NQ
0 200 400
#Words6070Recall (%)ANCE / TQA
0 200 400
#Words506070Recall (%)ANCE / WebQ
0 200 400
#Words304050Recall (%)ANCE / SQuAD
0 200 400
#Words506070Recall (%)ANCE / EQPassage Sentence Proposition
0 200 400
#Words40506070Recall (%)TAS-B / NQ
0 200 400
#Words6070Recall (%)TAS-B / TQA
0 200 400
#Words506070Recall (%)TAS-B / WebQ
0 200 400
#Words405060Recall (%)TAS-B / SQuAD
0 200 400
#Words607080Recall (%)TAS-B / EQPassage Sentence Proposition
0 200 400
#Words506070Recall (%)GTR / NQ
0 200 400
#Words6070Recall (%)GTR / TQA
0 200 400
#Words506070Recall (%)GTR / WebQ
0 200 400
#Words405060Recall (%)GTR / SQuAD
0 200 400
#Words607080Recall (%)GTR / EQPassage Sentence PropositionFigure 7: Recall of the gold answer in the retrieved text limited to first kwords.
Finer-grained retrieval has a higher
recall across all numbers of words.Passage ⇒Propositions
Decompose the ""Content"" into clear and simple propositions, ensuring they are interpretable out of
context.
1.Split compound sentence into simple sentences.
Maintain the original phrasing from the input
whenever possible.
2.For any named entity that is accompanied by additional descriptive information, separate this
information into its own distinct proposition.
3.Decontextualize the proposition by adding necessary modifier to nouns or entire sentences
and replacing pronouns (e.g., ""it"", ""he"", ""she"", ""they"", ""this"", ""that"") with the full name of the
entities they refer to.
Present the results as a list of strings, formatted in JSON.
]
Input : <a new passage >
Output :
Figure 8: Prompt for generating propositions from a passage using GPT-4."
100,95c543b8fe00c9b1adbed90ddbc9e22abacf34b8ecc804f58850ed67a54b4cc7-2,deeplearning,dense_x_retieval_what_retrieval_granularity_shoud_we_use.pdf,"(2022).
2019.
2023.
2013.
2022.
2020.
2023a.
2023b.
2022.
2023.
2021.
2022.
2023.
2019.
2008.
2022.
2021.
2000.
2021.
2019.
2022.
2021.
2023.
2017.
2023.
2020.
2022.
2022.
2020.
2019.
2021a.
2021b.
2020.
2021.
2023.
2023.
2021.
2022.
2023.
2023.
2020.
2016.
2004.
2016.
2022.
2022.
2023.
2021.
2020.
2016.
2019.
2021.
2022.
2021.
2019.
2023.
2021.
2023.
2022.
2023.
2022.
2020.
2023.
2011.
2023a.
2023b.
2022.
The
dump dates back to October 13, 2021."
101,95c543b8fe00c9b1adbed90ddbc9e22abacf34b8ecc804f58850ed67a54b4cc7-3,deeplearning,dense_x_retieval_what_retrieval_granularity_shoud_we_use.pdf,"/githubhttps://github.com/ct123098/
factoid-wikiQuestion: What is the angle of the Tower of Pisa?
Passage
RetrievalPrior to restoration work performed be-
tween 1990 and 2001, the tower leaned at
an angle of 5.5 degrees, but thetower now
leans atabout 3.99 degrees.
This means
the top of the Leaning Tower of Pisa is dis-
placed horizontally 3.9 meters (12 ft 10 in)
from the center.
Prior to restoration work performed between 1990 and 2001, the Leaning Tower of Pisa leaned at an angle of 5.5 degrees.2.
The Leaning Tower of Pisa now leans at about 3.99 degrees.3.
The top of the Leaning Tower of Pisa is displaced horizontally 3.9 meters (12 ft 10 in) from the center.Prior to restoration work performed between 1990 and 2001, the tower leaned at an angle of 5.5 degrees , // but the tower now leans at about 3.99 degrees.
// This means the top of the Learning Tower of Pisa is displaced horizontally 3.9 meters (12 ft 10 in) from the center.WIkipedia
FactoidWiki
?
The proposi-
tion describes the information regarding the Tower
of Pisa’s current leaning angle in a self-contained
way and precisely responds to what the question
is querying.
Following Chen et al.
Next, we use the seed set to finetune a Flan-
T5-large model (Chung et al., 2022).
# units Avg.
6.1 QA Performance
Table 3 shows the evaluation results.
+4.9, +5.9, and +6.9 EM@100 (26%, 19%, 22%,
26% relative improvement), respectively.
We illustrate this phenomenon by an analysis
shown in Figure 4.
Each Cowboys and Steelers
player wore a special patch with the Bicen-
tennial logo on their jerseys...Title: Super Bowl X ✗
The overall theme of the Super Bowl
entertainment was to celebrate the
United States Bicentennial.Title: Super Bowl XLV ✓
... As this was the 50th Super Bowl game,
the league [SuperBowl 50] emphasized
the""golden anniver sary"" with various gold-
themed initiatives during the 2015 season, as
well as...
Q2: The atomic number of indium which belongs to 5th period is?
Title: Period 5 element ✗
The periodic table is laid out in rows to illus-
trate recurring (periodic) trends in the chemi-
cal behaviour of the elements as their atomic
number increases: ...Title: Period 5 element ✓
Indium is a chemical element with the
symbol In and atomic number 49.Title: Period 5 element ✓
Indium is a chemical element with the sym-
bol In and [Indium has a] atomic number 49.
This rare, very soft, malleable ...
Q3: What is the function of the pericardial sac?
Title: Pericardium ✓
The pericardium, also called pericardial sac
...
It separates the heart from interference of
other structures, protects itagainst infection
andblunt trauma, andlubricates theheart’s
movements.Title: Pericardium ✗
The pericardium, also called pericar-
dial sac, is a double-walled sac con-
taining the heart and the roots of the
great vessels.Title: Cardiac muscle ✓
On the outer aspect of the myocardium is the
epicardium which forms part of thepericar-
dialsacthatsurrounds, protects, andlubri-
cates theheart.
Q4: What is the main character’s name in layer cake?
Title: Layer Cake (film) ✓
...
The film’s plot revolves around a London-
based criminal, played by Daniel Craig, ...
Craig’s character is unnamed in the film and
is listed in the credits as ""XXXX"" .Title: Angelic Layer ✗
The primary protagonist is Misaki
Suzuhara.Title: Plot twist ✗
Sometimes the audience may discover that
the true identity of a character is , in fact,
unknown [in Layer Cake] , as in Layer Cake
or the eponymous assassins in V for Vendetta
and The Day of the Jackal.
The underlined text is the correct answer.
References
Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang,
and Jimmy Lin.
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and
Hannaneh Hajishirzi.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang.
Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni,
Daniel Andor, Livio Baldini Soares, Jacob Eisenstein,
Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al.
arXiv
preprint arXiv:2212.08037 .
Wei-Cheng Chang, X Yu Felix, Yin-Wen Chang, Yim-
ing Yang, and Sanjiv Kumar.
Sihao Chen, Senaka Buthpitiya, Alex Fabrikant, Dan
Roth, and Tal Schuster.
Sihao Chen, Hongming Zhang, Tong Chen, Ben Zhou,
Wenhao Yu, Dian Yu, Baolin Peng, Hongwei Wang,
Dan Roth, and Dong Yu.
arXiv preprint arXiv:2311.04335 .
Xilun Chen, Kushal Lakhotia, Barlas Oguz, Anchit
Gupta, Patrick Lewis, Stan Peshterliev, Yashar
Mehdad, Sonal Gupta, and Wen-tau Yih.
Hao Cheng, Hao Fang, Xiaodong Liu, and Jianfeng
Gao.
Eunsol Choi, Jennimaria Palomaki, Matthew Lamm,
Tom Kwiatkowski, Dipanjan Das, and Michael
Collins.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
arXiv preprint arXiv:2210.11416 .
Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo
Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall,
and Ming-Wei Chang.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S Weld.
Communications of the ACM ,
51(12):68–74.
Luyu Gao and Jamie Callan.
Tianyu Gao, Xingcheng Yao, and Danqi Chen.
arXiv preprint arXiv:2104.08821 .
Daniel Gildea and Daniel Jurafsky.
Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong
Yang, Jimmy Lin, and Allan Hanbury.
arXiv
preprint arXiv:1905.01969 .
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-
tian Riedel, Piotr Bojanowski, Armand Joulin, and
Edouard Grave.
Gautier Izacard and Edouard Grave.
Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,
Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie
Callan, and Graham Neubig.
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer.
arXiv preprint arXiv:1705.03551 .
Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and
Greg Durrett.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih.
Akhil Kedia, Mohd Abbas Zaidi, and Haejun Lee.
Daniel Khashabi, Yeganeh Kordi, and Hannaneh Ha-
jishirzi.
arXiv preprint
arXiv:2202.12359 .
Omar Khattab and Matei Zaharia.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, et al.
Jaewoong Lee, Heejoon Lee, Hwanhee Lee, and Ky-
omin Jung.
Jinhyuk Lee, Alexander Wettig, and Danqi Chen.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen tau Yih, Tim Rock-
täschel, Sebastian Riedel, and Douwe Kiela.
Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz,
Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun
Chen.
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang.
Yi Luan, Jacob Eisenstein, Kristina Toutanova, and
Michael Collins.
Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg,
and Jianfeng Gao.
Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric
Nyberg, and Jianfeng Gao.
Sewon Min, Julian Michael, Hannaneh Hajishirzi, and
Luke Zettlemoyer.
Stephen Mussmann and Stefano Ermon.
PMLR.
Ani Nenkova and Rebecca Passonneau.
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng
Gao, Saurabh Tiwary, Rangan Majumder, and
Li Deng.
CoRR ,
abs/1611.09268.
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo
Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan,
Keith Hall, Ming-Wei Chang, and Yinfei Yang.
Barlas Oguz, Kushal Lakhotia, Anchit Gupta, Patrick
Lewis, Vladimir Karpukhin, Aleksandra Piktus,
Xilun Chen, Sebastian Riedel, Scott Yih, Sonal
Gupta, and Yashar Mehdad.
Gpt-4 technical report.
ArXiv ,
abs/2303.08774.
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
Lewis, Majid Yazdani, Nicola De Cao, James Thorne,
Yacine Jernite, Vladimir Karpukhin, Jean Maillard,
Vassilis Plachouras, Tim Rocktäschel, and Sebastian
Riedel.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang.
arXiv preprint
arXiv:1606.05250 .
Nils Reimers and Iryna Gurevych.
Devendra Sachan, Mostofa Patwary, Mohammad
Shoeybi, Neel Kant, Wei Ping, William L. Hamil-
ton, and Bryan Catanzaro.
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,
Christopher Potts, and Matei Zaharia.
Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,
and Danqi Chen.
arXiv preprint
arXiv:2109.08535 .
Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur
Parikh, Ali Farhadi, and Hannaneh Hajishirzi.
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan
Scales, David Dohan, Ed H Chi, Nathanael Schärli,
and Denny Zhou.
PMLR.Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-
hishek Srivastava, and Iryna Gurevych.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom.
Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna
Gurevych.
Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan
Parvez, and Graham Neubig.
Ji Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita
Sharma, Damien Jose, and Paul Bennett.
Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,
Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold
Overwijk.
arXiv preprint arXiv:2007.00808 .
Fangyuan Xu, Weijia Shi, and Eunsol Choi.
Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo,
Jax Law, Noah Constant, Gustavo Hernandez Abrego,
Steve Yuan, Chris Tar, Yun-Hsuan Sung, et al.
Wen-tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek.
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,
Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,
Michael Zeng, and Meng Jiang.
Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin
Ma, Hongwei Wang, and Dong Yu.
arXiv preprint arXiv:2311.09210 .
Shunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang,
and Nan Duan.
After filtering, 42,857 pairs were used to
fine-tune a Flan-T5-Large model.
4.
Input : Title: ¯Eostre.
"", ""Georg Franck von Franckenau was a professor of
medicine.
"", ""Richard Sermon was a scholar."
102,95c543b8fe00c9b1adbed90ddbc9e22abacf34b8ecc804f58850ed67a54b4cc7-4,deeplearning,dense_x_retieval_what_retrieval_granularity_shoud_we_use.pdf,"Section: Theories and interpretations, Connection to Easter Hares.
Content:
The earliest evidence for the Easter Hare (Osterhase) was recorded in south-west Germany in
1678 by the professor of medicine Georg Franck von Franckenau, but it remained unknown in
other parts of Germany until the 18th century.
Scholar Richard Sermon writes that ""hares were
frequently seen in gardens in spring, and thus may have served as a convenient explanation for the
origin of the colored eggs hidden there for children.
Alternatively, there is a European tradition
that hares laid eggs, since a hare’s scratch or form and a lapwing’s nest look very similar, and
both occur on grassland and are first seen in the spring.
In the nineteenth century the influence
of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular throughout Europe.
German immigrants then exported the custom to Britain and America where it evolved into the
Easter Bunny.""
Output : [ ""The earliest evidence for the Easter Hare was recorded in south-west Germany in
1678 by Georg Franck von Franckenau.
"", ""The evidence for the Easter Hare remained unknown in other parts of Germany until
the 18th century.
"", ""Richard Sermon writes a hypothesis about
the possible explanation for the connection between hares and the tradition during Easter"", ""Hares
were frequently seen in gardens in spring.
"", ""Hares may have served as a convenient explanation
for the origin of the colored eggs hidden in gardens for children.
"", ""There is a European tradition
that hares laid eggs.
"", ""A hare’s scratch or form and a lapwing’s nest look very similar.
"", ""Both
hares and lapwing’s nests occur on grassland and are first seen in the spring.
"", ""In the nineteenth
century the influence of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular
throughout Europe.
"", ""German immigrants exported the custom of the Easter Hare/Rabbit to
Britain and America.
"", ""The custom of the Easter Hare/Rabbit evolved into the Easter Bunny in
Britain and America."""
103,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-1,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Thus, a scaled error is
deﬁned as
qt=et
1
n¡1n
å
i=2jYi¡Yi¡1j
which is clearly independent of the scale of the data.
A scaled error is less than one if it arises
from a better forecast than the average one-step na ¨ıve forecast computed in-sample.
Con-
versely, it is greater than one if the forecast is worse than the average one-step na ¨ıve forecast
computed in-sample.
The errors have been scaled by the one-step in-sample forecast errors
from the na ¨ıve method, and then averaged across all series.
So a value of 2 indicates that the
out-of-sample forecast errors are, on average, about twice as large as the in-sample one-step
forecast errors from the na ¨ıve method.
Because the scaling is based on one-step forecasts, the
scaled errors for multi-step forecasts are typically larger than one."
104,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-2,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Another look at measures of forecast accuracy
Rob J Hyndman
Department of Econometrics and Business Statistics,
Monash University, VIC 3800, Australia.
Telephone: +1{513{529{4826
E-Mail: koehleab@muohio.edu
2 November 2005
1Another look at measures of forecast accuracy
Abstract: We discuss and compare measures of accuracy of univariate time series forecasts.
2Another look at measures of forecast accuracy 3
1 Introduction
Many measures of forecast accuracy have been proposed in the past, and several authors have
made recommendations about what should be used when comparing the accuracy of forecast
methods applied to univariate time series data.
It is our contention that many of these proposed
measures of forecast accuracy are not generally applicable, can be inﬁnite or undeﬁned, and can
produce misleading results.
To demonstrate the inadequacy of many measures of forecast accuracy, we provide three exam-
ples of real data in Figure 1.
Suppose we are
interested in comparing the forecast accuracy of four simple methods: (1) the historical mean
using data up to the most recent observation; (2) the “na ¨ıve” method or random-walk method
based on the most recent observation; (3) simple exponential smoothing and (4) Holt’s method.
We compare the in-sample performance of the methods (based on
one-step-ahead forecasts) and the out-of-sample performance (based on forecasting the data in
the hold-out period using only information from the ﬁtting period).
Tables 1–3show some forecast error measures for these methods applied to the example data.
2 A critical survey of accuracy measures
LetYtdenote the observation at time tand Ftdenote the forecast of Yt.
5 Conclusion
Despite two decades of papers on measures of forecast error, we believe that some fundamental
problems have been overlooked.
6 Acknowledgments
We thank Michelle Hibon for kindly providing the forecasts submitted to the M3-competition,
and two anonymous referees for providing some thoughtful comments.Another look at measures of forecast accuracy 17
References
AHLBURG , D.A., C HATFIELD , C., T AYLOR , S.J., T HOMPSON , P.A., W INKLER , R.L., M URPHY ,
A.H., C OLLOPY , F., and FILDES , R.(1992) A commentary on error measures.
ARMSTRONG , J.S., and COLLOPY , F.(1992) Error measures for generalizing about forecasting
methods: empirical comparisons.
(1993) On the limitations of comparing mean square
forecast errors.
(2004) On MAPE-R as a measure of estimation and
forecast accuracy.
International J. Forecast-
ing,8, 81–98.Another look at measures of forecast accuracy 18
GARDNER , E.(1990) Evaluating forecast performance in an inventory control system.
(2000) Economic and statistical measures of forecast
accuracy.
LAWRENCE , M., and O’C ONNOR , M. (2005) Judgmental forecasting in the presence of loss
functions.
MAKRIDAKIS , S., A NDERSON , A., C ARBONE , R., F ILDES , R., H IBON , M., L EWANDOWSKI ,
R., N EWTON , J., P ARZEN , P., and WINKLER , R.(1982) The accuracy of extrapolation (time
series) methods: results of a forecasting competition.
(1990) An MSE statistic for comparing forecast accuracy across series."
105,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-3,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"KOEHLER , A.B.
THOMPSON , P.A.
TSAY, R.S."
106,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-4,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"2.1 Scale-dependent measures
There are some commonly used accuracy measures whose scale depends on the scale of the
data.
A serious deﬁciency in relative error measures is that e¤
tcan be small.
In particular, the measures used in the M-competition and the
M3-competition, and the measures recommended by other authors, all have problems—they
can give inﬁnite or undeﬁned values in commonly occurring situations.
MAKRIDAKIS , S.(1993) Accuracy measures: theoretical and practical concerns."
107,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-5,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Data source: Tsay (2002), chapter 1.
Relative errors have a statistical
distribution with undeﬁned mean and inﬁnite variance.
CHATFIELD , C. (1988) Apples, oranges and mean square error."
108,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-6,time_series,another_lookat_measures_of_forecast_accuracy.pdf,".,n.
.,j."
109,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-7,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"However, this approach has the disadvantage that the denominator
grows with the sample size for non-stationary series containing a unit root."
110,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-8,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Forecasting ,8, 99–111.
International J Forecasting ,8, 69–80.
International Journal of Forecasting 16, 521–530.
Forecasting ,4,
515–518.
Forecasting ,12, 617–637.
Interna-
tional Journal of Forecasting ,4, 405–408.
Forecasting ,19, 537–560.
Forecasting ,17, 537–584.
Forecasting ,21, 3–14.
Forecasting ,9, 527–529.
Forecasting ,1, 111–153.
Forecasting ,16, 451–476.
Forecasting ,6219–227."
111,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-9,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"The acronyms are deﬁned below and we explicitly deﬁne the measures in Sections 2and 3.
Data source: ‘Product C’ in
Makridakis, Wheelwright and Hyndman (1998), chapter 1."
112,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-10,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Billah et al.
BILLAH , B., K ING, M.L., S NYDER , R.D., and KOEHLER , A.B.
BOWERMAN , B.L., O’C ONNELL , R.T., and KOEHLER , A.B.
CLEMENTS , M.P., AND HENDRY , D.F.
COLEMAN , C.D., and SWANSON , D.A.
GRANGER , C.W.J., and PESARAN , M.H.
SWANSON , D.A., T AYMAN , J.,and BARR, C.F."
113,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-11,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Armstrong and Col-
lopy (1992) recommended the use of GMRAE, MdRAE and MdAPE.
Fildes (1992) also rec-
ommended the use of MdAPE and GMRAE (although he described the latter as the relative
geometric root mean square error or GRMSE).
However, some of the values for MdRAE and GMRAE are undeﬁned as explained above.
Some authors (e.g., Swanson, et al., 2000) have noted that measures based on percentage errors
are often highly skewed, and therefore transformations (such as logarithms) can make them
more stable.
Armstrong and Collopy (1992) recommended the use of relative absolute errors,
especially the GMRAE and MdRAE.
Fildes (1992) also prefers the GMRAE although he ex-
presses it in an equivalent (but more complex) form as the square root of the geometric mean
of squared relative errors.
2.4 Relative measures
Rather than use relative errors, one can use relative measures.
(2000) Another error measure for selection of the best fore-
casting method: the unbiased absolute percentage error."
114,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-12,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Of the measures in Tables 1–3, only the MASE can be used for these series due to the occur-
rence of inﬁnite and undeﬁned values.
Table 4gives the MASE for each of the M3 methods along with the HKSG method.
Comparing Table 4with the results of the original M3 analysis (Makridakis & Hibon, 2000)
shows that MASE does not substantially affect the main conclusions about the best-performing
methods.
(2001) The asymmetry of the sAPE measure and other comments on the M3-
competition."
115,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-13,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Inter-
national J."
116,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-14,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"The
methods used in the M-competition and the M3-competition, and many of the measures recom-
mended by previous authors on this topic, are found to be degenerate in commonly occurring situ-
ations.
The M-competition and M3-competition also used rankings amongst competing methods.
Similarly, both competitions
included measures based on the percentage of times one method was better than a benchmark
method.
4 Application to M3-competition data
We demonstrate the use of MASE using the M3-competition data (Makridakis & Hibon, 2000).
The best performing method in each
category is highlighted with its MASE in bold."
117,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-15,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"MAPE Mean Absolute Percentage Error
MdAPE Median Absolute Percentage Error
sMAPE Symmetric Mean Absolute Percentage Error
sMdAPE Symmetric Median Absolute Percentage Error
MdRAE Median Relative Absolute Error
GMRAE Geometric Mean Relative Absolute Error
MASE Mean Absolute Scaled Error
1Data downloaded from http://www.forecasters.org/data/m3comp/m3comp.htmAnother look at measures of forecast accuracy 4
(a) M3 series 472
Year1960 1965 1970 1975 1980 1985 1990 19955000 7000 9000
(b) Disney monthly log return
YearPercentage return
1990 1992 1994 1996 1998 2000−20 0 10
(c) Sales of lubricant
MonthUnits sold
0 5 10 15 20 25 30 350 2 4 6 8 10
Figure 1: Example (a): Series 472 from the M3-competition.
The vertical dashed lines indicate the end of
the data used for ﬁtting and the start of the “hold-out” set used for out-of-sample forecast.Another look at measures of forecast accuracy 5
Example A Mean Random walk SES Holt
In Out In Out In Out In Out
MAPE 14.09 25.57 2.01 5.00 2.08 5.03 1.31 3.94
MdAPE 17.44 26.13 1.61 5.71 1.86 5.74 1.04 4.20
sMAPE 0.16 0.29 0.02 0.05 0.02 0.05 0.01 0.04
sMdAPE 0.19 0.30 0.02 0.06 0.02 0.06 0.01 0.04
MdRAE 6.50 4.61 Undeﬁned Undeﬁned 1.02 1.01 0.50 0.80
GMRAE ¥ ¥ Undeﬁned Undeﬁned ¥ ¥ ¥ ¥
MASE 7.88 17.23 1.00 3.42 1.04 3.44 0.66 2.69
Table 1: Forecast error measures for M3 series N0472.
Example B Mean Random walk SES Holt
In Out In Out In Out In Out
MAPE ¥ 96.56 ¥ 125.90 ¥ 102.32 ¥ 113.90
MdAPE 101.61 96.92 159.25 119.19 97.06 98.63 98.70 97.91
sMAPE ¡0.80 ¡2.04 ¥¡3.03 ¡1.10 ¡0.57 ¡0.80 ¡1.12
sMdAPE 0.91 0.35 0.66 0.22 1.01 0.43 0.91 0.48
MdRAE 0.71 0.82 1.00 1.00 0.63 0.88 0.64 0.94
GMRAE 0.66 1.06 1.00 1.00 0.62 1.01 0.60 1.10
MASE 0.72 1.01 1.00 1.06 0.70 1.02 0.70 1.04
Table 2: Forecast error measures for Disney stocks.
Example C Mean Random walk SES Holt
In Out In Out In Out In Out
MAPE ¥ ¥ Undeﬁned Undeﬁned ¥ ¥ ¥ ¥
MdAPE ¥ ¥ Undeﬁned Undeﬁned ¥ ¥ ¥ ¥
sMAPE 1.68 1.39 Undeﬁned Undeﬁned 1.62 1.37 1.65 1.62
sMdAPE 2.00 2.00 Undeﬁned Undeﬁned 2.00 2.00 2.00 2.00
MdRAE 1.00 ¥ Undeﬁned Undeﬁned 1.17 ¥ 0.91 ¥
GMRAE ¥ ¥ Undeﬁned Undeﬁned ¥ ¥ ¥ ¥
MASE 0.89 0.39 1.00 0.15 0.83 0.35 0.79 0.20
Table 3: Forecast error measures for lubricant sales.Another look at measures of forecast accuracy 6
In these tables, we have included measures that have been previously recommended for use
in comparing forecast accuracy across many series.
Note that with random walk forecasts, the in-sample results for MASE and all results for
MdRAE and GMRAE are 1 by deﬁnition, as they involve comparison with na ¨ıve forecasts.
Figure 2shows the MASE at each forecast horizon for ﬁve forecasting methods applied to the
M3-competition data.
In particular, as with the original M3 analysis, the Theta method (Assimakopoulos &Another look at measures of forecast accuracy 14
1 2 3 4 5 61 2 3 4Annual data
Forecast horizonMASETheta
Robust−Trend
ForecastPro
HKSG
1 2 3 4 5 6 7 81 2 3 4Quarterly data
Forecast horizonMASETheta
Robust−Trend
ForecastPro
HKSG
5 10 151 2 3 4Monthly data
Forecast horizonMASETheta
Robust−Trend
ForecastPro
HKSG
1 2 3 4 5 6 7 81 2 3 4Other data
Forecast horizonMASETheta
Robust−Trend
ForecastPro
HKSG
Figure 2: Mean Absolute Scaled Errors at different forecast horizons for ﬁve forecasting methods applied
to the M3-competition data.Another look at measures of forecast accuracy 15
Yearly Quarterly Monthly Other All
Theta 2.81 1.97 2.08 1.95 2.20
Theta-sm 2.81 2.00 2.09 1.95 2.22
Robust-Trend 2.63 2.15 2.14 1.92 2.23
Comb SHD 2.88 2.05 2.12 2.09 2.26
ForecastX 2.77 2.22 2.20 1.97 2.31
ForecastPro 3.03 2.35 2.04 1.96 2.33
Dampen 3.03 2.10 2.18 2.08 2.34
HKSG 3.06 2.29 2.15 1.86 2.36
RBF 2.72 2.19 2.27 2.70 2.37
BJ automatic 3.16 2.21 2.21 2.30 2.42
Flores/Pearce1 2.94 2.23 2.31 2.27 2.42
Holt 3.18 2.40 2.15 2.04 2.43
ARARMA 3.48 2.29 2.07 2.05 2.43
SmartFcs 3.00 2.39 2.23 2.08 2.43
PP-autocast 3.02 2.12 2.44 2.09 2.46
Pegels 3.58 2.30 2.13 1.85 2.47
Flores/Pearce2 3.02 2.41 2.27 2.34 2.47
Autobox3 3.18 2.45 2.23 2.01 2.47
Automatic ANN 3.06 2.35 2.34 2.13 2.49
Winter 3.18 2.37 2.43 2.04 2.55
SES 3.17 2.27 2.44 3.14 2.59
Autobox1 3.68 2.61 2.20 2.12 2.62
Naive2 3.17 2.28 2.50 3.13 2.62
Autobox2 2.75 2.20 3.39 1.90 2.87
Table 4: Mean Absolute Scaled Error for the M3-forecasting competition.
This new measure is also easily interpretable: values of
MASE greater than one indicate the forecasts are worse, on average, than in-sample one-step
forecasts from the naive method.
However, in situations where there are very different scales
including data which are close to zero or negative, we suggest the MASE is the best available
measure of forecast accuracy."
118,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-16,time_series,another_lookat_measures_of_forecast_accuracy.pdf,".
.
.
.
."
119,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-17,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Also note that the
sMAPE can take negative values although it is meant to be an “absolute percentage error”.
As was seen in the examples in Section 1, sMAPE and sMdAPE can take negative values."
120,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-18,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"International
J.
International J.
International J.
International J.
International J.
International J."
121,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-19,time_series,another_lookat_measures_of_forecast_accuracy.pdf,".,Fn+mbased on data
from times t=1,.
.,Fm+hwhere each Fj+h
is based on data from times t=1,.
Then we compute a single
Fn+hbased on data from times t=1,."
122,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-20,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"J.
J.
J."
123,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-21,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"In the latter case, it is algebraically possible to cancel the
numerator and denominator, although numerical results will be undeﬁned.
It
would seem more natural to deﬁne them with absolute values in the denominator, and so avoid
this problem, but this is not what is usually done."
124,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-22,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"We do not suggest these are the best methods for these data, but they are all simple methods
that are widely applied.
These are useful when comparing different methods on the same set of data, but should
not be used, for example, when comparing across data sets that have different scales.
They have a meaningful scale,
are widely applicable, and are not subject to the degeneracy problems seen in the examples in
Section 1."
125,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-23,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Then deﬁne the forecast
error et=Yt¡Ft.
2.2 Measures based on percentage errors
The percentage error is given by pt=100et/Yt.
For the same value of Yt, the value of 2 jYt¡Ftj/(Yt+Ft)has a
heavier penalty when forecasts are low compared to when forecasts are high.
Let rt=et/e¤
tdenote the relative error where e¤
tis the forecast
error obtained from the benchmark method.
Usually, the benchmark method is the random
walk where Ftis equal to the last observation; this is what was used in the examples in Sec-
tion 1.Another look at measures of forecast accuracy 10
Then we can deﬁne:
Mean Relative Absolute Error (MRAE) =mean (jrtj)
Median Relative Absolute Error (MdRAE) =median (jrtj)
Geometric Mean Relative Absoluate Error (GMRAE) =gmean (jrtj)
and so on.
For example, let MAE bdenote
the MAE from the benchmark method.
When the benchmark method is a random walk, and the forecasts are all one-step forecasts, the
relative RMSE is Theil’s Ustatistic (Thiel, 1966, chapter 2), sometimes called U2.
For example relative MAE measures
the improvement possible from the proposed forecast method relative to the benchmark fore-
cast method.
If the RMSSE is used, it may be
preferable to use the in-sample RMSE from the na ¨ıve method in the denominator of qt."
126,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-24,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"The MdRAE, sMAPE and sMdAPE were used
in the M3-competition (Makridakis & Hibon, 2000).
Never-
theless, the MSE was used by Makridakis et al., 1985, in the M-competition.
MAKRIDAKIS , S., and HIBON , M. (2000) The M3-competition: results, conclusions and impli-
cations."
127,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-25,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"They also give no indication of the size of the forecast errors.
However, they are more sensitive to outliers than MAE or MdAE which has
led some authors (e.g., Armstrong, 2001) to recommend against their use in forecast accuracy
evaluation.
A further disadvantage of methods based on percentage errors is that they assume a meaning-Another look at measures of forecast accuracy 9
ful zero.
For example, they make no sense in measuring forecast error for temperatures on the
Fahrenheit or Celsius scales.
Thus, it provides a form of shrinkage which
limits the ability of the model to produce anything wildly inaccurate."
128,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-26,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Demography ,2, 193–201."
129,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-27,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"ARMSTRONG , J.S.
Armstrong."
130,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-28,time_series,another_lookat_measures_of_forecast_accuracy.pdf,".,nfor each of mdifferent series.
It makes no sense to compute the MAE across series (due to their different scales).
Scaling by the in-
sample na ¨ıve MAE only assumes that series has no more than one unit root, which is almost
always true for real data.
For example, if all series are on the same scale, then the MAE may be preferred because it is
simpler to explain."
131,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-29,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"This means, for example, that the MAPE is often substantially larger than the MdAPE.
Excessively large (or inﬁnite) MAPEs were avoided in the M3-competition by only
including data that were positive (Makridakis and Hibon, 2000, p.462)."
132,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-30,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"These show series N0472 from the M3-competition1, monthly log
stock returns for the Walt Disney Corporation, and monthly sales of a lubricant product sold
in large containers.
Note that the Disney return series and the lubricant sales series both in-
clude exact zero observations, and the Disney series contains negative values."
133,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-31,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Note that there are many inﬁnite values occurring in Tables 1–3due to division by zero.
The undeﬁned values
arise due to the division of zero by zero.
Some of these are due to computations of the form
Yt/(Yt¡Yt¡1)where Yt¡1=Yt=0 and others are due to computations of the form (Yt¡
Yt¡1)/(Yt¡Yt¡1)where Yt=Yt¡1."
134,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-32,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"The most commonly used scale-dependent measures are based on the absolute error or squaredAnother look at measures of forecast accuracy 8
errors:
Mean Square Error (MSE) =mean (e2
t)
Root Mean Square Error (RMSE) =p
MSE
Mean Absolute Error (MAE) =mean (jetj)
Median Absolute Error (MdAE) =median (jetj)
Often, the RMSE is preferred to the MSE as it is on the same scale as the data.
Historically,
the RMSE and MSE have been popular, largely because of their theoretical relevance in statis-
tical modelling.
The most commonly used measures are:
Mean Absolute Percentage Error (MAPE) =mean (jptj)
Median Absolute Percentage Error (MdAPE) =median (jptj)
Root Mean Square Percentage Error (RMSPE) =q
mean (p2
t)
Root Median Square Percentage Error (RMdSPE) =q
median (p2
t).
This observation led to the use of the so-called “symmetric”
measures (Makridakis, 1993) deﬁned by
Symmetric Mean Absolute Percentage Error (sMAPE) =mean (200jYt¡Ftj/(Yt+Ft))
Symmetric Median Absolute Percentage Error (sMdAPE) =median (200jYt¡Ftj/(Yt+Ft))
The problems arising from small values of Ytmay be less severe for sMAPE and sMdAPE.
Similar measures can be deﬁned using RMSEs, MdAEs, MAPEs, etc.
This is often known as “Percent Better” and can be expressed
as
PB(MAE) =100 mean (IfMAE <MAE bg)
PB(MSE) =100 mean (IfMSE <MSE bg)
However, these give no indication about the amount of improvement possible.
The Mean Absolute Scaled Error is simply
MASE =mean (jqtj).
Related measures such as Root Mean Squared Scaled Error (RMSSE) and Median Absolute
Scaled Error (MdASE) can be deﬁned analogously.
Of these measures, we prefer MASE as it is less sensitive to outliers and more easily interpreted
than RMSSE, and less variable on small samples than MdASE.
In some circumstances an asymmmetric loss function may be preferred (see, e.g., Lawrence &
O’Connor, 2005), in which case some other (asymmetric) function of the scaled errors may be
appropriate."
135,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-33,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"See Goodwin
and Lawton (1999) and Koehler (2001) for further discussion on this point.
See Coleman and Swanson (2004) for further discussion."
136,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-34,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Then, a relative MAE is given by
RelMAE =MAE /MAE b.
Note that Armstrong and
Collopy refer to the relative MAE as CumRAE."
137,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-35,time_series,another_lookat_measures_of_forecast_accuracy.pdf,(1992).
138,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-36,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"In fact, rthas inﬁnite
variance because e¤
thas positive probability density at 0.
One common special case is when et
and e¤
tare normally distributed, in which case rthas a Cauchy distribution."
139,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-37,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Instead, we propose that the mean absolute scaled error become the standard measure for
comparing forecast accuracy across multiple time series.
Keywords: forecast accuracy, forecast evaluation, forecast error measures, M-competition, mean
absolute scaled error.
We consider it the best available
measure of forecast accuracy and we argue for it in Section 3.
Percentage errors have the advantage of being
scale-independent, and so are frequently used to compare forecast performance across different
data sets.
2.3 Measures based on relative errors
An alternative way of scaling is to divide each error by the error obtained using another stan-
dard method of forecasting.
In fact, Theil’s
deﬁnition is ambiguous and the relative RMSPE with the random walk as a benchmark method
is also sometimes called Theil’s Ustatistic (e.g., in Makridakis, Wheelwright and Hyndman,
1998).Another look at measures of forecast accuracy 11
Thompson’s (1990) LMR measure is simply log (RelMSE ).
We propose a new but related idea that is suitable for all situations, by scaling the error based
on the in-sample MAE from the na ¨ıve (random walk) forecast method.
(2005) used a similar error
measure when they computed the absolute value of the forecast error as a percentage of the in-
sample standard deviation.
If multi-step forecasts are being computed, it is possible to scale
by the in-sample MAE computed from multi-step na ¨ıve forecasts.Another look at measures of forecast accuracy 13
We propose that measures based on scaled errors should become the standard approach in
comparing forecast accuracy across series on different scales.
Here, the
absolute scaled errors have been averaged across all out-of-sample forecast horizons used in
the M3 competition, and then averaged across all series.
We propose that scaled errors become the standard measure for forecast accuracy, where the
forecast error is scaled by the in-sample mean absolute error obtained using the na ¨ıve forecast-Another look at measures of forecast accuracy 16
ing method."
140,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-38,time_series,another_lookat_measures_of_forecast_accuracy.pdf,We do not distinguish these scenarios in this paper.
141,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-39,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"The cause of the problems with M3 series N0472 is the occurrence
of consecutive observations taking the same value, something that occurs very often with real
data."
142,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-40,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"A third scenario arises when we wish to compare the
accuracy of methods across many series at a single forecast horizon."
143,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-41,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"We use the notation mean (xt)to denote the sample mean of fxtgover the period of interest
(or over the series of interest).
Analogously, we use median (xt)for the sample median and
gmean (xt)for the geometric mean."
144,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-42,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Hyndman & Billah (2003) demonstrated that this method
is equivalent to simple exponential smoothing (SES) with drift where the drift is half the value
of the slope of a linear regression ﬁtted to the data."
145,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-43,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"We
do not include those here as they are dependent on the number of methods being considered.
Again, such measures are not included here as they do not indicate the size of the
errors."
146,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-44,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"For seasonal data,
the “na ¨ıve2” method is sometimes used for comparison; this gives forecasts based on the last
observation adjusted for seasonality using classical decomposition (Makridakis, Wheelwright
and Hyndman, 1998).
The methods Theta, Robust-
Trend and ForecastPro were part of the M3-competition, and are described in Makridakis & Hi-
bon (2000).
(2001) “Evaluating forecasting methods”, Chapter 14 in Principles of fore-
casting: a handbook for researchers and practitioners , ed., J.S.
ASSIMAKOPOULOS , V.and NIKOLOPOULOS , K. (2000) The theta model: a decomposition ap-
proach to forecasting.
(2005) Exponential smoothing
model selection for forecasting.
(2004) Forecasting, time series and
regression: an applied approach , Thomson Brooks/Cole: Belmont, CA.
Accessed
18 May 2005.
http://hops.wharton.upenn.edu/forecast/paperpdf/armstrong-unbiasedAPE.pdf
DIEBOLD , F.X.
(2001) Elements of forecasting , 2nd ed., South-Western: Cincinnati, Ohio.
FILDES , R.(1992) The evaluation of extrapolative forecasting methods.
HANKE , J.E., and REITSCH , A.G. (1995) Business forecasting , 5th ed., Prentice-Hall: Englewood
Cliffs, NJ.
HYNDMAN , R.J., K OEHLER , A.B., S NYDER , R.D., and GROSE , S.(2002) A state space frame-
work for automatic forecasting using exponential smoothing methods.
MAKRIDAKIS , S., W HEELWRIGHT , S., and HYNDMAN , R.J. (1998) Forecasting: methods and ap-
plications , 3rd ed., John Wiley & Sons: New York.
THEIL , H. (1966) Applied economic forecasting .
(2002) Analysis of ﬁnancial time series , John Wiley & Sons: New York."
147,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-45,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"International J. Fore-
casting , 18(3), 439–454."
148,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-46,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Rand McNally: Chicago, IL."
149,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-47,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Working Paper 6/05, Department of Econometrics and
Business Statistics, Monash University, Australia.
Center for Population Studies, University of Mississippi.
Accessed 18 May 2005.
http://www.olemiss.edu/depts/population studies/WorkingPapers.html
COLLOPY , F.,and ARMSTRONG , J.S.
(2000) A note on the measurement of accuracy
for subnational demographic estimates."
150,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-48,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"This inappropriate
use of the MSE was widely criticized (e.g., Chatﬁeld, 1988; Armstrong and Collopy, 1992)."
151,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-49,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"The
relative measures are all computed relative to a na ¨ıve (random walk) method.
The random walk or “na ¨ıve” method (where Ftis equal to the last observation) is the most
common benchmark method for such calculations, although another frequently used possibil-
ity is the mean method (where Ftis equal to the mean of all observations).
A related approach is to use the percentage of forecasts for which a given method is more
accurate than the random walk."
152,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-50,time_series,another_lookat_measures_of_forecast_accuracy.pdf,An advantage of these methods is their interpretability.
153,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-51,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Example (b): ten years of monthly log stock
returns for the Walt Disney Corporation, 1990–1999.
Example (c):
three years of monthly sales of a lubricant product sold in large containers."
154,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-52,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"These three series are not degenerate or unusual—
intermittent demand data often contain zeros and many time series of interest to forecastersAnother look at measures of forecast accuracy 7
take negative observations.
These measures have the disadvantage of being inﬁnite or undeﬁned if Yt=0 for any tin
the period of interest, and having an extremely skewed distribution when any Ytis close to
zero.
Where the data involves small counts (which is common with intermittent demand data; see
Gardner, 1990) it is impossible to use these measures as occurrences of zero values of Ytoccur
frequently.
However, ever there if Ytis close to zero, Ftis also likely to be close to zero."
155,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-53,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Telephone: +61{3{9905{2358
Email: Rob.Hyndman@buseco.monash.edu
Anne B Koehler
Department of Decision Sciences & Management Information Systems
Miami University
Oxford, Ohio 45056, USA."
156,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-54,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Working paper.
Manage-
ment Science ,36, 490–499."
157,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-55,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Nikolopoulos, 2000) does very well."
158,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-56,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"The HKSG method uses the state space modelling approach of Hyndman, Koehler,
Snyder and Grose (2002), but only including the additive models.
All methods were participants
in the M3-competition except for HKSG which is based on the method of Hyndman, Koehler, Snyder and
Grose (2002)."
159,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-57,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Most textbooks recommend the use of
the MAPE (e.g., Hanke and Reitsch, 1995, p.120, and Bowerman, O’Connell & Koehler, 2004,
p.18) and it was the primary measure in the M-competition (Makridakis, et al., 1982).
In con-
trast, Makridakis, Wheelwright & Hyndman (1998, p45) warn against the use of the MAPE
in some circumstances, including those encountered in these examples.
The MAPE and MdAPE also have the disadvantage that they put a heavier penalty on positive
errors than on negative errors.
If all data are positive and much greater than zero, the MAPE may still be
preferred for reasons of simplicity."
160,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-58,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Armstrong and Collopy (1992) recommend the use of “winsorizing” to trim extreme values.
This will avoid the difﬁculties associated with small values of e¤
t, but adds some complexity to
the calculation and a level of arbitrariness as the amount of trimming must be speciﬁed."
161,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-59,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Rather, we simply look at ways of sum-
marizing forecast accuracy assuming that we have mforecasts and that we observe the data at
each forecast period.
One common situation where it is not possible to use such measures is where one
is measuring the out-of-sample forecast accuracy at a single forecast horizon across multiple
series.
Thus, it is
possible to have one method that performs very slightly better than the benchmark method for
99 series but much worse on 1 series, thus giving it a PB score of 99 even though the benchmark
method is preferable.Another look at measures of forecast accuracy 12
3 Scaled errors
Relative measures and measures based on relative errors both try to remove the scale of the
data by comparing the forecasts with those obtained from some benchmark forecast method,
usually the na ¨ıve method.
Relative measures can only be com-
puted when there are several forecasts on the same series, and so cannot be used to measure
out-of-sample forecast accuracy at a single forecast horizon."
162,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-60,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"In particular, we do not recommend the use of any of the measures
that were used in the M-competition and the M3-competition."
163,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-61,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Further, these measures are not as “symmet-
ric” as their name suggests."
164,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-62,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Diebold, 2001, gives some examples of suitable asymmetric functions.
GOODWIN , P., and LAWTON , R. (1999) On the asymmetry of the symmetric MAPE."
165,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-63,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"While this has some nice statistical
properties, it is not so easily interpreted which is possibly why it has not been widely used.
The only circumstance under which these measures would be inﬁnite or undeﬁned
is when allhistorical observations are equal.
This is widely applicable, and is always deﬁned and ﬁnite except in the irrelevant
case where all historical data are equal.
Of course, there will be situations where some of the existing measures may still be preferred."
166,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-64,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Kluwer Academic
Publishers: Norwell, MA."
167,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-65,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"However, they both have problems."
168,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-66,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Di-
vision by numbers close to zero also results in very large numbers.
Thus, the measure
still involves division by a number close to zero."
169,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-67,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"The forecasts may be computed from a common base time, and be of varying
forecast horizons.
Alternatively, the forecasts may be from varying base times, and be of a
consistent forecast horizon.
However, they require several forecasts on the same series to enable a MAE (or MSE) to be
computed."
170,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-68,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"However, this is an
artiﬁcial solution that is impossible to apply in practical situations."
171,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-69,time_series,another_lookat_measures_of_forecast_accuracy.pdf,Updated October 2000.
172,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-70,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"We provide our own recommendations of what should be used in
empirical comparisons."
173,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-71,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"When RelMAE <1, the proposed method is better than the benchmark method
and when RelMAE >1, the proposed method is worse than the benchmark method.
When MASE <1, the proposed method gives, on average, smaller errors than the one-step
errors from the na ¨ıve method."
174,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-72,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"To our knowledge, the MASE has not been proposed before."
175,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-73,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"Thus, we may compute out-of-sample forecasts Fn+1,.
That is, we may compute forecasts F1+h,.
The in-sample forecasts in the examples above were
based on the second scenario with h=1."
176,d0c311c5cbe30c3cc008b9e54d1f6d107078ef0e30d18760d77f62d479c33268-74,time_series,another_lookat_measures_of_forecast_accuracy.pdf,"This equivalence does not seem to have been noticed by any of the
discussants in the commentary by Ahlburg et al."
177,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-1,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball.
Evan Hernandez, Belinda Z Li, and Jacob Andreas.
Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir
Radev, Noah A Smith, Yejin Choi, and Kentaro Inui.
Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and
Hannaneh Hajishirzi.
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.
Kosuke Nishida, Kyosuke Nishida, Masaaki Nagata, Atsushi Otsuka, Itsumi Saito, Hisako Asano, and
Junji Tomita.
Tal Schuster, Adam Fisch, and Regina Barzilay.
Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.
Robyn Speer, Joshua Chin, and Catherine Havasi.
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and
Yejin Choi."
178,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-2,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"In International Conference on Learning Representations , 2018.
In International Conference on
Learning Representations , 2020.
In International Conference on Learning Representations , 2019.
InInternational Conference on Learning Representations , 2021.
In The Eleventh International Conference on Learning Represen-
tations , 2022.
In International Conference on
Learning Representations , 2019.
In The Eleventh International Conference on Learning Representations ,
2022.
In International Conference on
Learning Representations , 2019.
In International Conference on Learning Representations ,
2019.
In The Eleventh International Conference on
Learning Representations , 2022.
In The Eleventh International Conference on Learning Representations , 2022.
In The Eleventh International Conference on Learning Representations , 2022.
In International
conference on learning representations , 2021."
179,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-3,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"For example, grade school students
might expect LLMs to be absolutely factual about knowledge and information in common textbooks,
NLP researchers might expect LLMs to have a basic understanding of current NLP research, cooking
amateurs might expect LLMs to understand the basic recipes and cuisines for different occasions,
and more."
180,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-4,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,".
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
."
181,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-5,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP) , pp.
In
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp.
), Proceedings of the 2023 Conference on Em-
pirical Methods in Natural Language Processing , pp.
In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) , pp.
In
Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pp.
In Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP) , pp.
In Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP) , pp.
In Proceedings of the 2017
conference on empirical methods in natural language processing , pp.
In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP) , pp.
In Proceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing , pp.
In Proceedings
of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp.
In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP) , pp.
In Proceedings of the 2020 conference on empirical methods in
natural language processing: system demonstrations , pp.
In
Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pp."
182,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-6,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Bottom-Up Approach Bottom-up starts by prompting available knowledge cards, then progres-
sively goes through the three knowledge selectors, and these outputs are incorporated into the LLM
via the prompt context.
Knowledge Stream Analysis Inbottom-up , three hyperparameters (§2.3) govern the “knowledge
stream” from knowledge cards to the general-purpose LLMs.
One straightforward way to combine the two knowledge
integration approaches would be: in each step of top-down, the LLM proposes multiple knowledge
cards as candidates, then employs the bottom-up approach with the pool of these knowledge cards for
knowledge generation."
183,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-7,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,", sn”.
IIS2142739, NSF Grants
No.
2931–2937, 2017.
Answer : Ron Wyden
//two-way setting
Question : Who won the 24th congressional district of Texas in the 2022 U.S. midterm elections?
A. Jan McDowell B. Beth Van Duyne
Answer : B
//four-way setting
Question : Who won the 6th congressional district of Pennsylvania in the 2022 U.S. midterm elections?"
184,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-8,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"sports
Factuality Selector
Knowledge : Tom Brady returned to his hometown of San Mateo, CA .
Answer : Answer : Knowledge: Tom 
Brady returned to 
his hometown of 
San Mateo, CA .
Tom Brady returned to his hometown of San Mateo .
Knowledge :Tom Brady returned to his hometown of San Mateo, CA .
sports
Knowledge :Tom Brady returned to his hometown of San Mateo, CA ."
185,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-9,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"KNOWL -
EDGE CARD successfully updates the knowledge
of Codex by adding a single knowledge card.Tasks and Datasets 1) For general-purpose
QA, we adopt MMLU (Hendrycks et al., 2020),
a multiple-choice QA dataset covering 57 tasks
in humanities, STEM, social sciences, and oth-
ers.
MIDTERM QApresents three evaluation datasets and settings: open-book, 2-way, and 4-way multiple
choice.
We did
not consider existing temporal QA datasets (Jang et al., 2021; Dhingra et al., 2022; Kasai et al., 2022)
since they do not focus on any specific event or knowledge domain.
4 R ESULTS
MMLU For general-purpose knowledge QA, we use the MMLU benchmark (Hendrycks et al.,
2020).
Measuring massive multitask language understanding.
S2orc: The semantic
scholar open research corpus.
Answering while summarizing: Multi-task learning for multi-hop qa with evidence
extraction.
Yago 4: A reason-able knowledge
base.
Conceptnet 5.5: An open multilingual graph of
general knowledge.
Wikidata: a free collaborative knowledgebase.
Dataset Details 1) The MMLU dataset (Hendrycks et al., 2020) contains a total of 15,908 four-
choice questions further divided into 57 sub-tasks in four domains: humanities, social sciences,
STEM, and others.
2https://github.com/sdtblck/Opensubtitles_dataset
3https://github.com/thoppe/The-Pile-PubMed
4https://github.com/noanabeshima/wikipedia-downloader
5https://www.yelp.com/dataset
21Published as a conference paper at ICLR 2024
¡in-context examples with the same format¿
...
22Published as a conference paper at ICLR 2024
Algorithm 1: Bottom-Up Approach
Data: question q; in-context examples prompt sicl; knowledge cards C={spec1, ."
186,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-10,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Modular LMs Mixture-of-Experts (MoE) (Masoudnia & Ebrahimpour, 2014) aims to activate one
expert based on the input instance, which has been adopted in language model research (Gururangan
et al., 2022; Roller et al., 2021; Lewis et al., 2021; Kudugunta et al., 2021; Pfeiffer et al., 2022).
One billion word benchmark for measuring progress in statistical language modeling.
Palm:
Scaling language modeling with pathways.
The pile: An 800gb dataset of diverse text for
language modeling.
Demix layers:
Disentangling domains for modular language modeling.
Scaling expert language models with unsupervised domain discovery.
Nonparametric masked language modeling.
Lifting the curse of multilinguality by pre-training modular transformers.
Bitfit: Simple parameter-efficient fine-tuning
for transformer-based masked language-models."
187,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-11,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"6Published as a conference paper at ICLR 2024
+ BookCorpus + PubMed + IMDB + News + Wikipedia+1.0+2.0+3.0ACC
+ BookCorpus + PubMed + IMDB + News + Wikipedia+1.0+2.0Balanced ACC
Figure 2: Performance on misinformation de-
tection when each knowledge card is separately
added.
KNOWLEDGE CARD also outperforms SI ET AL .
Table 4 demon-
strates that KNOWLEDGE CARD outperforms REPLUG on both settings of misinformation detection,
suggesting that knowledge cards present a better knowledge repository.
Since modular knowledge cards have the ability to change
or update LLM knowledge, malicious actors could advance their agenda by submitting malicious
knowledge cards trained on disinformation, hyperpartisan content, propaganda, and more, while
framing them as benign knowledge domains and deceive LLM users.
We encourage the responsible use of
KNOWLEDGE CARD, while we also call on users and researchers to be mindful of this dual-use risk.
To this end, we gradually add
five knowledge cards (PubMed, IMDB, Book-
Corpus, News, and Wikipedia) to KNOWLEDGE
CARD and evaluate performance with the misin-
formation dataset, 2-way setting, bottom-up ap-
proach, and the ChatGPT model.
Table 7 demon-
strates that the addition of knowledge cards, es-
pecially in-domain ones (News in this case), is indeed helpful in improving the base large language
model.
Setting # card BAcc MaF
VANILLA 0 80.1 70.5
+ PUBMED 1 80.7 70.6
+ IMDB 2 80.6 71.2
+ B OOK CORPUS 3 82.3 72.9
+ N EWS 4 85.7 73.1
+ W IKIPEDIA 5 76.5 75.3
Table 7: KNOWLEDGE CARD performance
on the two-way misinformation dataset when
five knowledge cards are gradually added.Working Examples We present the specific
prompts, generated knowledge documents, and
prompts for the bottom-up approach, and the top-
down approach with automatic and explicit selection
in Tables 9, 10, and 11 respectively.
Different knowledge cards dohave varying factuality score distributions."
188,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-12,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Specifically, we evaluate the factuality of knowledge documents with two measures.
We first evaluate summarization factuality , ensuring that the pruned version ˜difactually captures the
important points in the original di.
Concretely, we adopt factuality evaluation models (Kry ´sci´nski
et al., 2020; Feng et al., 2023a) as a scoring function sum-fact( ·,·), where each knowledge document
dis assigned a summarization factuality score ssum
d= sum-fact( ˜d|d)∈[0,1].
We then propose to evaluate whether the generated knowledge document is well-supported by
real-world knowledge through retrieval-augmented fact checking .
, tk, then employ a fact-checking
model (Schuster et al., 2021) as a scoring function fact-check( ·,·).
We then assign a fact-checked
factuality score to each dbased on the retrieved document that most supports d, formally sfact
d=
max 1≤i≤kfact-check( d|ti)∈[0,1].
We then average the summarization factuality score and the
fact-checking score for each document to obtain sd.
While it is straightforward to greedily select ℓknowledge documents with the highest sdscores,
new and more recent knowledge might not be well-supported by existing fact-checking tools.
As a
result, we propose top-kfactuality sampling to allow for flexibility while remaining stringent towards
knowledge documents that are clearly wrong.
(Appendix E) We use MPNet (Song et al., 2020) as the encoder in the relevance selector , Pegasus
(Zhang et al., 2020) as the summarization model in the pruning selector , the WikiSearch API as the
retrieval system in the factuality selector , and FactKB (Feng et al., 2023a) and VitaminC (Schuster
et al., 2021) as the summarization and fact-checking factuality scoring functions.
Figure 3 demonstrates
that while all three knowledge selectors are helpful,
the factuality selector contributes most to model per-
formance and thus plays a crucial role in ensuring the
quality of generated knowledge documents.
FactKB: Generaliz-
able factuality evaluation using language models enhanced with factual knowledge.
Evaluating the factual
consistency of abstractive text summarization.
Brio: Bringing order to abstractive
summarization.
Understanding factuality in abstractive
summarization with frank: A benchmark for factuality metrics.
To ensure the factuality of generated knowledge documents, we employed a retrieval-augmented
factuality selector based on both summarization factuality metrics and fact-checking models while
enabling flexibility through our proposed top-k factuality sampling .
We quantitatively evaluate this bias in Appendix D.
Although top-k factuality sampling enables flexibility to some extent, it remains an important problem
to design factuality evaluation measures that are generalizable and adaptable to varying and emerging
domains.
Factuality Scores of Knowledge Cards We use the MMLU
datasets as queries to prompt different knowledge cards, generate
knowledge documents, and evaluate their factuality with the factu-
ality selector ( §2.2).
We illustrate the factuality score distributions
of different knowledge cards in Figure 9, which shows that different knowledge cards have varying
inherent factuality.
We hypothesize that the distribution of factuality scores given by the factuality
selector could guide efforts to evaluate the quality of community-contributed knowledge cards.
24Published as a conference paper at ICLR 2024
0 0.2 0.4 0.6 0.8 1.0pubmed
yelp
realnews_1
wikipedia
realnews_2
realnews_3
realnews_4
acl_papers
math
1B
kgap
cpnet
gutenberg
POLITICS
reddit
IMDB
ddb
wikidata
yago
midterm
bookcorpus
legal_contracts
atomic
opensubtitles
twitter
Figure 9: Factuality score distributions of the 25 knowledge cards when prompted with questions in
the MMLU benchmark."
189,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-13,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Formally, we retain
diifi∈top-kj(sim(enc( dj),enc(q)))where top-k is the top- kargmax operation.
arXiv preprint arXiv:1312.3005 , 2013.
arXiv preprint arXiv:2107.03374 , 2021.
arXiv preprint arXiv:2204.02311 , 2022.
arXiv preprint arXiv:2210.11416 , 2022.
arXiv preprint
arXiv:2212.01378 , 2022.
arXiv preprint arXiv:2108.03861 , 2021.
arXiv preprint arXiv:2101.00027 , 2020.
arXiv preprint
arXiv:2303.14177 , 2023.
arXiv preprint arXiv:2111.13654 , 2021.
arXiv preprint arXiv:2304.00740 , 2023.
arXiv preprint arXiv:2211.09699 , 2022.
arXiv preprint arXiv:2208.03299 , 2022.
arXiv preprint arXiv:2207.05221 , 2022.
arXiv preprint arXiv:2207.13332 , 2022.
arXiv preprint arXiv:2212.14024 , 2022.
arXiv preprint arXiv:2304.07327 ,
2023.
arXiv preprint
arXiv:2305.04757 , 2023.
arXiv preprint arXiv:2210.03350 ,
2022.
arXiv preprint arXiv:2305.08702 ,
2023.
arXiv preprint arXiv:2304.11406 , 2023.
arXiv preprint
arXiv:2301.12652 , 2023.
arXiv preprint arXiv:2304.11411 , 2023.
arXiv preprint arXiv:1910.03771 , 2019.
arXiv preprint arXiv:2205.01068 , 2022.
arXiv preprint
arXiv:2307.10442 , 2023.
arXiv preprint arXiv:2206.10128 , 2022."
190,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-14,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Existing works propose addressing these limitations through retrieval
augmentation or generated knowledge prompting.
We adopt a separate encoder-based LM enc(·)that maps a token
sequence to a feature vector and cosine similarity sim(·,·)to measure relevance.
8Published as a conference paper at ICLR 2024
6 R ELATED WORK
Retrieval-Augmented Language Models Augmenting language models with retrieval has ad-
vanced the state-of-the-art in open-domain QA (Guu et al., 2020; Izacard et al., 2022; Lewis et al.,
2020; Hu et al., 2022), text classification (Zhao et al., 2023), and language modeling (Hu et al., 2022;
Borgeaud et al., 2022; Min et al., 2023).
The retrieval system could be integrated into encoder-decoder
(Izacard et al., 2022) and decoder-only models (Borgeaud et al., 2022; Shi et al., 2022; Rubin et al.,
2022), or leveraged to interpolate the next token probability distributions (Khandelwal et al., 2019;
Zhong et al., 2022).
Related works also
propose to use LM parametric knowledge for retrieval (Tay et al., 2022), answer commonsense
questions with self-talk (Shwartz et al., 2020), generate queries (Wang et al., 2022; Zhuang et al.,
2022) or token sequences (Bevilacqua et al., 2022) for document augmentation.
In addition, recitation-
augmented language models (Sun et al., 2022) propose to augment QA examples with diversified
knowledge recitations, while (Yu et al., 2022) shows that generated knowledge is, under certain
circumstances, better than retrieval.
Autoregressive search engines: Generating substrings as document identifiers.
KALM: Knowledge-
aware integration of local, document, and global contexts for long document understanding.
Retrieval augmented
language model pre-training.
Demonstrate-search-predict: Composing retrieval and language models for
knowledge-intensive nlp.
Retrieval-augmented genera-
tion for knowledge-intensive nlp tasks.
Replug: Retrieval-augmented black-box language models.
Unsupervised
commonsense question answering with self-talk.
A neural corpus indexer for document retrieval.
Aligning books and movies: Towards story-like visual explanations by watching
movies and reading books.
Bridging the gap between indexing and retrieval for differentiable search index with query
generation."
191,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-15,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Oshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou.
Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A Smith, and Luke Zettlemoyer.
Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel.
Hamid Karimi, Proteek Roy, Sari Saba-Sadiya, and Jiliang Tang.
Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer.
Adam Roberts, Colin Raffel, and Noam Shazeer.
Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani.
Taylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric Wallace, and Sameer Singh."
192,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-16,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"We design three respective selectors to control for such factors.
We provide prompt examples in Tables 10 and 11 in the Appendix.
BAcc and MaF are balanced accu-
racy and macro F1.
While
the three selectors all contribute to model perfor-
mance, the factuality selector is most crucial.
We investigate these control measures
and report performance in Figure 4."
193,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-17,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"5Published as a conference paper at ICLR 2024
Type Model Human.
(2022), RePlug, and RePlug LSR (Shi et al., 2023).
10Published as a conference paper at ICLR 2024
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
doi: 10.18653/v1/2023.emnlp-main.59.
11Published as a conference paper at ICLR 2024
Tianxing He, Jun Liu, Kyunghyun Cho, Myle Ott, Bing Liu, James Glass, and Fuchun Peng.
12Published as a conference paper at ICLR 2024
Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu.
13Published as a conference paper at ICLR 2024
Saeed Masoudnia and Reza Ebrahimpour.
14Published as a conference paper at ICLR 2024
Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap.
15Published as a conference paper at ICLR 2024
Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe
Zhao, Jai Gupta, et al.
16Published as a conference paper at ICLR 2024
Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D Man-
ning, and Jure Leskovec.
17Published as a conference paper at ICLR 2024
A D ISCUSSION
Modularity at every turn."
194,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-18,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Advances
in Neural Information Processing Systems , 35:31668–31683, 2022.
Advances in Neural Information Processing Systems , 33:
9459–9474, 2020.
Advances in
Neural Information Processing Systems , 35:17703–17716, 2022.
Advances in Neural Information Processing Systems , 34:17555–17566, 2021.
Advances in Neural Information Processing Systems , 33:
16857–16867, 2020.
Advances in Neural
Information Processing Systems , 35:21831–21843, 2022.
Advances in Neural Information Processing Systems , 35:25600–25614, 2022.
In Advances
in Neural Information Processing Systems .
Advances in neural information processing
systems , 32, 2019."
195,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-19,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"9Published as a conference paper at ICLR 2024
ACKNOWLEDGEMENTS
We thank the reviewers, the area chair, members of Tsvetshop, and the UW NLP Group for their feed-
back.
In
Findings of the Association for Computational Linguistics: NAACL 2022 , pp.
In Findings of the Association for
Computational Linguistics: ACL 2023 , pp.
In Findings of the Association for Computational
Linguistics: ACL 2022 , pp."
196,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-20,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Do you need more information?
(Yes or No)
Yes
What kind of information do you need?
Do you need more information?
Do you need more information?
Do you need more information?"
197,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-21,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Eugene Bagdasaryan and Vitaly Shmatikov.
Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem
Chumachenko, Pavel Samygin, and Colin Raffel.
Steve Cayzer.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony
Robinson.
Nicola De Cao, Wilker Aziz, and Ivan Titov.
Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem
Choshen.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob
Steinhardt.
P Izmailov, AG Wilson, D Podoprikhin, D Vetrov, and T Garipov.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.
Wojciech Kry ´sci´nski, Bryan McCann, Caiming Xiong, and Richard Socher.
Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S Weld.
Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov.
Thomas Pellissier Tanon, Gerhard Weikum, and Fabian Suchanek.
Jonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Sebastian Ruder.
Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe.
Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli.
Denny Vrande ˇci´c and Markus Kr ¨otzsch.
Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel."
198,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-22,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Wenhu Chen, Yu Su, Xifeng Yan, and William Yang Wang.
Shangbin Feng, Zilong Chen, Wenqian Zhang, Qingyao Li, Qinghua Zheng, Xiaojun Chang, and
Minnan Luo.
Shangbin Feng, Vidhisha Balachandran, Yuyang Bai, and Yulia Tsvetkov.
Shangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov.
Shangbin Feng, Zhaoxuan Tan, Wenqian Zhang, Zhenyu Lei, and Yulia Tsvetkov.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang.
Linmei Hu, Tianchi Yang, Luhao Zhang, Wanjun Zhong, Duyu Tang, Chuan Shi, Nan Duan, and
Ming Zhou.
Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo Luo.
Yujian Liu, Xinliang Frederick Zhang, David Wegsman, Nicholas Beauchamp, and Lu Wang.
Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin
Jiang.
Weijia Shi, Julian Michael, Suchin Gururangan, and Luke Zettlemoyer.
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.
Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou.
Heng Wang, Wenqian Zhang, Yuyang Bai, Zhaoxuan Tan, Shangbin Feng, Qinghua Zheng, and
Minnan Luo.
Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu.
Xinran Zhao, Hongming Zhang, Xiaoman Pan, Wenlin Yao, Dong Yu, and Jianshu Chen.
Zexuan Zhong, Tao Lei, and Danqi Chen.
Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin
Jiang."
199,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-23,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al.
Shangbin Feng, Zhaoxuan Tan, Herun Wan, Ningnan Wang, Zilong Chen, Binchi Zhang, Qinghua
Zheng, Wenqian Zhang, Zhenyu Lei, Shujie Yang, et al.
Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou.
Yujia Qin, Cheng Qian, Xu Han, Yankai Lin, Huadong Wang, Ruobing Xie, Zhiyuan Liu, Maosong
Sun, and Jie Zhou.
Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber,
and Lijuan Wang.
Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian
Tang.
Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Qi Chen, Yuqing Xia, Cheng-
min Chi, Guoshuai Zhao, Zheng Liu, et al.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Quoc V Le, Denny
Zhou, et al.
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,
Michael Zeng, and Meng Jiang.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al."
200,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-24,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"yes no1,290 498
1,140 71auto: 2-way
314 171
1,576 938auto: 4-way
correct incorrectyes no1,004 42
1,642 311exp: 2-way
correct incorrect364 68
1,718 849exp: 4-way
Figure 6: Confusion matri-
ces of yes/no and correctness
intop-down , enabling fine-
grained error analysis.LLM Compatibility While we follow previous works (Sun et al.,
2022; Shi et al., 2023) and adopt Codex as the default black-box
LLM, KNOWLEDGE CARD is compatible with different models.
However, this ability is far from perfect,
evident in the non-negligible category of “no, incorrect”, suggesting that prompting LLMs to ac-
knowledge knowledge limitations requires further research (Kadavath et al., 2022; Zhao et al., 2023),
while new approaches to abstain could be easily integrated into KNOWLEDGE CARD.
In addition,
the “yes, incorrect” categories suggest that specialized LMs occasionally fail to provide enough
information.
These confusion matrices provide fine-grained error analysis and guidance as to whether
the general-purpose LLM, the yes/no question, or knowledge cards require further improvements.
It is demonstrated that LLMs give moderately
consistent responses: 79.9% of cases received unanimous yes or
no from the three prompts, while 20.1% examples received mixed
results."
201,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-25,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"We encourage better-calibrated answers to the yes/no question through
in-context learning (Wei et al.
5-shot in-context learning is adopted to evaluate KNOWLEDGE CARD and baselines.
Baselines We compare KNOWLEDGE CARD with a wide range of baseline methods in three
categories.
We present model performance on MidtermQA in
Table 3, which demonstrates that KNOWLEDGE CARD substantially outperforms all baselines in
the open-book setting by as much as 57.3% in exact match scores (EM).
Two
potential solutions include 1) increasing the model size of knowledge cards and 2) using specialized
training objectives for knowledge cards, while both approaches require additional training and more
computational resources.
These results suggest that the selection patterns could also indicate whether a new and
more in-topic knowledge card is needed for any given task.
(Yes/No) ” to
identify if external knowledge is required and use in-context learning
to encourage well-calibrated responses.
This suggests that a potential improvement to KNOWLEDGE
CARD is to employ multiple yes/no questions to probe knowledge
limitations and use an ensemble of answers to improve robustness.
, specn};
relevance, pruning, and factuality selector ϕrel,ϕprune,ϕfact
Result: answer string sans
PROMPT =sicl
KNOWLEDGE LIST = []
forspec∈ Cdo
KNOWLEDGE LIST.append( spec(q,n1))
end
KNOWLEDGE LIST =ϕrel(q,KNOWLEDGE LIST,n2)
KNOWLEDGE LIST =ϕprune(KNOWLEDGE LIST)
KNOWLEDGE LIST =ϕfact(KNOWLEDGE LIST,n3)
PROMPT += “Knowledge: ”
fors∈KNOWLEDGE LIST do
PROMPT +=s
end
PROMPT += “Question: ” +q+ “Answer: ”
sans=LLM (PROMPT )
return sans
Algorithm 2: Top-Down Approach
Data: question q; in-context examples prompt sicl; knowledge cards C={spec1, ."
202,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-26,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Knowledge is
collaborative (Cayzer, 2004): LLMs should be able to represent and incorporate diverse and evolving
knowledge, from multi-faceted sources and perspectives, while enabling collaborative contribution
from various stakeholders.
Community-driven knowledge could aggregate new knowledge from
domain experts and enable the development of specialized LLMs, tailored to specific industries or
applications.
Wikipedia factoids, biomedical literature,
mathematical formulae, and commonsense knowledge graphs are all valuable knowledge components
in various contexts, thus LLMs should be able to represent and incorporate knowledge contributed by
stakeholders across multi-faceted domains and industries.
Similarly, the
collaborative nature of knowledge is reflected by enabling individuals to contribute trained knowledge
cards on their desired knowledge source to KNOWLEDGE CARD, expanding the knowledge of
general-purpose LLMs through community-driven efforts.
In Workshop on Broadening Research Collaborations 2022 , 2022.
Semantic blogging and decentralized knowledge management."
203,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-27,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"PMLR, 2022.
PMLR, 2020.
PMLR, 2019.
PMLR, 2023.
PMLR, 2021.
PMLR, 2017.
PMLR,
2022.
PMLR, 2020."
204,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-28,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"sports
biomedical literature
NLP papers
book corpusChoose an information 
source from the 
following:               ,   
                                       ,
                       ,
                         .
sports
biomedical literature
NLP papers
book corpus
sportsWhat kind of 
information do you 
need?
Springer, 2009.
(Yes or No)
Yes
Choose an information source from the following: sports, biomedical literature, NLP papers, book corpus."
205,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-29,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Ultimately, KNOWLEDGE CARD framework enables dynamic synthesis
and updates of knowledge from diverse domains.
Compared to existing approaches, knowledge cards
enable flexible and targeted information access, searching over domains, and employing private and
personalized knowledge sources.
It is illustrated that: 1) n1has a marginal impact, suggesting
that knowledge cards generate largely homogeneous knowledge even with temperature sampling
(Caccia et al., 2018); 2) larger n2leads to performance drops, suggesting that the three knowledge
selectors ensure knowledge quality; 3) n3 = 1 , where only one knowledge document is adopted at a
time (as in previous works (Sun et al., 2022; Shi et al., 2023)) is worse than larger values, showing
the advantage of multi-domain knowledge synthesis uniquely enabled by K NOWLEDGE CARD.
Compared to retrieval models and search engines, KNOWLEDGE CARD enables
flexible information seeking, searching over knowledge domains, and employing private knowledge
sources.
Modular ontologies: concepts,
theories and techniques for knowledge modularization , volume 5445.
However, if KNOWLEDGE
CARD is adopted for more fine-grained use cases, the “biomedical” domain could be further divided
into sub-domains and one knowledge card could be trained for each."
206,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-30,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"This research is supported in part by the Office of the Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract #2022-
22072200004.
This material is also funded by the DARPA Grant under Contract No.
We also gratefully acknowledge support from NSF CAREER Grant No.
IIS2125201, IIS2203097, and the Alfred P. Sloan Foundation Fellowship.
Accessed: 2023-09-27.
IEEE, 2022.
ACM
Computing Surveys , 55(12):1–38, 2023.
In addition, for other tasks (e.g.
Computation Resources Details We used a GPU cluster with 16 NVIDIA A40 GPUs, 1988G
memory, and 104 CPU cores for the experiments."
207,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-31,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Through extensive experiments, we demonstrate
thatKNOWLEDGE CARD achieves state-of-the-art performance on six benchmark
datasets.
As an increasing amount of powerful LLMs are released behind API
calls, not directly accessible, and are prohibitively expensive to train or adapt, KNOWLEDGE CARD
specifically focuses on augmenting black-box LLMs to enrich their knowledge capabilities.
We envision KNOWLEDGE CARD as an
initiative to encourage LM developers to collaborate in expanding the knowledge of large language
models while reducing the carbon footprint from retraining gigantic LMs from scratch.
Cuad: An expert-annotated nlp dataset
for legal contract review.
3) When new knowledge, information, and domains emerge, more knowledge cards could
be trained and uploaded to a model-sharing infrastructure (Wolf et al., 2020) by any member of the
machine learning community and adopted to improve general-purpose LLMs.
When general-purpose LLMs are released, everyone uses the
same LLM with the same API calls, while real-world users have heterogeneous use cases and
expectations that require personalization (Salemi et al., 2023).
While OPT-1.3B is adopted as the default architecture in this work,
other sizes of OPT , from 125M all the way up to tens of billions, could all be used to initialize
knowledge cards.
These two levels of
heterogeneity allow for flexibility in knowledge card training: larger and more capable models could
be trained on large corpora and extensive knowledge domains by compute-rich individuals, while
smaller knowledge cards trained on small and dedicated domains by computationally underprivileged
researchers could also help improve black-box LLMs, democratizing LLM research."
208,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-32,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"top-down is
advantageous in tasks and domains where external knowledge is not always necessary.
We expect top-down to perform better
when external knowledge is not always necessary.
This observation suggests that top-down
approaches are better at tasks where external knowledge is not always necessary."
209,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-33,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"We leverage the
widely adopted LUN misinformation detection
dataset (Rashkin et al., 2017) with both 2-way
and 4-way classification settings.
Misinformation Detection To examine whether KNOWLEDGE CARD successfully integrates multi-
faceted knowledge from diversified sources, we adopt the LUN misinformation dataset (Rashkin et al.,
2017) with two- and four-way classification settings.
Results in
Figure 2 demonstrate that patching the LLM with all five LMs leads to various levels of performance
gains on misinformation detection, while the most in-domain LMs (Wikipedia and news) lead to
greater improvements.
Twibot-22: Towards graph-based twitter
bot detection.
Compare to the knowledge: Graph neural fake news detection with external knowledge.
Multi-source multi-class fake news
detection.
robust fact verification with
contrastive evidence.
Defending against neural fake news.
” and report the results on the 2-way misinformation
task in Figure 7.
2) The LUN dataset (Rashkin et al., 2017) is a misinformation detection dataset
with two- or four-way classification settings, either with true/false only or fine-grained categories
of trusted, hoax, propaganda, or satire."
210,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-34,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Yes or No:Q: Who is the senior senator of 
Tom Brady ’s birthplace ?
Question : Who is 
the senior senator 
of Tom Brady ’s 
birthplace ?
Question : Who is the senior senator of Tom Brady ’s birthplace ?
Question : Who is the senior senator of Tom Brady ’s birthplace ?
Question : Who is the senior senator from Tom Brady’s birth place?
Question : Who is the senior senator from Tom Brady’s birth place?
Question : Who is the senior senator from Tom Brady’s birth place?"
211,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-35,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"In addition, parameter averaging (Matena & Raffel, 2022;
McMahan et al., 2017; Izmailov et al., 2018; Wortsman et al., 2022; Li et al., 2022; Gururangan et al.,
2023), model fusion (Don-Yehiya et al., 2022; Borzunov et al., 2022), continual learning (Jang et al.,
2021; Qin et al., 2022; Ke et al., 2022; Qin et al., 2023), and other collaborative approaches (K ¨opf
et al., 2023; Sha, 2023; Luo et al., 2023) have also shed light on the possibility of distributed LM
training.
Petals: Collaborative inference and fine-tuning
of large models.
Cold fusion: Collaborative descent for distributed multitask finetuning.
methods for detecting, updating,
and visualizing model beliefs.
Averaging weights leads to wider
optima and better generalization.
Beyond distillation: Task-level mixture-of-experts for efficient inference.
Mixture of experts: a literature survey.
Merging models with fisher-weighted averaging.
Model
soups: averaging weights of multiple fine-tuned models improves accuracy without increasing
inference time.
While existing modular LM proposals often require modular
sub-models to be of the same size and architecture for parameter averaging and model fusion (Li
et al., 2022), the knowledge cards in this work could be fully heterogeneous."
212,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-36,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"2206–2240.
933–952, Singapore, December 2023a.
2116–2138, July 2023c.
5557–5576, 2022.
2790–2799.
3154–3169, 2022a.
2890–2903, 2022b.
9802–9822, 2023.
2097–2118, July 2023.
3419–3448, 2022.
3479–3495, 2022.
2789–2810, 2022.
2655–2671, 2022.
3254–3265, 2022.
4602–4625,
2022.
23965–23998.
1–9, 2022.
5657–5673, 2022.
25"
213,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-37,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Following Kadavath et al.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,
George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.
Communications of the
ACM , 47(12):47–52, 2004.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.
Bhuwan Dhingra, Jeremy R Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and
William W Cohen.
In Houda
Bouamor, Juan Pino, and Kalika Bali (eds.
URL
https://aclanthology.org/2023.emnlp-main.59 .
Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit
Bansal, and Srinivasan Iyer.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,
Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas
Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al.
Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts,
and Matei Zaharia.
Andreas K ¨opf, Yannic Kilcher, Dimitri von R ¨utte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,
Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich ´ard Nagyfi, et al.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¨aschel, et al.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Stephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al.
Get your vitamin c!
Communica-
tions of the ACM , 57(10):78–85, 2014.
Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,
Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al."
214,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-38,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Published as a conference paper at ICLR 2024
KNOWLEDGE CARD: FILLING LLM S’ KNOWLEDGE GAPS
WITH PLUG-INSPECIALIZED LANGUAGE MODELS
Shangbin Feng1Weijia Shi1Yuyang Bai2
Vidhisha Balachandran3Tianxing He1Yulia Tsvetkov1
1University of Washington2Xi’an Jiaotong University3Carnegie Mellon University
shangbin@cs.washington.edu
ABSTRACT
By design, large language models (LLMs) are static general-purpose models,
expensive to retrain or update frequently.
Improving language models by retrieving from trillions of tokens.
Scaling instruction-finetuned language models.
Editing factual knowledge in language models.
Time-aware language models as temporal knowledge bases.
Few-shot learning with
retrieval augmented language models.
Augmented large language models with parametric knowledge guiding.
Red teaming language models with language models.
Lamp: When large
language models meet personalization.
Thrust: Adaptively propels large language models with external knowledge."
215,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-39,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"In
the third task, to evaluate the ability to update the knowledge of general-purpose LLMs, we curate
MIDTERM QA, a QA dataset focusing on the 2022 U.S. midterm elections while the knowledge
cutoff of LLMs is generally 2021 or earlier.
Experiments demonstrate that KNOWLEDGE CARD
outperforms all baselines by at least 55.6% on exact match scores, showcasing the ability for temporal
knowledge update while only adding one knowledge card trained on midterm election news with 100x
fewer parameters than the general-purpose LLM.
3)
To evaluate temporal knowledge update , we curate MIDTERM QA, a QA benchmark focusing on the
2022 U.S. midterm elections since the knowledge cutoff of black-box LLMs is often 2021 or earlier.
MidtermQA To examine whether KNOWLEDGE CARD could update the parametric knowledge
of LLMs, we train an additional knowledge card on news articles regarding the 2022 U.S. midterm
elections and plug it into KNOWLEDGE CARD.
(Codex + Contriever) that uses
the same midterm election news as retrieval corpora.
Race Codex K NOWLEDGE CARD
AL, senate Doug Jones ✗ Katie Britt ✓
PA, senate Bob Casey ✗ John Fetterman ✓
CA, 3rd Mike Thompson ✗ Kevin Kiley ✓
IN, 2nd Jackie Walorski ✗ Jim Banks ✗
NV , governor Steve Sisolak ✗ Joe Lombardo ✓
Table 5: While vanilla Codex falsely claims
that these incumbents won again in the 2022
elections, KNOWLEDGE CARD successfully
updates the knowledge of black-box LLMs.Qualitative Analysis We curated MIDTERM QAto
evaluate whether KNOWLEDGE CARD enables effi-
cient knowledge update.
We examine the 88 races
where the incumbent was not re-elected: Codex an-
swered 1 out of the 88 questions correctly, while
bottom-up andtop-down with automatic and explicit
selection answered 63, 77, and 42 correctly.
Table 5
shows that Codex states the incumbents would win
again in 2022, while KNOWLEDGE CARD success-
fully updates LLMs with 100x more parameters.
A. Christopher Hoeppner B. Doug Mastriano C. Chrissy Houlahan D. Guy Ciarrocchi
Answer : C
Table 8: Examples of the MidtermQA dataset for the three settings.
IMDB movie reviews (Wang et al., 2023), political knowledge graph KGAP (Feng et al., 2021),
legal contracts (Hendrycks et al., 2021), math problems (Saxton et al., 2019), midterm election news
(ours), open subtitles2, political news corpora POLITICS (Liu et al., 2022c), biomedical literature3,
RealNews (Zellers et al., 2019), Reddit (Feng et al., 2023b), Twitter (Feng et al., 2022), Wikidata
knowledge graph (Vrande ˇci´c & Kr ¨otzsch, 2014), Wikipedia dump4, YAGO (Pellissier Tanon et al.,
2020), and Yelp reviews5.
3) We curate MidtermQA , a QA dataset focusing on the 2022
U.S. midterm elections to evaluate KNOWLEDGE CARD’s ability for temporal knowledge update.
Specifically, we first collect the results of the 510 races in congressional, senate, or gubernatorial
elections in the 2022 midterms.
We then construct three datasets: a) open-book, where we ask LLMs
to directly answer the name of the election winner for a given race, b) two-way, where we ask LLMs
to choose the winner from the two front runners, and c) four-way, where we increase the difficulty
by including two other politicians contesting in the same state to create a distraction.
We present
examples of the MidtermQA dataset in Table 8."
216,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-40,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Generated knowledge prompting
approaches (Shin et al., 2020; Liu et al., 2022a; Sun et al., 2022) prompt LLMs to incorporate and
generate contextual documents to encourage knowledge-aware generation.
1Published as a conference paper at ICLR 2024
they are hardly compatible with the current landscape of model sharing (Wolf et al., 2019) and do not
facilitate community-driven efforts to fill in LLMs’ knowledge gaps.
However, this could introduce unnecessary information in
the LLM’s prompt context (Zhao et al., 2023).
(2022), who showed that
LLMs possess preliminary abilities to identify their inherent knowledge limitations, we propose the
top-down approach, putting the LLM in charge to iteratively identify whether external knowledge is
needed and selectively activate relevant knowledge cards through various strategies.
; Press et al., 2022): specifically, we introduce a set of in-context
learning examples that encompass two distinct categories of questions posed to the LLM.
In this case, the corresponding output
label for the query is “Yes.” In this way, we prompt the LLM to learn to request external knowledge
through in-context learning; we analyze the effectiveness of this approach in Section 5.
In addition, generated knowledge prompting
approaches (GKP, recitation, GRTR) underperform vanilla Codex, showing that probing LLMs for
explicit knowledge is counterproductive when internal LLM knowledge is outdated or wrong.
Generated knowledge prompting
(Liu et al., 2022a) is one of the early approaches to tap into the parametric knowledge of LLMs by
prompting them to generate background information and re-using it for QA.
While recent works propose to edit LLM knowledge (Meng et al., 2022; Hernandez et al.,
2023), they are hardly compatible with black-box LLMs.
In addition, parametric knowledge in LLMs
is far from modular and collaborative, while LMs should be able to incorporate knowledge contributed
by all stakeholders in LLM research and applications.
User-centric LLM adaptation.
Our analysis ( §5) shows that this strategy is effective but far from perfect: LLMs
are occasionally over-confident about their knowledge capabilities.
We leave it to future work on
designing better strategies for LLMs to abstain, acknowledge knowledge limitations, and seek help
from external information sources."
217,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-41,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"These specialized LMs are later prompted to generate background
information to support general-purpose LLMs.
We use Codex
(CODE -DAVINCI -002) (Chen et al., 2021) as the default, general-purpose, black-box LLM.
Specifically, n1controls how many
7Published as a conference paper at ICLR 2024
12345
n1 (n2 = 5, n3 = 3)0.850.90
35 81012
n2 (n1 = 3, n3 = 3)0.850.90
12345
n3 (n1 = 3, n2 = 5)0.850.90
ACC
Macro F1
Figure 4: Investigating the impact of n1,n2, and n3, which
govern the knowledge stream from modular knowledge cards
to general-purpose LLMs.
documents each LM generates, n2controls how many are retained after the three knowledge selectors,
andn3controls how many are put into the context of LLMs.
We
illustrate LLM responses along with the correctness of their answer
in Figure 6.
However, existing modular LMs mostly operate in the white-box setting, i.e.assuming
access to the model parameters, token probabilities, and more.
1) While Codex is the default LLM in the experiments, KNOWL -
EDGE CARD also works with TEXT -DAVINCI -003 and GPT-3.5- TURBO (§5) and could be easily
adapted to future LLMs.
Different color boxes indicate knowledge
documents generated by different specialized LMs."
218,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-42,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Its modularity will ensure that
relevant knowledge can be continuously updated through the collective efforts of
the research community.1
1 I NTRODUCTION
Large language models (LLMs) have demonstrated an impressive ability to encode world knowledge
in model parameters (Petroni et al., 2019; Roberts et al., 2020).
Factuality Selector Language models are prone to hallucination (Ji et al., 2023) and the knowledge
cards are no exception.
Spinning language models: Risks of propaganda-as-a-
service and countermeasures.
Do language models have beliefs?
Language models (mostly)
know what they know.
Large language
models struggle to learn long-tail knowledge.
Language models as knowledge bases?
How much knowledge can you pack into the
parameters of a language model?
We envision two lines of
approaches towards this ethical risk: on the technical side, research on adversarial manipulation of
language models and corresponding defense tactics (Bagdasaryan & Shmatikov, 2022; Perez et al.,
2022) could be integrated to alleviate the impact of malicious knowledge cards; on the social side, we
could rely on and reinforce the existing rules for model sharing on popular infrastructures (Wolf et al.,
2020) to prevent such malicious contribution from happening."
219,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-43,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"We then
propose three content selectors to dynamically select and retain information in
documents generated by knowledge cards, specifically controlling for relevance ,
brevity , and factuality of outputs.
We then propose three levels of knowledge selectors to
dynamically select and refine generated documents and control for topic relevance, document brevity,
and knowledge factuality ( §2.2).
In addition, different communities might
2Published as a conference paper at ICLR 2024
Relevance  SelectorPruning  SelectorFactuality SelectorAnswer:
Relevance SelectorDo you need more information?
To this end, we
propose to select and retain knowledge documents based on relevance.
Concretely, given a set of m
generated documents {d1,···,dm}and the query q, we aim to retain the top- krelevant documents
and discard irrelevant information.
To effectively incorporate generated documents from multiple
LMs while fitting into the LLM context length limit, we propose to prune knowledge documents.
Formally, given mdocuments {d1,···,dm}, we adopt a pruning model prune( ·), operationalized
most simply as a summarization system (Zhang et al., 2020; Liu et al., 2022b), to obtain the condensed
versions separately {˜d1,···,˜dm}.
Given a set of mpruned knowledge documents {˜d1,···,˜dm}, their original
versions {d1,···,dm}, and the query q, we filter out the non-factual knowledge and retain ℓ
documents.
Specifically, given a knowledge
document d, we retrieve kdocuments from a retrieval corpus t1, .
Formally, we first obtain Dkas the set of knowledge
documents with the top- kfactuality scores where k > ℓ is a hyperparameter.
We then define a
sampling probability distribution over all mknowledge documents:
p(˜di|q) =(
exp(sdi)/P
dj∈Dkexp(sdj),if˜di∈ Dk.
We sample ℓknowledge documents from {˜d1,···,˜dm}with probabilities {p(˜d1|q),···, p(˜dm|
q)}.
In this way, knowledge documents with very low factuality scores are strictly removed while
flexibility is built in through sampling from the knowledge with factuality scores near the top.
Formally, given nknowledge cards C={c1,···,cn}and the query q,
we generate n1documents with each knowledge card through temperature sampling (Holtzman
et al., 2019) to obtain {d1,···,dn×n1}.
We first apply the relevance selector to retain n2most
relevant documents {d1,···,dn2}, then conduct knowledge pruning through the pruning selector
{˜d1,···,˜dn2}, and finally leverage the factuality selector to obtain n3high-quality knowledge
documents {˜d1,···,˜dn3}.
In The Semantic Web: 17th International Conference, ESWC 2020, Heraklion, Crete, Greece,
May 31–June 4, 2020, Proceedings 17 , pp.
2) If better models for embedding space similarity, abstractive summa-
rization, and fact-checking are developed, the three knowledge selectors ( §2.2) could be seamlessly
updated.
, sn},TOP-K= 1)
end
ifEXP then
PROMPT += “Choose an information source from the following: ”
fors∈ Sdo
prompt += s
end
spec =LLM (PROMPT )
end
KNOWLEDGE =ϕfact(spec( q, n1),1)
PROMPT += “Knowledge: ” + KNOWLEDGE
end
ifRESPONSE == “No” then
break
end
end
PROMPT += “Answer: ”
sans=LLM (PROMPT )
return sans
23Published as a conference paper at ICLR 2024
one billion tokensACL papersatomicbook corpusConceptNetbiomedical KGgutenbergIMDBpolitical KGlegal contractsmathopensubtitlesPOLITICSpubmedrealnews_1realnews_2realnews_3realnews_4reddittwitterwikidatayagoyelpWikipediaelectrical engineering
management
college CS
nutrition
medical genetics
high school european history
computer security
college biology
high school biology
high school microeconomics
high school world history
college physics
high school geography
clinical knowledge
high school macroeconomics
marketing
security studies
human aging
high school CS
elementary mathematics
conceptual physics
anatomy
global facts
us foreign policy
college chemistry
logical fallacies
sociology
high school us history
miscellaneous
college medicine
virology
jurisprudence
prehistory
business ethics
international law
high school statistics
high school psychology
professional psychology
professional accounting
astronomy
high school mathematics
professional medicine
econometrics
high school government and politics
machine learning
moral disputes
high school physics
philosophy
formal logic
human sexuality
world religions
college mathematics
public relations
abstract algebra
high school chemistry0.71.40.713.8 1.40.00.00.00.70.00.70.01.40.70.00.70.70.00.00.00.062.8 0.713.8
0.01.00.01.01.91.01.91.90.01.01.01.90.01.91.01.90.01.00.00.01.949.5 0.030.1
0.01.00.010.017.0 1.00.00.01.03.01.00.00.03.00.00.01.00.01.01.01.051.0 1.07.0
0.30.30.30.70.71.30.30.00.00.30.00.70.00.00.30.30.00.70.30.00.361.8 0.031.4
0.00.01.04.03.07.00.00.00.00.00.00.02.01.00.01.02.00.00.01.00.043.0 0.035.0
0.00.60.00.00.60.01.20.63.00.00.00.01.20.00.01.21.80.01.20.010.338.8 0.039.4
1.01.01.03.07.02.00.02.03.00.00.01.00.00.03.01.00.00.03.00.02.041.0 3.026.0
0.70.00.04.90.70.70.00.00.00.70.70.70.00.00.00.00.00.00.00.70.065.3 0.724.3
0.00.00.33.22.30.30.60.00.60.00.00.60.60.30.30.30.60.30.30.60.070.6 0.317.4
0.00.40.81.30.01.71.70.40.80.80.00.80.80.41.70.40.00.40.40.80.063.4 0.422.3
0.41.31.32.50.40.41.30.08.02.10.80.40.80.40.41.31.30.00.80.48.049.8 0.417.3
1.01.00.028.4 0.00.00.00.00.00.00.01.00.00.00.01.00.00.01.01.00.057.8 1.06.9
0.50.50.00.51.51.00.00.51.00.51.00.00.00.00.00.50.50.50.00.02.053.0 1.035.4
0.40.40.84.91.51.50.00.40.00.40.00.00.40.40.00.40.40.40.00.80.058.9 0.827.5
0.80.31.02.60.51.00.51.01.31.01.01.00.01.00.81.00.50.81.01.31.063.8 0.815.9
0.00.90.00.90.91.30.42.60.90.40.90.00.00.40.90.40.91.70.90.40.455.1 3.026.9
0.80.81.62.01.24.51.61.22.02.01.21.21.21.20.81.60.80.40.42.01.228.6 2.938.4
0.41.30.42.20.42.20.01.81.32.20.90.90.01.30.00.40.40.40.00.00.957.4 0.024.7
1.01.00.07.025.0 2.00.00.01.00.00.02.01.02.00.01.02.00.01.04.01.038.0 0.011.0
1.61.30.35.03.41.91.10.51.10.81.31.30.31.31.11.31.10.81.90.31.362.4 0.87.9
0.40.00.916.6 0.40.90.40.00.00.40.00.40.41.30.00.00.90.40.00.40.062.6 0.413.2
0.00.70.01.50.71.50.00.00.00.00.00.00.01.50.00.71.50.70.70.00.077.0 0.712.6
0.00.00.01.00.01.00.00.05.00.00.00.00.00.00.01.01.01.00.00.04.035.0 0.051.0
1.00.00.02.00.02.00.01.07.02.03.05.00.00.00.00.03.00.00.00.01.040.0 0.033.0
0.00.00.040.011.0 0.00.00.00.00.01.00.01.01.00.00.00.01.00.00.00.033.0 0.012.0
1.20.01.24.31.80.61.20.00.01.80.60.61.81.20.00.60.60.00.61.82.532.5 0.644.2
0.00.00.01.00.00.50.00.51.51.50.00.51.00.00.50.50.01.00.01.01.021.9 0.067.7
1.50.00.52.01.01.50.01.521.1 2.90.50.01.51.00.00.50.51.00.50.56.921.1 0.533.8
0.10.40.32.00.90.30.40.60.81.00.30.40.00.40.30.50.40.00.10.81.055.3 0.333.6
0.00.60.65.81.70.60.00.00.01.20.60.00.60.00.60.00.00.00.60.00.058.4 0.628.3
0.60.00.01.21.23.00.61.20.00.60.00.00.00.00.01.80.60.00.01.21.249.4 1.236.1
0.90.90.91.90.90.00.00.01.920.4 0.90.00.90.00.00.90.00.91.90.00.032.4 0.034.3
0.30.30.61.50.90.00.60.00.00.00.30.00.30.00.30.30.90.60.00.31.950.9 0.039.8
2.00.00.01.01.01.03.03.01.04.01.02.01.00.01.00.00.00.00.03.02.026.0 0.048.0
0.00.00.00.00.00.80.00.01.719.0 1.70.00.00.00.00.00.80.00.00.02.540.5 0.832.2
0.50.50.52.81.40.50.90.50.90.00.90.50.90.50.00.50.50.50.50.50.068.5 0.018.1
0.71.30.21.71.31.30.71.10.61.11.10.61.10.90.70.90.21.30.60.60.663.5 0.617.6
0.50.70.21.30.30.70.30.80.71.80.80.80.30.30.70.00.50.50.30.50.353.3 0.534.0
0.40.70.70.70.40.00.41.10.43.51.10.71.40.71.11.10.00.40.40.70.469.1 0.414.5
0.70.72.014.5 1.30.70.70.00.00.00.70.70.70.01.30.00.70.00.70.72.046.1 0.026.3
1.10.02.216.3 7.40.40.72.61.11.11.51.51.51.90.70.70.40.00.41.51.152.2 1.52.2
0.00.00.00.00.00.00.00.00.00.40.00.00.00.00.00.00.00.00.00.00.061.8 0.037.9
0.90.00.03.57.90.90.00.90.91.80.01.81.80.90.90.91.80.00.00.01.862.3 0.011.4
0.50.00.00.00.00.50.50.517.1 6.20.00.50.50.00.00.00.00.50.00.00.040.4 0.532.1
0.90.91.83.625.9 0.00.90.00.90.00.90.00.00.00.00.90.00.90.90.90.050.9 0.09.8
0.90.60.90.30.31.70.30.61.21.40.30.31.40.30.90.30.90.90.00.61.235.0 0.349.7
0.01.30.020.5 2.60.00.00.00.00.70.00.70.70.70.00.00.00.00.70.70.067.5 0.04.0
0.00.60.01.90.30.00.00.01.90.60.00.00.00.61.00.30.30.30.00.30.028.0 0.063.7
1.61.60.015.1 0.02.40.00.80.83.20.80.81.61.62.40.00.81.60.81.60.848.4 4.09.5
0.02.30.80.80.03.10.00.00.00.00.00.00.00.00.00.00.01.50.80.80.055.0 0.834.4
0.60.00.00.00.60.60.00.00.60.60.00.00.00.00.60.00.60.00.01.21.847.4 0.045.6
0.01.01.015.021.0 0.01.01.00.00.02.03.00.02.00.02.01.00.01.01.00.039.0 0.09.0
1.80.90.00.92.70.90.92.71.81.81.80.00.90.00.00.00.90.00.91.80.032.7 2.743.6
4.00.02.011.016.0 0.00.01.02.01.00.02.02.01.01.03.02.01.01.02.00.042.0 2.04.0
0.50.00.020.2 7.40.00.00.50.00.50.00.00.01.00.00.00.00.00.00.00.064.5 0.05.4
Figure 8: Statistics of knowledge card selection in the top-down approach with automatic selection
across 57 sub-tasks in the MMLU benchmark."
220,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-44,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Knowledge cards serve as parametric repositories that are selected
at inference time to generate background knowledge for the base LLM.
Specifically, the bottom-up approach starts by prompting all
knowledge cards to generate multiple documents, then performs selection with the three knowledge
selectors, while concatenating the final knowledge paragraph with the query for LLM generation.
This motivates us to propose the top-down approach, where the general-purpose LLM itself
decides whether external knowledge is necessary for the given query, then relevant knowledge cards
are selectively activated for knowledge integration; this process is repeated until the general-purpose
LLM has enough confidence to generate a response.
We then use them to
produce background knowledge for the general-purpose LLMs, while employing three knowledge
selectors to ensure quality in knowledge synthesis ( §2.2).
Finally, we propose bottom-up andtop-
down , two approaches to condition the LLM on the content sourced from knowledge cards and
post-processed using the knowledge selectors (§2.3).
Given a query to the LLM, these knowledge cards are selectively activated and used with
prompted generation.
Formally, given query q, specialized LM cdefines a mapping c(q) :q→dq
where qis used as prompt to generate a continuation as the knowledge document dq, which are later
prepended into the context of general-purpose LLMs through various mechanisms (§2.3).
In this way, the modularity of knowledge is demonstrated through the effortless addition, removal, or
selective activation of various knowledge cards during the LLM generation process.
Relevance Selector While we expect knowledge cards to generate background information that is
relevant and helpful to the query q, LMs sometimes deviate from the query (Holtzman et al., 2019).
We additionally propose a top-down approach, in
which the LLM proactively seeks external information from selected knowledge cards.
If the LLM
answers “ No”, we directly prompt the LLM to generate based on the query, without resorting to
knowledge cards.
If the LLM requests external knowledge by answering “ Yes”, we employ two
strategies (Algoithm 2) to select a relevant knowledge card and generate background knowledge.
•Automatic Selection (AUTO ) We further prompt the LLM with “ What kind of information do you
need?
” and select one knowledge card based on its response rq.
Concretely, we identify which
LM description {s1,···,sn}is most relevant to rqwith the relevance selector ( §2.2) and activate
the corresponding LM to generate multiple knowledge documents, then select one with the highest
factuality score based on the factuality selector (§2.2) to obtain d.
•Explicit Selection (EXP) Alternatively, we ask the LLM to directly select one knowledge card
by prompting with “ Choose an information source from the following: s1, .
If the LLM
responds with si, we activate the corresponding knowledge card cito generate multiple knowledge
documents and select one with the factuality selector (§2.2) to obtain d.
Upon obtaining the document, we append “ Knowledge: d” to the LLM context.
In this way, the top-down approach enables LLMs
to take charge in identifying their inherent knowledge limitations and seeking help from external
knowledge cards proactively."
221,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-45,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"In Proceedings of the
2021 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies , pp.
In Proceedings of the 2022 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies , pp.
In Proceedings of the 2021
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies , pp.
In Proceedings of the
2022 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies , pp.
In Proceedings of the 2022 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies , pp.
In Proceedings of the 2021 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies , pp.
In Proceedings of the 2022 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies , pp.
Huggingface’s transformers:
State-of-the-art natural language processing.
Transformers: State-of-the-art
natural language processing.
(2023a) to construct textual
corpora and use them as training data."
222,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-46,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Please provide the 
state Tom Brady is 
from.What kind of 
information do you 
need?
Please provide the 
state Tom Brady is 
from.What kind of 
information do you 
need?
Please provide the 
state Tom Brady is 
from.
The state Tom Brady is from."
223,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-47,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes
notwithstanding any copyright annotation therein."
224,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-48,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Both bottom-up andtop-down consistently improve different
LLMs across various datasets and evaluation metrics.
Recent advances incorporated frozen (Mallen et al., 2023; Si et al., 2022; Khattab
et al., 2022) and trainable retrievers (Shi et al., 2023) as well as search engines (Press et al., 2022)
to augment LLMs.
Transformer memory as a differentiable search index.
For the black-box LLMs, we used the OpenAI
API to access CODE -DAVINCI -002, TEXT -DAVINCI -003, and GPT-3.5- TURBO in the experiments."
225,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-49,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"We first ask the LLM a yes/no question to determine
whether external knowledge is needed for the given query q, specifically “ Do you need more
information?
(Yes or No) ”.
The first
category consists of questions that the LLM is capable of answering accurately without the need for
any extra information.
For these questions, the response to the query “Do you need more information
(Yes or No)?” is “No.” The second category comprises questions that the LLM cannot answer
correctly without the provision of additional information.
(Yes or No) ” again, repeat the above process, until the LLM
answers “ No” and generates a knowledge-informed response.
Yes/No in Top-Down Intop-down (§2.3), we begin by asking
LLMs if they might need external knowledge for the given query and
adopt in-context examples to encourage well-calibrated answers.
The vast majority of queries are mapped to the “yes,
correct” and “no, correct” categories, suggesting that LLMs have
preliminary abilities to “know what they know” and seek external
information if necessary.
Prompting LLMs to seek help through yes/no questions is not perfect.
3 yes 2 yes, 1 no 2 no, 1 yes 3 no0200400600800100012001400
Figure 7: Yes/No questions in
top-down are mostly consis-
tent across three prompt tem-
plates, while there is space for
improvement in future work.Yes/No Template Sensitivity In the top-down approach, we
prompt LLMs with “ Do you need more information?"
226,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-50,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"In 2022 IEEE Symposium on Security and Privacy (SP) , pp.
In International conference on
machine learning , pp.
In International conference on machine learning , pp.
In International Conference on Machine Learning , pp.
In 34th Conference on Uncertainty in Artificial Intelligence 2018,
UAI 2018 , pp.
In International Conference on Machine Learning ,
pp.
In International Conference on Machine Learning ,
pp.
In
First Workshop on Interpolation Regularizers and Beyond at NeurIPS 2022 , 2022.
The Artificial
Intelligence Review , 42(2):275, 2014.
In Artificial intelli-
gence and statistics , pp.
In Proceedings of the AAAI conference on artificial intelligence , volume 31,
2017.
In International Conference on Machine Learning , pp.
In International Conference on Machine Learning ,
pp.
In Proceedings of the IEEE international conference on computer
vision , pp.
We illustrate the selection results of
the automatic selection strategy on the MMLU dataset separated into the 57 sub-tasks.
E E XPERIMENT DETAILS
Algorithm Details We present an algorithmic sum-
mary of the bottom-up andtop-down approach in
Algorithm 1 and 2."
227,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-51,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,".Yes No
auto selection exp selection
general -purpose LLM
Bottom -Up Bottom -Up Top-Down Top-DownChoose an information 
source from the 
following:               ,   
                                       ,
                       ,
                         .
Among them, the top-down approach with explicit selection performs best, improving
Codex by 6.6% overall accuracy.
Answer :
Table 9: Prompt example for the bottom-up approach.
(Yes or No)
No
Answer :
Table 10: Prompt example for the top-down approach with automatic selection.
(Yes or No)
No
Answer :
Table 11: Prompt example for the top-down approach with explicit selection."
228,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-52,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Finally, we propose two complementary integra-
tion approaches to augment the base LLM with the (relevant, factual) knowledge
curated from the specialized LMs.
The lack of knowledge modularity has made gener-
alization to new domains and targeted updates of knowledge stored in LMs difficult.
While the bottom-up approach uniquely enables multi-domain knowledge synthesis, it also presents
the risk of presenting irrelevant information to LLM in contexts where external information is not
needed.
We propose bottom-up and
top-down to integrate general-purpose LLMs with modular and specialized LMs for multi-domain
knowledge synthesis ( bottom-up ) and proactively seeking external knowledge ( top-down ).
Pruning Selector Existing works mostly integrate one piece of external knowledge into LLMs (Sun
et al., 2022; Shi et al., 2023), while tasks requiring integration of multiple domains of information,
3Published as a conference paper at ICLR 2024
such as misinformation detection (Karimi et al., 2018) and multi-hop QA (Nishida et al., 2019), are not
well supported by existing paradigms.
This pruning method allows for the integration into the main
LLM of information from multiple domains while preserving space for in-context learning.
Bottom-up enables multi-domain knowledge
synthesis across all available sources, but these might occasionally introduce irrelevant information
which may adversely impact LLM inference.
However, this line of work assumes that the encoded knowledge
in LLM parameters is all we need, while LLM knowledge suffers from hallucination (Ji et al., 2023),
struggles to encode long-tail facts (Mallen et al., 2023), and can not be efficiently updated (De Cao
et al., 2021)."
229,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-53,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"We first introduce knowledge
cards —specialized language models trained on corpora from specific domains
and sources.
We first
curate specialized LMs, knowledge cards , trained on corpora from diverse sources and domains to
serve as modular knowledge repositories ( §2.1).
We train various knowledge cards , LMs trained on
specialized knowledge corpora from diversified domains and sources ( §2.1).
We train knowledge cards on various knowledge
domains and employ three knowledge selectors for quality control.
To this end, we propose to curate knowledge cards , specialized LMs that are much smaller than
black-box LLMs, trained on diversified knowledge corpora from a wide range of domains and sources.
Concretely, we obtain nknowledge cards C={c1,c2,···,cn}, each starting from an existing LM
checkpoint and further trained on a specific knowledge corpora Diwith the causal language modeling
objective.
3 E XPERIMENT SETTINGS
Implementation Forknowledge cards , we use OPT-1.3B (Zhang et al., 2022) as the starting point
and separately train 25 specialized LMs on a wide range of knowledge sources and domains, including
corpora in the Pile (Gao et al., 2020), branch-train-merge (Li et al., 2022), knowledge graphs (Speer
et al., 2017; West et al., 2022; Vrande ˇci´c & Kr ¨otzsch, 2014; Pellissier Tanon et al., 2020; Feng et al.,
2021; Zhang et al., 2021), news and social media (Liu et al., 2022c; Feng et al., 2023b), and more.
52.1 54.5 84.7 61.4
GenerateGKP 45.0 46.9 89.1 53.5
RECITATION 44.4 46.4 89.3 52.3
GRTR 55.6 58.4 77.4 59.0
KNOWLEDGE CARDBOTTOM -UP 83.6 85.6 81.6 64.5
TOP-DOWN AUTO 87.5 89.3 89.5 63.0
TOP-DOWN EXP 75.3 75.7 91.9 67.6
Table 3: Performance on MidtermQA.
Generated Knowledge Prompting LMs acquire knowledge through training on gargantuan textual
corpora (Petroni et al., 2019; Dhingra et al., 2022; He et al., 2021).
We first present knowledge cards, specialized LMs trained
on various domains and sources of knowledge, and propose three knowledge selectors to ensure
knowledge quality.
Knowledge graph based synthetic
corpus generation for knowledge-enhanced language model pre-training.
By default, KNOWLEDGE CARD employs
language models trained on varying domains and corpora as modular knowledge sources.
While knowledge cards could be of any
size or model architecture, we used OPT-1.3B , a relatively small language model to initialize knowl-
edge cards trained on different domains and sources.
Knowledge Card Selection In the top-down approach, we ask large language models to choose
relevant knowledge cards and obtain external knowledge.
LM Training Inference Stage
Hyperparameter Value Hyperparameter Value
LEARNING RATE 2e-5 n1 3
WEIGHT DECAY 1e-5 n2 5
MAX EPOCHS 10 n3 3
BATCH SIZE 32 MAX ITERATION 1
OPTIMIZER ADAM TEMPERATURE 0.1
ADAM EPSILON 1e-6 DEFAULT LLM CODEX
ADAM BETA 0.9,0.98
WARMUP RATIO 0.06
Table 6: Hyperparameter settings.Knowledge Card Accumulation We expect
KNOWLEDGE CARD to perform better when
relevant knowledge cards are gradually added
to the system.
Knowledge Cards Domains We train a total of 25 knowledge cards from the following corpora
and domains: one billion tokens (Chelba et al., 2013), ACL papers (Lo et al., 2020), commonsense
knowledge graph ATOMIC (West et al., 2022), book corpus (Zhu et al., 2015), ConceptNet (Speer
et al., 2017), biomedical knowledge graph UMLS (Zhang et al., 2021), Gutenberg (Rae et al., 2019),
20Published as a conference paper at ICLR 2024
//open-book setting
Question : Who won the senate race of Oregon in the 2022 U.S. midterm elections?
Training knowledge cards took from around 7 hours
to 10 days depending on the training corpora size."
230,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-54,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Social STEM Other All
Vanilla LMCODEX 74.2 76.9 57.8 70.1 68.3
PALM 77.0 81.0 55.6 69.6 69.3
FLAN-PALM - - - - 72.2
RetrievalATLAS 46.1 54.6 38.8 52.8 47.9
REPLUG 76.0 79.7 58.8 72.1 71.4
REPLUG LSR 76.5 79.9 58.9 73.2 71.8
GenerateGKP 73.3 74.5 59.5 71.4 70.0
RECITATION 76.9 78.1 59.0 74.0 71.9
KNOWLEDGE CARDBOTTOM -UP 77.2 76.7 57.9 72.2 70.7
TOP-DOWN AUTO 77.7 78.9 59.2 73.0 72.0
TOP-DOWN EXP 78.6 80.9 59.6 74.3 72.8
Table 1: Model performance on the MMLU Bench-
mark.
KNOWLEDGE CARD improves Codex by at
least 3.5% while top-down outperforms all baselines.Type ModelTwo-Way Four-Way
BAcc MaF BAcc MaF
Vanilla LM CODEX 65.6 51.0 52.8 44.0
RetrievalREPLUG 78.8 67.8 55.8 53.0
REPLUG LSR 78.8 68.5 57.5 54.4
GenerateGKP 73.5 60.3 61.1 46.3
RECITATION 65.0 47.7 64.2 48.6
GRTR 66.1 49.1 51.6 36.9
KNOWLEDGE CARDBOTTOM -UP 89.8 87.3 70.6 67.3
TOP-DOWN AUTO 86.4 78.7 63.0 60.2
TOP-DOWN EXP 91.3 86.0 69.4 65.5
Table 2: Performance on misinformation de-
tection.
Type ModelOpen-Book Multiple-Choice
EM F1 2-way 4-way
Vanilla LM CODEX 55.1 57.9 90.9 60.8
RetrievalREPLUG 44.8 - 85.7 62.8
REPLUG LSR 37.2 - 86.9 65.3
SI ET AL .
ModelTwo-Way Four-Way
BAcc MaF BAcc MaF
REPLUG 78.8 67.8 55.8 53.0
REPLUG LSR 78.8 68.5 57.5 54.4
BOTTOM -UP 90.0 87.0 65.3 63.3
TOP-DOWN AUTO 80.7 70.9 60.1 56.8
TOP-DOWN EXP 80.6 70.0 59.7 56.5
Table 4: KNOWLEDGE CARD outperforms
retrieval LM R EPLUG in the Wikipedia-only
setting, suggesting that modular LMs present
a better knowledge repository than retrieval.Knowledge Selector Study In Section 2.2, we pro-
pose three levels of knowledge selectors to control
for various factors and ensure knowledge quality.
Specialized LMs In order to assess
the effectiveness of modular specialized LMs as compared to non-parametric sources like retrieval,
we exclusively use the Wikipedia LM in KNOWLEDGE CARD and compare with the state-of-the-art
retrieval LM REPLUG that also uses Wikipedia as the retrieval knowledge source.
Compared to retrieval models, using
language models as knowledge sources enables flexible information seeking (rather than rigid token
exact match), searching over knowledge domains, and employing private knowledge sources.
However, domains with more
Wikipedia entries might be better supported by retrieved documents and might receive higher factuality
scores.
Figure 8
demonstrates that for most tasks knowledge selection exhibits spike-like patterns on Wikipedia
19Published as a conference paper at ICLR 2024
corpora and encyclopedic knowledge graphs, suggesting that the majority of tasks have a few clearly
relevant knowledge cards.
For knowledge graphs, we follow Feng et al."
231,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-55,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Adapters are also proposed for task transfer and parameter-efficient fine-tuning (Houlsby et al., 2019;
Pfeiffer et al., 2020; Zaken et al., 2022).
Mass-editing
memory in a transformer.
Mad-x: An adapter-based framework
for multi-task cross-lingual transfer.
Recyclable tuning for continual pre-training."
232,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-56,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Nearest neighbor zero-shot
inference."
233,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-57,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"In addition,
hyperparameters n1,n2, andn3enable fine-grained control over the knowledge synthesis process.
These hyperparameters enable fine-
grained control over the knowledge synthesis process.
Hyperparameters We present hyperparameter settings in Table 6."
234,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-58,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"For misinformation analysis that tests
multi-domain knowledge integration, KNOWLEDGE CARD outperforms all baseline approaches by
at least 15.8% and 10.0% balanced accuracy scores on two- and four-way classification settings.
2) To evaluate multi-domain
knowledge synthesis , we adopt misinformation
detection, since news articles often encompass
facts and opinions at the intersection of differ-
ent domains and perspectives.
ACC Macro F185.087.590.092.595.0
-1.4
-0.6-1.8
-1.0-3.3
-2.5full model
w/o pruningw/o relevance
w/o factualityFigure 3: Ablation study of the three knowledge
selectors on misinformation detection.
We
conduct ablation studies to remove each knowledge
selector in the bottom-up approach and re-evaluate
on misinformation detection.
Kgap: Knowledge graph augmented political perspective detection in news media.
Politics:
Pretraining with same-story article comparison for ideology prediction and stance detection.
Truth of varying
shades: Analyzing language in fake news and political fact-checking.
Detecting spoilers in movie reviews with external movie knowledge and user
networks.
The factuality selector is biased towards information-rich domains and existing knowledge."
235,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-59,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"For
general-purpose knowledge QA ,KNOWLEDGE CARD improves Codex performance by 6.6% on
MMLU and even outperforms the 3-times larger Flan-PaLM.
As shown in Table 1, all three configurations of KNOWLEDGE CARD significantly improve
vanilla Codex.
Table 2 demonstrates that KNOWLEDGE CARD
significantly improves Codex by at least 31.7% and 19.4% in balanced accuracy scores for both
settings.
This indicates that one
knowledge card with 1.3B parameters successfully updates the parametric knowledge of the 175B
Codex through KNOWLEDGE CARD.
To this end, we evaluate the change in performance when five
knowledge cards are separately added to augment Codex with the top-down approach."
236,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-60,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Transactions of
the Association for Computational Linguistics , 10:257–273, 2022.
Association for Computational Linguistics.
In
Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers) , pp.
In
Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers) , pp.
In
Proceedings of the 16th Conference of the European Chapter of the Association for Computational
Linguistics: Main Volume , pp.
InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and
the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) ,
pp.
In Proceedings of the 27th international conference on computational linguistics , pp.
InFindings of the Association for Computational Linguistics: EMNLP 2021 , pp.
In Proceedings
of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers) , pp.
In Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pp.
In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics , pp.
In Proceedings of the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pp.
In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics , pp.
Transactions of the Association for Computational Linguistics , 9:176–194, 2021.
In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers) , pp."
237,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-61,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, et al.
Suchin Gururangan, Margaret Li, Mike Lewis, Weijia Shi, Tim Althoff, Noah A Smith, and Luke
Zettlemoyer.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi.
Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, KIM Gyeonghun, Stan-
ley Jungkyu Choi, and Minjoon Seo.
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,
Andrea Madotto, and Pascale Fung.
Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang
Luong, and Orhan Firat.
Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A Smith, and Luke
Zettlemoyer.
Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig.
Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau.
Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke
Zettlemoyer.
Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese,
Nat McAleese, and Geoffrey Irving.
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis.
Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana V olkova, and Yejin Choi.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettle-
moyer, and Wen-tau Yih.
Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,
Sean Welleck, and Yejin Choi.
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and
Sanja Fidler."
238,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-62,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"11737–11762, July 2023b.
1121–1133, April 2021.
15696–15707.
1546–1557, 2018.
1354–1374, 2022c.
1273–1282.
Springer, 2020.
11328–11339.
19–27, 2015."
239,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-63,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,Prompting gpt-3 to be reliable.
240,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-64,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Language gans falling short.
Evaluating large
language models trained on code.
From pretraining data to language
models to downstream tasks: Tracking the trails of political biases leading to unfair NLP models.
Measuring and manipulating knowledge repre-
sentations in language models.
Towards continual knowledge learning of language models.
Survey of hallucination in natural language generation.
Openassistant
conversations–democratizing large language model alignment.
When not to trust language models: Investigating effectiveness of parametric and non-parametric
memories.
Measuring
and narrowing the compositionality gap in language models.
Symbolic knowledge distillation: from general language models to
commonsense models.
Generate rather than retrieve: Large language models are strong
context generators.
Greaselm: Graph reasoning enhanced language models."
241,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-65,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"The views and
conclusions contained herein are those of the authors and should not be interpreted as necessarily rep-
resenting the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government.
An important risk is the dual use
and exploitation from malicious actors."
242,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-66,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"N
B
GA
P
CH
S
...N
B
GA
P
CH
S
...
Dianne Feinstein
Dianne Feinstein
knowledge cards
 San Mateo is 
located in the 
northwest of 
California ...Dianne Feinstein, 
the senior senator 
from California, is 
rumored to retire ... Tom Brady returned 
to his hometown of 
San Mateo ...general -purpose LLM
Q: Who is the senior senator of Tom Brady ’s birthplace ?
San Mateo is located in the northwest of California ...
Dianne Feinstein, the senior senator from California ... Tom BradyKnowledge:
returned to his hometown of San Mateo ...
Answer :  San Mateo is located in the northwest of California ...
Dianne Feinstein, the senior senator from California ... Tom BradyKnowledge:
returned to his hometown of San Mateo ...
San Mateo is located in the northwest of California ."
243,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-67,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Scott Yih, Sebastian Riedel, and Fabio
Petroni.
Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin.
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave.
Michael S Matena and Colin A Raffel.
Fabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller.
Heiner Stuckenschmidt, Christine Parent, and Stefano Spaccapietra.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R ´emi Louf, Morgan Funtowicz, et al.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R ´emi Louf, Morgan Funtowicz, et al."
244,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-68,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"We follow the official demonstration set and test set
in our experiments.
We use the official test set in (Hu et al., 2021) with 2,999
examples throughout the experiments."
245,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-69,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"0, if˜di/∈ Dk.
D A NALYSIS (CONT .)"
246,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-70,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Retrieval vs.
retrieval and search engine) while
they could be complementary (Appendix A)."
247,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-71,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Italic texts indicate
that this field is generated by black-box LLMs.
Italic texts indicate that
this field is generated by black-box LLMs."
248,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-72,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"(Yes or No) ”
RESPONSE =LLM (PROMPT )
ifRESPONSE == “Yes” then
ifAUTO then
PROMPT += “What kind of information do you need?
”
RESPONSE =LLM (PROMPT )
spec =ϕrel(RESPONSE ,{s1, ."
249,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-73,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Kgpt: Knowledge-grounded pre-training
for data-to-text generation.
The curious case of neural text
degeneration.
Parameter-efficient transfer learning for
nlp.
Continual
pre-training of language models.
Generalization
through memorization: Nearest neighbor language models.
Branch-train-merge: Embarrassingly parallel training of expert language models.
Mpnet: Masked and permuted
pre-training for language understanding.
Recitation-augmented language
models.
Kepler: A unified model for knowledge embedding and pre-trained language representation.
Pegasus: Pre-training with extracted
gap-sentences for abstractive summarization.
Opt: Open pre-trained transformer language
models.
Training language models with memory augmentation."
250,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-74,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"HR001120C0124.
769–786.
876–885, 2018.
6265–6274.
583–596."
251,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-75,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Furthermore, only a handful of knowledge cards would be relevant for a given query.
Knowledge cards heterogeneity.
1) Different knowledge
cards could have different sizes.
In addition, 2) knowledge cards could have different model architectures.
Knowledge cards hierarchy.
We believe that knowledge cards could reflect the hierarchical nature
of knowledge.
18Published as a conference paper at ICLR 2024
B L IMITATIONS
Knowledge cards are not perfect knowledge generators.
juris prudence and high school U.S.
history), it is not clear which knowledge cards would be most helpful, thus the selection is more
spread-out.
, specn};
knowledge card names S={s1, ."
252,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-76,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"¡in-context examples with the same format¿
...
¡in-context examples with the same format¿
..."
253,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-77,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Specifically, bottom-up activates all available knowledge cards at once and employs the three
knowledge selectors to control for knowledge quality.
We expect the bottom-
up approach to be strong in multi-domain knowledge synthesis since multiple knowledge cards
could be activated at once to provide background knowledge from diverse perspectives.
Top-Down Approach Inbottom-up , we assume that every query would benefit from external
knowledge generated by knowledge cards.
bottom-up performs best
due to multi-domain knowledge integration.
In
addition, top-down generally outperforms bottom-up likely because MMLU contains math-related
questions that do not necessitate external knowledge.
In addition, bottom-up outperforms both variants of top-down , thanks to its methodology to
jointly activate knowledge cards from various domains and enable multi-domain knowledge synthesis.
In addition, top-down outperforms bottom-up , indicating that
the selective activation of knowledge cards is better when there is a specific knowledge card tied to
the task domain.
Combining bottom-up and top-down."
254,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-78,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Retrieval-augmented LMs (Guu et al., 2020;
Borgeaud et al., 2022; Shi et al., 2023) employ retrieval systems to fetch relevant documents from a
general and fixed retrieval corpus (e.g., Wikipedia or the Pile (Gao et al., 2020)), leveraging external
knowledge from non-parametric sources to aid LLM generation.
Extensive experiments demonstrate that KNOWLEDGE CARD outperforms vanilla LLMs, retrieval-
augmented LMs, and generated prompting approaches on three tasks across six datasets.
2.1 K NOWLEDGE CARDS
While existing approaches rely on one fixed source of knowledge to improve LLMs (one retrieval
corpus (Guu et al., 2020; Borgeaud et al., 2022; Shi et al., 2023), one knowledge graph (Wang et al.,
2021; Zhang et al., 2021; Feng et al., 2023c), or one pretrained LLM itself (Shin et al., 2020; Liu
et al., 2022a; Sun et al., 2022)), we hypothesize that since knowledge is modular, general-purpose
LLMs should be augmented with modular plug-and-play knowledge repositories that allow users to
collaboratively add, remove, edit, or update information.
2.2 K NOWLEDGE SELECTORS
While it is possible to directly adopt dqas relevant knowledge, we identify three key challenges in
the successful integration of knowledge cards and general-purpose LLMs: relevance, brevity, and
factuality.
2.3 K NOWLEDGE INTEGRATION
After defining the modular components in KNOWLEDGE CARD (a general-purpose LLM, knowledge
cards, and knowledge selectors), we propose two approaches, bottom-up andtop-down , to integrate
the general-purpose LLM with external knowledge sources, which are selected outputs of knowledge
cards.
4Published as a conference paper at ICLR 2024
The final prompt for the LLM is a concatenation of knowledge documents and the query, formally
[“Knowledge: ”∥˜d1∥˜d2∥ ··· ∥ ˜dn3∥q] where ∥denotes concatenation.
1) vanilla black-box LLMs: Codex (Chen et al., 2021), PaLM (Chowdhery et al., 2022),
and Flan-PaLM (Chung et al., 2022); 2) generated knowledge prompting approaches: GKP (Liu et al.,
2022a), recitation (Sun et al., 2022), GRTR (Yu et al., 2022) (Note that we apply these methods to
the same LLM Codex (Chen et al., 2021) for a fair comparison); 3) retrieval-augmented language
models: Atlas (Izacard et al., 2022), Si et al.
This suggests that when LLMs perform poorly on knowledge-intensive tasks,
an additional knowledge card trained on in-domain corpora could help with K NOWLEDGE CARD.
We
additionally evaluate KNOWLEDGE CARD with two other LLMs,
TEXT -DAVINCI -003 and GPT-3.5- TURBO , and present results in Fig-
ure 5.
Extensive experiments demonstrate that KNOWLEDGE CARD
outperforms vanilla LLMs, retrieval LMs, and generated knowledge prompting approaches across
three tasks and six datasets, showcasing its ability to integrate multiple sources of information,
efficiently update LLM’s knowledge, and more.
In addition,
KNOWLEDGE CARD is also compatible with 1) retrieval systems, where the retrieved text could
similarly go through the three knowledge selectors and enrich LLM context, while retrieval corpora
are harder to share and use than modular language models; 2) knowledge graphs, when combined with
various proposals to construct natural language corpora out of symbolic knowledge bases (Agarwal
et al., 2021; Chen et al., 2020; Feng et al., 2023a), which is already included in our prototype; 3)
search engines, where content on the web could also be integrated into the black-box LLMs through
KNOWLEDGE CARD.
Since the
integration of general-purpose LLMs and modular knowledge cards happens at the natural language
level, any language generation models could be adopted as knowledge cards.
Inspired by the findings
that LLMs do not need external knowledge for every query (Zhao et al., 2023) and language models
(mostly) know what they know (Kadavath et al., 2022), we propose to ask yes/no questions to
decide whether to activate knowledge cards and encourage well-calibrated answers through in-
context learning."
255,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-79,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"We then iteratively
ask “ Do you need more information?
We leave further explorations to future work.
” and “ Would you like additional
information?"
256,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-80,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Following previous works (Si et al., 2022;
Shi et al., 2023), we adopt a 5-shot in-context
learning setting.
All models are evaluated based on 16-shot in-context learning.
The official dataset also provides a demonstration set, i.e.5-shot examples in each
sub-task to enable few-shot in-context learning."
257,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-81,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"While the two lines of work have achieved some success, these existing systems struggle to reflect
two key properties of knowledge.
Knowledge is modular (Stuckenschmidt et al., 2009): it is an
“archipelago” rather than a single “continent”, encapsulating information that exists in diversified
forms, domains, sources, perspectives, and more.
have different definitions and requirements for knowledge.
Compatible with diversified forms of knowledge.
Similar divisions could be
applied to sub-fields in NLP research, political news in different countries, and more.
Knowledge :."
258,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-82,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"In Thirty-sixth Conference on Neural Information Processing Systems Datasets and
Benchmarks Track , 2022.
In Thirty-fifth Conference on Neural Information Processing Systems
Datasets and Benchmarks Track (Round 1) , 2021.
Communication-efficient learning of deep networks from decentralized data.
Elle:
Efficient lifelong pre-training for emerging data.
Analysing mathematical
reasoning abilities of neural models."
259,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-83,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Answer : Dianne Feinstein
Dianne Feinstein
knowledge documents
news biomedgeogra
phyNLP 
paperspoliticsConcept
Netart 
historysports ... news biomedgeogra
phyNLP 
paperspoliticsConcept
Netart 
historysports ...knowledge cardsbegin
knowledge documentsknowledge documents
Figure 1: Overview of KNOWLEDGE CARD.
Dianne Feinstein, the senior
senator from California, is rumored to retire ."
260,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-84,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"3554–3565, 2021.
8635–8648, 2020.
6491–6506, 2021.
3929–3938.
754–763, 2021.
9332–9346, 2020.
3577–3599, 2021.
4969–4983, 2020.
2335–2345, 2019.
4812–4829, 2021.
2463–2473, 2019.
7654–7673, 2020.
5418–5426, 2020.
624–643, 2021.
4222–4235, 2020.
4615–4629, 2020.
38–45, 2020."
261,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-85,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"To this end, we
propose KNOWLEDGE CARD, a modular framework to plug in new factual and
relevant knowledge into general-purpose LLMs.
To this end, we propose KNOWLEDGE CARD, a novel framework to empower general-purpose
LLMs with modular and collaboratively-sourced knowledge through the integration of smaller, but
specialized language models.
Finally, we propose bottom-up andtop-down —two approaches
to empower general-purpose LLMs by integrating outputs from specialized LMs (i.e.,plugging in
knowledge cards into the LLM) ( §2.3).
Our findings demonstrate the potential of filling in
the knowledge gaps of general-purpose LLMs by integrating modular and collaborative knowledge
from small, independently trained, and specialized LMs.
2 M ETHODOLOGY
We introduce KNOWLEDGE CARD, a novel framework to empower general-purpose LLMs with
modular and collaborative knowledge (Figure 1).
These results suggest that we present an
effective approach for making general-purpose LLMs better in knowledge-intensive contexts.
KNOWLEDGE CARD enables modular
patching of LLMs while in-domain knowledge
cards help the most.
5 A NALYSIS
Patching LLM Knowledge When general-purpose LLMs struggle at tasks due to knowledge
limitations, KNOWLEDGE CARD could serve as an efficient approach to patch LLM weaknesses by
adding specialized language models.
In contrast, we propose to reflect the modularity and community-driven nature of knowledge by
integrating plug-and-play knowledge cards with general-purpose LLMs.
To this end, we propose KNOWLEDGE CARD
as a community-driven initiative to empower general-purpose LLMs with modular and collaborative
knowledge through the sharing and re-using of knowledge cards.
Since the most prominent LLMs
are only released behind API calls, we propose KNOWLEDGE CARD with the aim of empowering
black-box general-purpose LLMs with community-driven and collaborative knowledge.
7 C ONCLUSION
We propose KNOWLEDGE CARD, a novel framework to empower general-purpose LLMs with
modular and collaborative knowledge.
We then propose bottom-up andtop-down approaches to integrate knowledge
cards with general-purpose LLMs to enable multi-domain knowledge synthesis and grounding in
external information when necessary.
We envision KNOWLEDGE CARD as a community-
driven initiative to empower general-purpose LLMs with modular and collaborative knowledge.
As a result, KNOWLEDGE CARD presents a preliminary approach by letting the user select
and activate knowledge cards to empower LLMs with different skill sets and domain expertise.
C E THICS STATEMENT
We envision KNOWLEDGE CARD as a community-driven and collaborative initiative to improve
general-purpose LLMs in knowledge-intensive tasks and contexts."
262,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-86,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"While approaches such as retrieval augmentation could be extended for modularity,
1Resources are available at https://github.com/BunsenFeng/Knowledge Card.
Concretely, for the nknowledge cards C={c1,···,cn}, we also ask the knowledge card contributors
to submit a textual description of LMs S={s1,···,sn}such as “ biomedical literature ”, “college
calculus ”, or “ commonsense knowledge graph ”.
Note that KNOWLEDGE
CARD is also compatible with multiple knowledge formats (e.g.
davinci w/ bottom-up w/ autocard w/ expcard70758085
+10.0+12.0
+9.6
turbo w/ bottom-up w/ autocard w/ expcard707580
+5.0
+2.2 +2.6Figure 5: KNOWLEDGE CARD
is compatible with other LLMs,
specifically TEXT -DAVINCI -003
and GPT-3.5- TURBO .
All components in KNOWLEDGE CARD are modular and easily substi-
tuted with future state-of-the-art.
Such flexibility and compatibility are possible since KNOWLEDGE CARD
conducts knowledge integration through natural language.
If KNOWLEDGE CARD is adopted for general question answering, then a general
biomedical knowledge card trained on PubMed corpus would suffice.
In addition, Appendix A discussed KNOWLEDGE CARD’s compatibility
with diverse knowledge sources, including retrieval, knowledge graphs, and search engines, while
these knowledge repositories have their respective pros and cons.
We leave it to future work on
integrating multiple types of external knowledge stores to extend K NOWLEDGE CARD.
Encyclopedic knowledge graph YAGO and Wikipedia
are generally the most adopted knowledge cards."
263,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-87,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Analyz-
ing the forgetting problem in pretrain-finetuning of open-domain dialogue response models.
Promptcap:
Prompt-guided task-aware image captioning.
Learning to retrieve prompts for in-context
learning."
264,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-88,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Concurrently, top-down approaches surpass all baselines, including
Flan-PaLM with a few hundred billion more parameters.
Base layers:
Simplifying training of large, sparse models.
Compressive transformers for long-range sequence modelling.
Hash layers for large sparse models."
265,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-89,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,Realtime qa: What’s the answer right now?
266,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-90,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"REFERENCES
Sharegpt.
https://sharegpt.com/ , 2023."
267,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-91,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"As they are increasingly adopted for
knowledge-intensive tasks, it becomes evident that these design choices lead to
failures to generate factual, relevant, and up-to-date knowledge.
However, they still face various
challenges in knowledge-intensive tasks and contexts: they suffer from hallucination (Kry ´sci´nski et al.,
2020; Pagnoni et al., 2021; Ji et al., 2023), struggle to encode long-tail facts (Kandpal et al., 2023;
Mallen et al., 2023), and could not be easily updated with new and emerging knowledge (De Cao
et al., 2021; Hase et al., 2021).
That being said, existing approaches and systems did not employ modular orcollab-
orative knowledge sources that enable the plug-and-play updates and contributions from various
stakeholders.
In addition, these works often leverage only oneretrieval corpora and assume that it’s
“omniscient” while suffering from various issues such as domain coverage and knowledge update.
As a result, not all of the generated knowledge
documents are high-quality knowledge statements, occasionally suffering from degeneration, topic
deviation, and more.
In addition, new and emerging knowledge might be well supported by existing retrieval
corpora and receive low factuality scores."
268,baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-92,deeplearning,knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf,"Generated knowledge prompting for commonsense reasoning.
Autoprompt:
Eliciting knowledge from language models with automatically generated prompts.
Chain-of-thought prompting elicits reasoning in large language models.
While the three knowledge selectors in part alleviate the impact of low-quality
generated knowledge documents, we hypothesize that improving the knowledge generation of autore-
gressive language models is an important, yet orthogonal, research question for future work.
Since language models are
sensitive to minor changes in prompts, we devise two more questions:
“Is more information needed here?
, sn}; max trial k; relevance and factuality selector
ϕrel;ϕfact; binary flags AUTO and EXP
Result: answer string sans
PROMPT =sicl
PROMPT += “Question: ” +q
i= 0
while i≤kdo
PROMPT += “Do you need more information?"
