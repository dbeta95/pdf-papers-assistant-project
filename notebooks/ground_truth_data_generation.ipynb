{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground truth data generation\n",
    "\n",
    "As can be seen in the notebook \"create_preprocessors.ipynb\" we create three (since as mentioned proposal chunking won't be considered) different preprocessing strategies to split our documents into chunks:\n",
    "\n",
    "* Sentences splitting\n",
    "* Semantic chunking\n",
    "* Sequential semantic chunking\n",
    "\n",
    "This notebooks creates the ground truth for each scenario so that they can be evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time \n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "project_path = os.path.dirname(os.getcwd())\n",
    "sys.path.append(project_path)\n",
    "\n",
    "from src.preprocess import  extract_text_from_pdf, get_sentences, get_semantic_chunks, get_sequential_semantic_chunks\n",
    "from src.rag import RAG\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.environ['GOOGLE_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth data\n",
    "\n",
    "for the ground truth data we will choose at random 5 documents, chunks them and then generate questions for each chunk. This means a ground truth set is to be generated for each chunking strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_categories = os.listdir(os.path.join(project_path, 'docs'))\n",
    "papers = []\n",
    "index = 0\n",
    "for category in doc_categories:\n",
    "    for document in os.listdir(os.path.join(project_path, 'docs', category)):\n",
    "        index += 1\n",
    "        papers.append({\n",
    "            \"index\": index,\n",
    "            \"category\":category,\n",
    "            \"paper\":document\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 1,\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'an_overview_of_gradient_descent_optimization_algorithms.pdf'},\n",
       " {'index': 2,\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'attention_is_all_you_need.pdf'},\n",
       " {'index': 3,\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'dense_x_retieval_what_retrieval_granularity_shoud_we_use.pdf'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "sample = random.sample(papers, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 2,\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'attention_is_all_you_need.pdf'},\n",
       " {'index': 9,\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'the_matrix_calculus_you_need_for_deeplearning.pdf'},\n",
       " {'index': 3,\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'dense_x_retieval_what_retrieval_granularity_shoud_we_use.pdf'},\n",
       " {'index': 14,\n",
       "  'category': 'time_series',\n",
       "  'paper': 'another_lookat_measures_of_forecast_accuracy.pdf'},\n",
       " {'index': 4,\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the chunked data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating chunks\n",
    "chunks = []\n",
    "for doc in tqdm(sample):\n",
    "    index = doc.get('index')\n",
    "    category = doc.get('category')\n",
    "    paper = doc.get('paper')\n",
    "    \n",
    "    doc_id = hashlib.sha256(\n",
    "        f'{category}-{paper}-{index}'.encode('utf-8')\n",
    "    ).hexdigest()\n",
    "    \n",
    "    pdf_path = os.path.join(\n",
    "        project_path, 'docs', category, paper\n",
    "    )\n",
    "    \n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    doc_chunks = get_sentences(text, doc_id)\n",
    "    \n",
    "    for doc_chunk in doc_chunks:                \n",
    "        chunks.append({\n",
    "            'id': f'{doc_id}-{doc_chunk['chunk']}',\n",
    "            'category':category,\n",
    "            'paper': paper,\n",
    "            'text': doc_chunk['text']\n",
    "        })\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_splitting = pd.DataFrame(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>paper</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>The best\\nperforming models also connect the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>We propose a new simple network architecture, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>Experiments on two machine translation tasks s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>Our model achieves 28.4 BLEU on the WMT 2014 E...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id      category  \\\n",
       "0  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "1  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "2  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "3  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "4  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "\n",
       "                           paper  \\\n",
       "0  attention_is_all_you_need.pdf   \n",
       "1  attention_is_all_you_need.pdf   \n",
       "2  attention_is_all_you_need.pdf   \n",
       "3  attention_is_all_you_need.pdf   \n",
       "4  attention_is_all_you_need.pdf   \n",
       "\n",
       "                                                text  \n",
       "0  Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...  \n",
       "1  The best\\nperforming models also connect the e...  \n",
       "2  We propose a new simple network architecture, ...  \n",
       "3  Experiments on two machine translation tasks s...  \n",
       "4  Our model achieves 28.4 BLEU on the WMT 2014 E...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentence_splitting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_splitting.to_csv(\n",
    "    os.path.join(project_path, 'data', 'testing', 'sentence_splitting.csv'),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      " 20%|██        | 1/5 [01:02<04:09, 62.37s/it]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      " 40%|████      | 2/5 [02:52<04:31, 90.51s/it]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      " 60%|██████    | 3/5 [04:18<02:56, 88.45s/it]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      " 80%|████████  | 4/5 [05:14<01:15, 75.60s/it]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 5/5 [07:12<00:00, 86.58s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>paper</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>We used a beam size of 21and\u000b= 0:3\\nfor both W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>The Transformer allows for signiﬁcantly more p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>[4]Jianpeng Cheng, Li Dong, and Mirella Lapata.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>At each step the model is auto-regressive\\n[10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id      category  \\\n",
       "0  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "1  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "2  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "3  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "4  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "\n",
       "                           paper  \\\n",
       "0  attention_is_all_you_need.pdf   \n",
       "1  attention_is_all_you_need.pdf   \n",
       "2  attention_is_all_you_need.pdf   \n",
       "3  attention_is_all_you_need.pdf   \n",
       "4  attention_is_all_you_need.pdf   \n",
       "\n",
       "                                                text  \n",
       "0  We used a beam size of 21and\n",
       "= 0:3\\nfor both W...  \n",
       "1  The Transformer allows for signiﬁcantly more p...  \n",
       "2    [4]Jianpeng Cheng, Li Dong, and Mirella Lapata.  \n",
       "3  Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...  \n",
       "4  At each step the model is auto-regressive\\n[10...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating chunks\n",
    "chunks = []\n",
    "for doc in tqdm(sample):\n",
    "    index = doc.get('index')\n",
    "    category = doc.get('category')\n",
    "    paper = doc.get('paper')\n",
    "    \n",
    "    doc_id = hashlib.sha256(\n",
    "        f'{category}-{paper}-{index}'.encode('utf-8')\n",
    "    ).hexdigest()\n",
    "    \n",
    "    pdf_path = os.path.join(\n",
    "        project_path, 'docs', category, paper\n",
    "    )\n",
    "    \n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    doc_chunks = get_semantic_chunks(text, doc_id)\n",
    "    \n",
    "    for doc_chunk in doc_chunks:                \n",
    "        chunks.append({\n",
    "            'id': f'{doc_id}-{doc_chunk['chunk']}',\n",
    "            'category':category,\n",
    "            'paper': paper,\n",
    "            'text': doc_chunk['text']\n",
    "        })\n",
    "\n",
    "df_semantic_chunking = pd.DataFrame(chunks)\n",
    "df_semantic_chunking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_semantic_chunking.to_csv(\n",
    "    os.path.join(project_path, 'data', 'testing', 'semantic_chunking.csv'),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      " 20%|██        | 1/5 [16:17<1:05:08, 977.23s/it]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      " 40%|████      | 2/5 [45:39<1:11:57, 1439.02s/it]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      " 60%|██████    | 3/5 [1:12:52<50:55, 1527.75s/it]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      " 80%|████████  | 4/5 [1:26:58<20:58, 1258.54s/it]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 5/5 [2:11:10<00:00, 1574.14s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>paper</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>This\\nconsists of two linear transformations w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>We\\ntrained the base models for a total of 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>We\\nused beam search with a beam size of 4and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>While single-head\\nattention is 0.9 BLEU worse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id      category  \\\n",
       "0  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "1  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "2  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "3  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "4  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "\n",
       "                           paper  \\\n",
       "0  attention_is_all_you_need.pdf   \n",
       "1  attention_is_all_you_need.pdf   \n",
       "2  attention_is_all_you_need.pdf   \n",
       "3  attention_is_all_you_need.pdf   \n",
       "4  attention_is_all_you_need.pdf   \n",
       "\n",
       "                                                text  \n",
       "0  Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...  \n",
       "1  This\\nconsists of two linear transformations w...  \n",
       "2  We\\ntrained the base models for a total of 100...  \n",
       "3  We\\nused beam search with a beam size of 4and ...  \n",
       "4  While single-head\\nattention is 0.9 BLEU worse...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating chunks\n",
    "chunks = []\n",
    "for doc in tqdm(sample):\n",
    "    index = doc.get('index')\n",
    "    category = doc.get('category')\n",
    "    paper = doc.get('paper')\n",
    "    \n",
    "    doc_id = hashlib.sha256(\n",
    "        f'{category}-{paper}-{index}'.encode('utf-8')\n",
    "    ).hexdigest()\n",
    "    \n",
    "    pdf_path = os.path.join(\n",
    "        project_path, 'docs', category, paper\n",
    "    )\n",
    "    \n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    doc_chunks = get_sequential_semantic_chunks(text, doc_id)\n",
    "    \n",
    "    for doc_chunk in doc_chunks:                \n",
    "        chunks.append({\n",
    "            'id': f'{doc_id}-{doc_chunk['chunk']}',\n",
    "            'category':category,\n",
    "            'paper': paper,\n",
    "            'text': doc_chunk['text']\n",
    "        })\n",
    "\n",
    "df_sequential_semantic_chunking = pd.DataFrame(chunks)\n",
    "df_sequential_semantic_chunking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sequential_semantic_chunking.to_csv(\n",
    "    os.path.join(project_path, 'data', 'testing', 'sequential_semantic_chunking.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth datasets generation using an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You emulate a user of our academic assistant application, designed to aid college students, master students, and researchers.\n",
    "\n",
    "Formulate 5 insightful questions this user might ask based on a provided text chunk from an academic paper.\n",
    "Make the questions specific to this chunk and its context within the paper.\n",
    "Ensure the questions probe deeper into the subject matter, prompting critical thinking and analysis.\n",
    "Be complete and not too short. \n",
    "Use as fewer words as possible from the record. \n",
    "\n",
    "The record:\n",
    "\n",
    "category: {category}\n",
    "paper: {paper}\n",
    "text: {text}\n",
    "\n",
    "Provide the output in parsable JSON without using code blocks.\n",
    "The response must not include any json reference (like ```json).\n",
    "The code must be parsable as a python dictionary.\n",
    "Ensure that all special characters are correctly escaped using backslashes.\n",
    "The response must have the followinf format, without code references at the beginning:\n",
    "\n",
    "{{\"questions\": [\"question1\", \"question2\", ..., \"question5\"]}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAG(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the number of samples generated from the smallest dataset. In this case, the sequential semantic chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALLEST_DATASET_SIZE=154\n",
    "\n",
    "import random\n",
    "\n",
    "def subsample_dictionaries(list_of_dicts, sample_size):\n",
    "  \"\"\"\n",
    "  Randomly subsamples a specified number of dictionaries from a list.\n",
    "\n",
    "  Args:\n",
    "      list_of_dicts: The list of dictionaries to sample from.\n",
    "      sample_size: The desired number of dictionaries in the subsample.\n",
    "\n",
    "  Returns:\n",
    "      A new list containing the randomly selected dictionaries.\n",
    "  \"\"\"\n",
    "\n",
    "  if sample_size > len(list_of_dicts):\n",
    "    raise ValueError(\"Sample size cannot exceed the size of the list.\")\n",
    "\n",
    "  return random.sample(list_of_dicts, sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth for sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data set contains: 2929 sentences\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>paper</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>The best\\nperforming models also connect the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>We propose a new simple network architecture, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>Experiments on two machine translation tasks s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>Our model achieves 28.4 BLEU on the WMT 2014 E...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id      category  \\\n",
       "0  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "1  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "2  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "3  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "4  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "\n",
       "                           paper  \\\n",
       "0  attention_is_all_you_need.pdf   \n",
       "1  attention_is_all_you_need.pdf   \n",
       "2  attention_is_all_you_need.pdf   \n",
       "3  attention_is_all_you_need.pdf   \n",
       "4  attention_is_all_you_need.pdf   \n",
       "\n",
       "                                                text  \n",
       "0  Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...  \n",
       "1  The best\\nperforming models also connect the e...  \n",
       "2  We propose a new simple network architecture, ...  \n",
       "3  Experiments on two machine translation tasks s...  \n",
       "4  Our model achieves 28.4 BLEU on the WMT 2014 E...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentence_splitting = pd.read_csv(\n",
    "    os.path.join(project_path, 'data', 'testing', 'sentence_splitting.csv')\n",
    ")\n",
    "print(f\"The data set contains: {df_sentence_splitting.shape[0]} sentences\")\n",
    "df_sentence_splitting.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-1',\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'attention_is_all_you_need.pdf',\n",
       "  'text': 'Attention Is All You Need\\nAshish Vaswani\\x03\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\x03\\nGoogle Brain\\nnoam@google.comNiki Parmar\\x03\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\x03\\nGoogle Research\\nusz@google.com\\nLlion Jones\\x03\\nGoogle Research\\nllion@google.comAidan N. Gomez\\x03y\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser\\x03\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\x03z\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder.'},\n",
       " {'id': 'e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-2',\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'attention_is_all_you_need.pdf',\n",
       "  'text': 'The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism.'},\n",
       " {'id': 'e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-3',\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'attention_is_all_you_need.pdf',\n",
       "  'text': 'We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentece_splitting_docs = df_sentence_splitting.to_dict(orient=\"records\")\n",
    "sentece_splitting_docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a similar ammont of data for the three cases, therefore we will subsample 154 docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled number of senteces: 154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-157',\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'attention_is_all_you_need.pdf',\n",
       "  'text': 'On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0,\\noutperforming all of the previously published single models, at less than 1=4the training cost of the\\nprevious state-of-the-art model.'},\n",
       " {'id': '95c543b8fe00c9b1adbed90ddbc9e22abacf34b8ecc804f58850ed67a54b4cc7-250',\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'dense_x_retieval_what_retrieval_granularity_shoud_we_use.pdf',\n",
       "  'text': 'Sub-sentence en-\\ncoder: Contrastive learning of propositional semantic\\nrepresentations.'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_sampled_docs = subsample_dictionaries(sentece_splitting_docs, SMALLEST_DATASET_SIZE)\n",
    "print(f\"Sampled number of senteces: {len(ss_sampled_docs)}\")\n",
    "ss_sampled_docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'questions': ['What specific architectural or training techniques enabled the model to achieve such a significant BLEU score improvement compared to previous single models?',\n",
       "  'How does the reduced training cost of this model compare to the cost of ensemble models, and what are the trade-offs involved in choosing one over the other?',\n",
       "  \"What are the limitations of the model's performance on the WMT 2014 English-to-French translation task, and how might these be addressed in future research?\",\n",
       "  \"How does the model's performance on this specific task translate to its potential for other language pairs or different NLP tasks?\",\n",
       "  \"Considering the model's efficiency and performance, what are the potential implications for the development and deployment of large-scale language models in real-world applications?\"]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.format(**ss_sampled_docs[0])\n",
    "\n",
    "questions = rag.llm(prompt)\n",
    "\n",
    "json.loads(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(doc:dict) -> dict:    \n",
    "    \"\"\"Function that uses an LLM to generate questions about\n",
    "    a record from our documents.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of questions\n",
    "    \"\"\"\n",
    "    prompt = prompt_template.format(**doc)\n",
    "    response = rag.llm(prompt)\n",
    "    return json.loads(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'questions': ['What specific architectural or training techniques enabled the model to achieve such a significant BLEU score improvement compared to previous single models?',\n",
       "  'How does the reduced training cost of this model compare to the cost of ensemble models, and what are the trade-offs involved in choosing one over the other?',\n",
       "  \"What are the limitations of the model's performance on the WMT 2014 English-to-French translation task, and how might these be addressed in future research?\",\n",
       "  \"How does the model's performance on this specific task translate to its potential for other language pairs or different NLP tasks?\",\n",
       "  \"Considering the model's efficiency and performance, what are the potential implications for the development and deployment of large-scale language models in real-world applications?\"]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_questions(ss_sampled_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [14:20<00:00,  5.59s/it]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(ss_sampled_docs):\n",
    "    doc_id = doc['id']\n",
    "    if doc_id in ss_results:\n",
    "        continue\n",
    "    questions = generate_questions(doc)\n",
    "    ss_results[doc_id] = questions['questions']\n",
    "    time.sleep(5) # Introducing sys sleep to avoid over using the resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_final_results = []\n",
    "\n",
    "for doc_id, questions in ss_results.items():\n",
    "    for q in questions:\n",
    "        ss_final_results.append((doc_id, q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-157',\n",
       " 'What specific architectural or training techniques enabled the model to achieve such a significant BLEU score improvement compared to previous single models?')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_final_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ss_results = pd.DataFrame(ss_final_results, columns=['id', 'question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ss_results.to_csv(\n",
    "    os.path.join(project_path, 'data', 'testing', 'ground-truth-sentence-splitting.csv'),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth for semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data set contains: 269 sentences\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>paper</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>We used a beam size of 21and\u000b= 0:3\\nfor both W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>The Transformer allows for signiﬁcantly more p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>[4]Jianpeng Cheng, Li Dong, and Mirella Lapata.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>At each step the model is auto-regressive\\n[10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id      category  \\\n",
       "0  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "1  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "2  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "3  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "4  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "\n",
       "                           paper  \\\n",
       "0  attention_is_all_you_need.pdf   \n",
       "1  attention_is_all_you_need.pdf   \n",
       "2  attention_is_all_you_need.pdf   \n",
       "3  attention_is_all_you_need.pdf   \n",
       "4  attention_is_all_you_need.pdf   \n",
       "\n",
       "                                                text  \n",
       "0  We used a beam size of 21and\n",
       "= 0:3\\nfor both W...  \n",
       "1  The Transformer allows for signiﬁcantly more p...  \n",
       "2    [4]Jianpeng Cheng, Li Dong, and Mirella Lapata.  \n",
       "3  Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...  \n",
       "4  At each step the model is auto-regressive\\n[10...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_semantic_chunking = pd.read_csv(\n",
    "    os.path.join(project_path, 'data', 'testing', 'semantic_chunking.csv')\n",
    ")\n",
    "print(f\"The data set contains: {df_semantic_chunking.shape[0]} sentences\")\n",
    "df_semantic_chunking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled number of senteces: 154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-31',\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'attention_is_all_you_need.pdf',\n",
       "  'text': 'CoRR , abs/1409.0473, 2014.\\nCoRR , abs/1703.03906, 2017.\\nCoRR , abs/1406.1078, 2014.\\nCoRR , abs/1412.3555, 2014.\\nCurran Associates,\\nInc., 2015.\\nCoRR , abs/1512.00567, 2015.\\nCoRR , abs/1606.04199, 2016.'},\n",
       " {'id': 'baf07af8cb279975638f4875627751cd1ad7639ee446444a5496c3be708f5a12-45',\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf',\n",
       "  'text': 'In Proceedings of the\\n2021 Conference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies , pp.\\nIn Proceedings of the 2022 Conference of\\nthe North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies , pp.\\nIn Proceedings of the 2021\\nConference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies , pp.\\nIn Proceedings of the\\n2022 Conference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies , pp.\\nIn Proceedings of the 2022 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies , pp.\\nIn Proceedings of the 2021 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies , pp.\\nIn Proceedings of the 2022 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Human Language Technologies , pp.\\nHuggingface’s transformers:\\nState-of-the-art natural language processing.\\nTransformers: State-of-the-art\\nnatural language processing.\\n(2023a) to construct textual\\ncorpora and use them as training data.'}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_chunking_docs = df_semantic_chunking.to_dict(orient=\"records\")\n",
    "sc_sampled_docs = subsample_dictionaries(semantic_chunking_docs, SMALLEST_DATASET_SIZE)\n",
    "print(f\"Sampled number of senteces: {len(sc_sampled_docs)}\")\n",
    "sc_sampled_docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [04:47<00:00,  1.87s/it]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(sc_sampled_docs):\n",
    "    doc_id = doc['id']\n",
    "    if doc_id in sc_results:\n",
    "        continue\n",
    "    questions = generate_questions(doc)\n",
    "    sc_results[doc_id] = questions['questions']\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_final_results = []\n",
    "\n",
    "for doc_id, questions in sc_results.items():\n",
    "    for q in questions:\n",
    "        sc_final_results.append((doc_id, q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc_results = pd.DataFrame(sc_final_results, columns=['id', 'question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc_results.to_csv(\n",
    "    os.path.join(project_path, 'data', 'testing', 'ground-truth-semantic-chunking.csv'),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groud truth for sequential semantic chunkng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data set contains: 154 sentences\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>paper</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>This\\nconsists of two linear transformations w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>We\\ntrained the base models for a total of 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>We\\nused beam search with a beam size of 4and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>While single-head\\nattention is 0.9 BLEU worse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                 id  \\\n",
       "0           0  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...   \n",
       "1           1  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...   \n",
       "2           2  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...   \n",
       "3           3  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...   \n",
       "4           4  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...   \n",
       "\n",
       "       category                          paper  \\\n",
       "0  deeplearning  attention_is_all_you_need.pdf   \n",
       "1  deeplearning  attention_is_all_you_need.pdf   \n",
       "2  deeplearning  attention_is_all_you_need.pdf   \n",
       "3  deeplearning  attention_is_all_you_need.pdf   \n",
       "4  deeplearning  attention_is_all_you_need.pdf   \n",
       "\n",
       "                                                text  \n",
       "0  Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...  \n",
       "1  This\\nconsists of two linear transformations w...  \n",
       "2  We\\ntrained the base models for a total of 100...  \n",
       "3  We\\nused beam search with a beam size of 4and ...  \n",
       "4  While single-head\\nattention is 0.9 BLEU worse...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sequential_semantic_chunking = pd.read_csv(\n",
    "    os.path.join(project_path, 'data', 'testing', 'sequential_semantic_chunking.csv')\n",
    ")\n",
    "print(f\"The data set contains: {df_sequential_semantic_chunking.shape[0]} sentences\")\n",
    "df_sequential_semantic_chunking.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I forgot not to use the index when writing thefile so I am droping it next since running this chunking is quite slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sequential_semantic_chunking.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-1',\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'attention_is_all_you_need.pdf',\n",
       "  'text': 'Attention Is All You Need\\nAshish Vaswani\\x03\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\x03\\nGoogle Brain\\nnoam@google.comNiki Parmar\\x03\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\x03\\nGoogle Research\\nusz@google.com\\nLlion Jones\\x03\\nGoogle Research\\nllion@google.comAidan N. Gomez\\x03y\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser\\x03\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\x03z\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder.\\nThe best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism.\\nWe propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely.\\nExperiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train.\\nOur model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU.\\nOn the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\nWe show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\n\\x03Equal contribution.\\nListing order is random.\\nJakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea.\\nAshish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work.\\nNoam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail.\\nNiki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor.\\nLlion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations.\\nLukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\nyWork performed while at Google Brain.\\nzWork performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v5  [cs.CL]  6 Dec 2017transduction problems such as language modeling and machine translation [ 35,2,5].\\nNumerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences.\\nAligning the positions to steps in computation time, they generate a sequence of hidden\\nstatesht, as a function of the previous hidden state ht'},\n",
       " {'id': 'e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-2',\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'attention_is_all_you_need.pdf',\n",
       "  'text': 'This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0;xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer.\\nAnother way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel.\\nWe also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities.\\nIn\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30].\\nIn the embedding layers, we multiply those weights bypdmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\n5Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types.\\nnis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2\\x01d) O(1) O(1)\\nRecurrent O(n\\x01d2) O(n) O(n)\\nConvolutional O(k\\x01n\\x01d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r\\x01n\\x01d)O(1) O(n=r)\\ntokens in the sequence.\\nTo this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks.\\nThe positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed.\\nThere are many choices of positional encodings,\\nlearned and ﬁxed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos;2i)=sin(pos=100002i=d model)\\nPE(pos;2i+1)=cos(pos=100002i=d model)\\nwhereposis the position and iis the dimension.\\nThat is, each dimension of the positional encoding\\ncorresponds to a sinusoid.\\nThe wavelengths form a geometric progression from 2\\x19to10000\\x012\\x19.\\nWe\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)).\\nWe chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder.\\nMotivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer.\\nAnother is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network.\\nLearning long-range\\ndependencies is a key challenge in many sequence transduction tasks.\\nOne key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network.\\nThe shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 12].\\nHence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations.\\nIn terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations.\\nTo improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6the input sequence centered around the respective output position.\\nThis would increase the maximum\\npath length to O(n=r).\\nWe plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions.\\nDoing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network.\\nConvolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k\\x01n\\x01d+n\\x01d2).\\nEven with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models.\\nWe inspect attention distributions\\nfrom our models and present and discuss examples in the appendix.\\nNot only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs.\\nSentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens.\\nFor English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38].\\nSentence pairs were batched together by approximate sequence length.\\nEach training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs.\\nFor our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds.'}]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssc_docs = df_sequential_semantic_chunking.to_dict(orient=\"records\")\n",
    "ssc_docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [03:05<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(ssc_docs):\n",
    "    doc_id = doc['id']\n",
    "    if doc_id in ssc_results:\n",
    "        continue\n",
    "    questions = generate_questions(doc)\n",
    "    ssc_results[doc_id] = questions['questions']\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc_final_results = []\n",
    "\n",
    "for doc_id, questions in ssc_results.items():\n",
    "    for q in questions:\n",
    "        ssc_final_results.append((doc_id, q))\n",
    "        \n",
    "df_ssc_results = pd.DataFrame(ssc_final_results, columns=['id', 'question'])\n",
    "df_ssc_results.to_csv(\n",
    "    os.path.join(\n",
    "        project_path, 'data', 'testing', 'ground-truth-sequential-semantic-chunking.csv'\n",
    "    ),\n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
