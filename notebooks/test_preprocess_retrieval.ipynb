{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessors testing\n",
    "\n",
    "As can be seen in the notebook \"create_preprocessors.ipynb\" we create three (since as mentioned proposal chunking won't be considered) different preprocessing strategies to split our documents into chunks:\n",
    "\n",
    "* Sentences splitting\n",
    "* Semantic chunking\n",
    "* Sequential semantic chunking\n",
    "\n",
    "On the other hand, we contemplate three retrieval scenarios, implemented directly in code (the expected volume of data for this projects is not big enough for a vector DB to be neccesary and the implementation is usefull for concepts understanding):\n",
    "\n",
    "* Word matching using TFid vectorizer (as seen in the course in the minsearch implementation)\n",
    "* Hybrid serach implementing the embeddings with sentence transformers.\n",
    "* Hybrid search with RRF.\n",
    "\n",
    "This notebooks compares the capability of each scenario to create a good retrieval strategy. Though verifying the end to end behavior by evaluating the RAG with each metodology would be more recommendable, given time restrictions we will only explore the performance of the retrieval part by cheking the hit rate and MMR using a ground_truth data base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time \n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "project_path = os.path.dirname(os.getcwd())\n",
    "sys.path.append(project_path)\n",
    "\n",
    "from src.preprocess import  extract_text_from_pdf, get_sentences, get_semantic_chunks, get_sequential_semantic_chunks\n",
    "from src.rag import RAG\n",
    "\n",
    "GOOGLE_API_KEY = os.environ['GOOGLE_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth data\n",
    "\n",
    "for the ground truth data we will choose at random 5 documents, chunks them and then generate questions for each chunk. This means a ground truth set is to be generated for each chunking strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_categories = os.listdir(os.path.join(project_path, 'docs'))\n",
    "papers = []\n",
    "index = 0\n",
    "for category in doc_categories:\n",
    "    for document in os.listdir(os.path.join(project_path, 'docs', category)):\n",
    "        index += 1\n",
    "        papers.append({\n",
    "            \"index\": index,\n",
    "            \"category\":category,\n",
    "            \"paper\":document\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 1,\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'an_overview_of_gradient_descent_optimization_algorithms.pdf'},\n",
       " {'index': 2,\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'attention_is_all_you_need.pdf'},\n",
       " {'index': 3,\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'dense_x_retieval_what_retrieval_granularity_shoud_we_use.pdf'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "sample = random.sample(papers, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 2,\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'attention_is_all_you_need.pdf'},\n",
       " {'index': 9,\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'the_matrix_calculus_you_need_for_deeplearning.pdf'},\n",
       " {'index': 3,\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'dense_x_retieval_what_retrieval_granularity_shoud_we_use.pdf'},\n",
       " {'index': 14,\n",
       "  'category': 'time_series',\n",
       "  'paper': 'another_lookat_measures_of_forecast_accuracy.pdf'},\n",
       " {'index': 4,\n",
       "  'category': 'deeplearning',\n",
       "  'paper': 'knowledge_card_filling_llms_knowledge_gaps_with_plug_in_specialied_language_models.pdf'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the chunked data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# Creating chunks\n",
    "chunks = []\n",
    "for doc in tqdm(sample):\n",
    "    index = doc.get('index')\n",
    "    category = doc.get('category')\n",
    "    paper = doc.get('paper')\n",
    "    \n",
    "    doc_id = hashlib.sha256(\n",
    "        f'{category}-{paper}-{index}'.encode('utf-8')\n",
    "    ).hexdigest()\n",
    "    \n",
    "    pdf_path = os.path.join(\n",
    "        project_path, 'docs', category, paper\n",
    "    )\n",
    "    \n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    doc_chunks = get_sentences(text, doc_id)\n",
    "    \n",
    "    for doc_chunk in doc_chunks:                \n",
    "        chunks.append({\n",
    "            'id': f'{doc_id}-{doc_chunk['chunk']}',\n",
    "            'category':category,\n",
    "            'paper': paper,\n",
    "            'text': doc_chunk['text']\n",
    "        })\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_splitting = pd.DataFrame(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>paper</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>The best\\nperforming models also connect the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>We propose a new simple network architecture, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>Experiments on two machine translation tasks s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>Our model achieves 28.4 BLEU on the WMT 2014 E...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id      category  \\\n",
       "0  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "1  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "2  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "3  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "4  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "\n",
       "                           paper  \\\n",
       "0  attention_is_all_you_need.pdf   \n",
       "1  attention_is_all_you_need.pdf   \n",
       "2  attention_is_all_you_need.pdf   \n",
       "3  attention_is_all_you_need.pdf   \n",
       "4  attention_is_all_you_need.pdf   \n",
       "\n",
       "                                                text  \n",
       "0  Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...  \n",
       "1  The best\\nperforming models also connect the e...  \n",
       "2  We propose a new simple network architecture, ...  \n",
       "3  Experiments on two machine translation tasks s...  \n",
       "4  Our model achieves 28.4 BLEU on the WMT 2014 E...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentence_splitting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_splitting.to_csv(\n",
    "    os.path.join(project_path, 'data', 'testing', 'sentence_splitting.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      " 20%|██        | 1/5 [01:03<04:15, 63.91s/it]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      " 40%|████      | 2/5 [03:03<04:49, 96.46s/it]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      " 60%|██████    | 3/5 [04:36<03:10, 95.09s/it]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      " 80%|████████  | 4/5 [05:35<01:20, 80.91s/it]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 5/5 [07:42<00:00, 92.42s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>paper</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>We used a beam size of 21and\u000b= 0:3\\nfor both W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>The Transformer allows for signiﬁcantly more p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>[4]Jianpeng Cheng, Li Dong, and Mirella Lapata.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>At each step the model is auto-regressive\\n[10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id      category  \\\n",
       "0  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "1  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "2  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "3  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "4  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "\n",
       "                           paper  \\\n",
       "0  attention_is_all_you_need.pdf   \n",
       "1  attention_is_all_you_need.pdf   \n",
       "2  attention_is_all_you_need.pdf   \n",
       "3  attention_is_all_you_need.pdf   \n",
       "4  attention_is_all_you_need.pdf   \n",
       "\n",
       "                                                text  \n",
       "0  We used a beam size of 21and\n",
       "= 0:3\\nfor both W...  \n",
       "1  The Transformer allows for signiﬁcantly more p...  \n",
       "2    [4]Jianpeng Cheng, Li Dong, and Mirella Lapata.  \n",
       "3  Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...  \n",
       "4  At each step the model is auto-regressive\\n[10...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating chunks\n",
    "chunks = []\n",
    "for doc in tqdm(sample):\n",
    "    index = doc.get('index')\n",
    "    category = doc.get('category')\n",
    "    paper = doc.get('paper')\n",
    "    \n",
    "    doc_id = hashlib.sha256(\n",
    "        f'{category}-{paper}-{index}'.encode('utf-8')\n",
    "    ).hexdigest()\n",
    "    \n",
    "    pdf_path = os.path.join(\n",
    "        project_path, 'docs', category, paper\n",
    "    )\n",
    "    \n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    doc_chunks = get_semantic_chunks(text, doc_id)\n",
    "    \n",
    "    for doc_chunk in doc_chunks:                \n",
    "        chunks.append({\n",
    "            'id': f'{doc_id}-{doc_chunk['chunk']}',\n",
    "            'category':category,\n",
    "            'paper': paper,\n",
    "            'text': doc_chunk['text']\n",
    "        })\n",
    "\n",
    "df_semantic_chunking = pd.DataFrame(chunks)\n",
    "df_semantic_chunking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_semantic_chunking.to_csv(\n",
    "    os.path.join(project_path, 'data', 'testing', 'semantic_chunking.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      " 20%|██        | 1/5 [16:17<1:05:08, 977.23s/it]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      " 40%|████      | 2/5 [45:39<1:11:57, 1439.02s/it]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      " 60%|██████    | 3/5 [1:12:52<50:55, 1527.75s/it]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      " 80%|████████  | 4/5 [1:26:58<20:58, 1258.54s/it]c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 5/5 [2:11:10<00:00, 1574.14s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>paper</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>This\\nconsists of two linear transformations w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>We\\ntrained the base models for a total of 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>We\\nused beam search with a beam size of 4and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...</td>\n",
       "      <td>deeplearning</td>\n",
       "      <td>attention_is_all_you_need.pdf</td>\n",
       "      <td>While single-head\\nattention is 0.9 BLEU worse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id      category  \\\n",
       "0  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "1  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "2  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "3  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "4  e1ccff07e5c99304d9674e3bb8b21a9f3ad63a70834970...  deeplearning   \n",
       "\n",
       "                           paper  \\\n",
       "0  attention_is_all_you_need.pdf   \n",
       "1  attention_is_all_you_need.pdf   \n",
       "2  attention_is_all_you_need.pdf   \n",
       "3  attention_is_all_you_need.pdf   \n",
       "4  attention_is_all_you_need.pdf   \n",
       "\n",
       "                                                text  \n",
       "0  Attention Is All You Need\\nAshish Vaswani\u0003\\nGo...  \n",
       "1  This\\nconsists of two linear transformations w...  \n",
       "2  We\\ntrained the base models for a total of 100...  \n",
       "3  We\\nused beam search with a beam size of 4and ...  \n",
       "4  While single-head\\nattention is 0.9 BLEU worse...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating chunks\n",
    "chunks = []\n",
    "for doc in tqdm(sample):\n",
    "    index = doc.get('index')\n",
    "    category = doc.get('category')\n",
    "    paper = doc.get('paper')\n",
    "    \n",
    "    doc_id = hashlib.sha256(\n",
    "        f'{category}-{paper}-{index}'.encode('utf-8')\n",
    "    ).hexdigest()\n",
    "    \n",
    "    pdf_path = os.path.join(\n",
    "        project_path, 'docs', category, paper\n",
    "    )\n",
    "    \n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    doc_chunks = get_sequential_semantic_chunks(text, doc_id)\n",
    "    \n",
    "    for doc_chunk in doc_chunks:                \n",
    "        chunks.append({\n",
    "            'id': f'{doc_id}-{doc_chunk['chunk']}',\n",
    "            'category':category,\n",
    "            'paper': paper,\n",
    "            'text': doc_chunk['text']\n",
    "        })\n",
    "\n",
    "df_sequential_semantic_chunking = pd.DataFrame(chunks)\n",
    "df_sequential_semantic_chunking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sequential_semantic_chunking.to_csv(\n",
    "    os.path.join(project_path, 'data', 'testing', 'sequential_semantic_chunking.csv')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
