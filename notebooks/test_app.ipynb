{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App testing\n",
    "\n",
    "This notebook will be used to check that the app is functioning addecuately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import psycopg2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "APP_PORT = os.getenv('APP_PORT')\n",
    "POSTGRES_HOST = 'localhost'\n",
    "POSTGRES_DB = os.getenv('POSTGRES_DB')\n",
    "POSTGRES_USER = os.getenv('POSTGRES_USER')\n",
    "POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5000'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APP_PORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_endpoint = f'http://127.0.0.1:{APP_PORT}/question'\n",
    "question_response = requests.post(\n",
    "    url = question_endpoint,\n",
    "    json={\n",
    "        \"question\": \"What is the key idea of the paper 'Attention is all You need'?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The key idea of the paper \"Attention is all You Need\" is that **attention mechanisms can be used to create a powerful and effective neural network architecture for machine translation without using recurrent neural networks (RNNs) or convolutional neural networks (CNNs)**. This architecture, called the Transformer, relies solely on attention to learn relationships between words in a sentence, allowing it to process information in parallel and achieve state-of-the-art results in machine translation.  (\"Attention is all You need\") \\n',\n",
       " 'conversation_id': 'd757cae4-3fa6-43ce-943f-d12ec97317e6',\n",
       " 'question': \"What is the key idea of the paper 'Attention is all You need'?\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key idea of the paper \"Attention is all You Need\" is that **attention mechanisms can be used to create a powerful and effective neural network architecture for machine translation without using recurrent neural networks (RNNs) or convolutional neural networks (CNNs)**. This architecture, called the Transformer, relies solely on attention to learn relationships between words in a sentence, allowing it to process information in parallel and achieve state-of-the-art results in machine translation.  (\"Attention is all You need\") \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(question_response.json().get(\"answer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the category filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer architecture is a neural network architecture that relies entirely on attention mechanisms, replacing recurrent layers commonly used in encoder-decoder architectures with multi-headed self-attention.  (Attention is all You need) This architecture allows for faster training compared to models based on recurrent or convolutional layers. (Attention is all You need) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question_endpoint = f'http://127.0.0.1:{APP_PORT}/question'\n",
    "question_response = requests.post(\n",
    "    url = question_endpoint,\n",
    "    json={\n",
    "        \"question\": \"What is the transformer architechture?\",\n",
    "        \"category\": \"deeplearning\"\n",
    "    }\n",
    ")\n",
    "print(question_response.json().get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided research papers do not contain information about the transformer architecture. Therefore, I cannot answer your question. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question_endpoint = f'http://127.0.0.1:{APP_PORT}/question'\n",
    "question_response = requests.post(\n",
    "    url = question_endpoint,\n",
    "    json={\n",
    "        \"question\": \"What is the transformer architechture?\",\n",
    "        \"category\": \"statistics\"\n",
    "    }\n",
    ")\n",
    "print(question_response.json().get(\"answer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_endpoint = f'http://127.0.0.1:{APP_PORT}/feedback'\n",
    "feedback_response = requests.post(\n",
    "    url = feedback_endpoint,\n",
    "    json={\n",
    "        \"conversation_id\":'8476eb9d-c91b-4e35-a01f-30b1b3e09349',\n",
    "        \"feedback\":1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mesagge': 'Feedback received for conversation 8476eb9d-c91b-4e35-a01f-30b1b3e09349:1'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the informatiuon saved in the postgres db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversations\n",
      "feedback\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(\n",
    "    dbname=POSTGRES_DB,  # Replace with your database name if different\n",
    "    user=POSTGRES_USER,  # Replace with your database user if different\n",
    "    password=POSTGRES_PASSWORD,  # Replace with your database password\n",
    "    host=POSTGRES_HOST,  # Replace with the container's IP or hostname if different\n",
    "    port=\"5432\"\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';\")\n",
    "tables = cursor.fetchall()\n",
    "for table in tables:\n",
    "    print(table[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6237f3ce-1159-45a9-98e5-a07cf46c4089\n",
      "8476eb9d-c91b-4e35-a01f-30b1b3e09349\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT * from conversations;\")\n",
    "tables = cursor.fetchall()\n",
    "for table in tables:\n",
    "    print(table[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('8476eb9d-c91b-4e35-a01f-30b1b3e09349',\n",
       " \"What is the key idea of the paper 'Attention is all You need'?\",\n",
       " 'The key idea of the paper \"Attention is all You Need\" is that **attention mechanisms can be used to create a powerful and effective neural network architecture for machine translation without using recurrent neural networks (RNNs) or convolutional neural networks (CNNs)**. This architecture, called the Transformer, relies solely on attention to learn relationships between words in a sentence, allowing it to process information in parallel and achieve state-of-the-art results in machine translation.  (\"Attention is all You need\") \\n',\n",
       " 'models/gemini-1.5-flash-latest',\n",
       " 3.825112819671631,\n",
       " 'RELEVANT',\n",
       " 'The answer accurately summarizes the key idea of the paper, which is the introduction of the Transformer architecture that relies solely on attention mechanisms for machine translation, eliminating the need for RNNs or CNNs.',\n",
       " 2311,\n",
       " 100,\n",
       " 2411,\n",
       " 289,\n",
       " 55,\n",
       " 344,\n",
       " 0.0002415,\n",
       " datetime.datetime(2024, 10, 7, 2, 30, 34, 488973, tzinfo=datetime.timezone.utc))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, '8476eb9d-c91b-4e35-a01f-30b1b3e09349', 1, datetime.datetime(2024, 10, 7, 2, 31, 21, 285579, tzinfo=datetime.timezone.utc))\n",
      "(2, '360064bf-f064-4f7a-b3b5-c703ca241280', 1, datetime.datetime(2024, 10, 7, 3, 45, 41, 885506, tzinfo=datetime.timezone.utc))\n",
      "(3, 'd2110685-368c-4653-9c65-9e8628a0159b', 1, datetime.datetime(2024, 10, 7, 3, 45, 41, 898037, tzinfo=datetime.timezone.utc))\n",
      "(4, '360064bf-f064-4f7a-b3b5-c703ca241280', 1, datetime.datetime(2024, 10, 7, 3, 45, 44, 2862, tzinfo=datetime.timezone.utc))\n",
      "(5, 'd2110685-368c-4653-9c65-9e8628a0159b', 1, datetime.datetime(2024, 10, 7, 3, 45, 44, 8367, tzinfo=datetime.timezone.utc))\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT * from feedback;\")\n",
    "tables = cursor.fetchall()\n",
    "for table in tables:\n",
    "    print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
