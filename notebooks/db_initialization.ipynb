{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB Initialization\n",
    "\n",
    "In this notebook we desgin the process to initialice the vector db. Since we will design this process with a future cloud deployment in mind we will start by uploading the files to cloud storge and the index files within a directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import hashlib\n",
    "import tempfile\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "project_path = os.path.dirname(os.getcwd())\n",
    "sys.path.append(project_path)\n",
    "\n",
    "from src.preprocess import  extract_text_from_pdf, get_sequential_semantic_chunks\n",
    "from src.storage import StorageManager\n",
    "from src.db import ElasticsearchManager\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "GCP_PROJECT_ID = os.getenv('GCP_PROJECT_ID')\n",
    "BUCKET = os.getenv('BUCKET')\n",
    "INDEX_NAME = os.getenv('INDEX_NAME')\n",
    "ELASTICSEARCH_HOST = os.getenv('ELASTICSEARCH_HOST')\n",
    "ELASTICSEARCH_PORT = os.getenv('ELASTICSEARCH_PORT')\n",
    "EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL')\n",
    "TEXT_FIELDS = os.getenv('TEXT_FIELDS').split(',')\n",
    "KEYWORD_FIELDS = os.getenv('KEYWORD_FIELDS').split(',')\n",
    "WORKERS = int(os.getenv('WORKERS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by uplading the docs to the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\google\\auth\\_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "storage_manager = StorageManager(GCP_PROJECT_ID, BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_path = os.path.join(project_path, 'docs')\n",
    "doc_categories = os.listdir(docs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in doc_categories:\n",
    "    category_path = os.path.join(docs_path, category)\n",
    "    storage_manager.upload_dir(category_path, f'docs/{category}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's clear it and build a function to update the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_manager.delete_dir(\"docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_documents(docs_path:str, storage_manager:StorageManager) -> None:\n",
    "    \"\"\"Uploads documents to GCP bucket.\n",
    "\n",
    "    Args:\n",
    "        docs_path (str): Path to documents.\n",
    "        storage_manager (StorageManager): Storage manager object.\n",
    "    \"\"\"\n",
    "    doc_categories = os.listdir(docs_path)\n",
    "    for category in doc_categories:\n",
    "        category_path = os.path.join(docs_path, category)\n",
    "        storage_manager.upload_dir(category_path, f'docs/{category}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_documents(docs_path, storage_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lindex our documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticsearch_manager = ElasticsearchManager(ELASTICSEARCH_HOST, ELASTICSEARCH_PORT, EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index documentor_project created.\n"
     ]
    }
   ],
   "source": [
    "# We create an index first\n",
    "elasticsearch_manager.create_index(\n",
    "    index_name=INDEX_NAME,\n",
    "    text_fields=TEXT_FIELDS,\n",
    "    keyword_fields=KEYWORD_FIELDS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_document_from_blob(\n",
    "    blob, \n",
    "    doc_index:int, \n",
    "    elasticsearch_manager:ElasticsearchManager,\n",
    "    index_name:str\n",
    ") -> None:\n",
    "    \"\"\"Indexes a document from a GCP bucket blob.\n",
    "\n",
    "    Args:\n",
    "        blob (Blob): GCP bucket blob.\n",
    "        doc_id (str): Document ID.\n",
    "        elasticsearch_manager (ElasticsearchManager): Elasticsearch manager object.\n",
    "    \"\"\"\n",
    "    category, paper = blob.name.split('/')[1:]\n",
    "    \n",
    "    doc_id = hashlib.sha256(\n",
    "        f'{category}-{paper}-{doc_index}'.encode('utf-8')\n",
    "    ).hexdigest()\n",
    "    \n",
    "    pdf_path = os.path.join(tempfile.gettempdir(), 'paper.pdf')\n",
    "    blob.download_to_filename(pdf_path)\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "    doc_chunks = get_sequential_semantic_chunks(pdf_text,doc_id,WORKERS)\n",
    "    \n",
    "    docs = []\n",
    "    \n",
    "    for doc_chunk in doc_chunks:                \n",
    "        docs.append({\n",
    "            'id': f'{doc_id}-{doc_chunk['chunk']}',\n",
    "            'category':category,\n",
    "            'paper': paper,\n",
    "            'text': doc_chunk['text']\n",
    "        })\n",
    "        \n",
    "    elasticsearch_manager.index_documents(\n",
    "        docs=docs,\n",
    "        index_name=index_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "for index, blob in tqdm(enumerate(storage_manager.bucket.list_blobs(prefix='docs'))):\n",
    "    index_document_from_blob(\n",
    "        blob=blob,\n",
    "        doc_index=index+1,\n",
    "        elasticsearch_manager=elasticsearch_manager,\n",
    "        index_name=INDEX_NAME\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es_client = Elasticsearch(f\"http://{ELASTICSEARCH_HOST}:{ELASTICSEARCH_PORT}\")                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\Cursos\\llm_zoomcamp_final_project\\llm-project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "emb_model = SentenceTransformer(EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paper': 'metra_scalable_unsupervised_rl_with_metric_aware_abstraction.pdf',\n",
       "  'id': 'bf9d5038e3b94a5e4df68bd3140008123eb773a52547f06b71520d78e2029474-22',\n",
       "  'text': 'Count-based explo-\\nration with neural density models.\\nIn International Conference on Machine Learning (ICML) ,\\n2017.\\nSherjil Ozair, Corey Lynch, Yoshua Bengio, A ¨aron van den Oord, Sergey Levine, and Pierre Ser-\\nmanet.\\nWasserstein dependency measure for representation learning.\\nIn Neural Information\\nProcessing Systems (NeurIPS) , 2019.\\nSeohong Park and Sergey Levine.\\nPredictable mdp abstraction for unsupervised model-based rl.\\nIn\\nInternational Conference on Machine Learning (ICML) , 2023.\\nSeohong Park, Jongwook Choi, Jaekyeom Kim, Honglak Lee, and Gunhee Kim.\\nLipschitz-\\nconstrained unsupervised skill discovery.\\nIn International Conference on Learning Represen-\\ntations (ICLR) , 2022.\\n13Published as a conference paper at ICLR 2024\\nSeohong Park, Dibya Ghosh, Benjamin Eysenbach, and Sergey Levine.\\nHiql: Offline goal-\\nconditioned rl with latent states as actions.\\nIn Neural Information Processing Systems (NeurIPS) ,\\n2023a.\\nSeohong Park, Kimin Lee, Youngwoon Lee, and P. Abbeel.\\nControllability-aware unsupervised skill\\ndiscovery.\\nIn International Conference on Machine Learning (ICML) , 2023b.\\nDeepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell.\\nCuriosity-driven exploration\\nby self-supervised prediction.\\nIn International Conference on Machine Learning (ICML) , 2017.\\nDeepak Pathak, Dhiraj Gandhi, and Abhinav Kumar Gupta.\\nSelf-supervised exploration via dis-\\nagreement.\\nIn International Conference on Machine Learning (ICML) , 2019.\\nSilviu Pitis, Harris Chan, S. Zhao, Bradly C. Stadie, and Jimmy Ba.\\nMaximum entropy gain ex-\\nploration for long horizon multi-goal reinforcement learning.\\nIn International Conference on\\nMachine Learning (ICML) , 2020.\\nVitchyr H. Pong, Murtaza Dalal, S. Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine.\\nSkew-Fit:\\nState-covering self-supervised reinforcement learning.\\nIn International Conference on Machine\\nLearning (ICML) , 2020.\\nBen Poole, Sherjil Ozair, A ¨aron van den Oord, Alexander A. Alemi, and G. Tucker.\\nOn variational\\nbounds of mutual information.\\nIn International Conference on Machine Learning (ICML) , 2019.\\nA. H. Qureshi, Jacob J. Johnson, Yuzhe Qin, Taylor Henderson, Byron Boots, and Michael C. Yip.\\nComposing task-agnostic policies with deep reinforcement learning.\\nIn International Conference\\non Learning Representations (ICLR) , 2020.\\nSai Rajeswar, Pietro Mazzaglia, Tim Verbelen, Alexandre Pich’e, B. Dhoedt, Aaron C. Courville,\\nand Alexandre Lacoste.\\nMastering the unsupervised reinforcement learning benchmark from\\npixels.\\nIn International Conference on Machine Learning (ICML) , 2023.\\nNick Rhinehart, Jenny Wang, Glen Berseth, John D. Co-Reyes, Danijar Hafner, Chelsea Finn, and\\nSergey Levine.\\nInformation is power: Intrinsic control via information capture.\\nIn Neural Infor-\\nmation Processing Systems (NeurIPS) , 2021.\\nNikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun.\\nSemi-parametric topological memory\\nfor navigation.\\nIn International Conference on Learning Representations (ICLR) , 2018.\\nTom Schaul, Dan Horgan, Karol Gregor, and David Silver.',\n",
       "  'category': 'deeplearning'},\n",
       " {'paper': 'principles_and_algorithms_for_forecasting_groups_of_time_series_locality_and_globality.pdf',\n",
       "  'id': '57bb624b729dbf61681207875a0fad68cd1f424c3d02160aa318ac53b624556d-30',\n",
       "  'text': 'In International Conference on Machine Learning , pages 1225–1234, 2016.\\n[13]Vitaly Kuznetsov and Mehryar Mohri.\\nForecasting non-stationary time series: From heory to algorithms.\\nTechnical report, 2016. https://cims.nyu.edu/~vitaly/pub/fts.pdf .\\n[14]Daniel J McDonald, Cosma Rohilla Shalizi, and Mark Schervish.\\nNonparametric risk bounds for time-series\\nforecasting.\\nThe Journal of Machine Learning Research , 18(1):1044–1083, 2017.\\n[15]Vitaly Kuznetsov and Mehryar Mohri.\\nGeneralization bounds for non-stationary mixing processes.\\nMachine\\nLearning , 106(1):93–117, 2017.\\n[16]Wassily Hoeﬀding.\\nProbability inequalities for sums of bounded random variables.',\n",
       "  'category': 'time_series'},\n",
       " {'paper': 'metra_scalable_unsupervised_rl_with_metric_aware_abstraction.pdf',\n",
       "  'id': 'bf9d5038e3b94a5e4df68bd3140008123eb773a52547f06b71520d78e2029474-23',\n",
       "  'text': 'Universal value function approximators.\\nInInternational Conference on Machine Learning (ICML) , 2015.\\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and P. Abbeel.\\nHigh-dimensional\\ncontinuous control using generalized advantage estimation.\\nIn International Conference on\\nLearning Representations (ICLR) , 2016.\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\\nProximal policy\\noptimization algorithms.\\nArXiv , abs/1707.06347, 2017.\\nRamanan Sekar, Oleh Rybkin, Kostas Daniilidis, P. Abbeel, Danijar Hafner, and Deepak Pathak.\\nPlanning to explore via self-supervised world models.\\nIn International Conference on Machine\\nLearning (ICML) , 2020.\\nYounggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, P. Abbeel, and Kimin Lee.\\nState entropy\\nmaximization with random encoders for efficient exploration.\\nIn International Conference on\\nMachine Learning (ICML) , 2021.\\nNur Muhammad (Mahi) Shafiullah and Lerrel Pinto.\\nOne after another: Learning incremental skills\\nfor a changing world.\\nIn International Conference on Learning Representations (ICLR) , 2022.\\nArchit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman.\\nDynamics-\\naware unsupervised discovery of skills.\\nIn International Conference on Learning Representations\\n(ICLR) , 2020.\\n14Published as a conference paper at ICLR 2024\\nPranav Shyam, Wojciech Ja ´skowski, and Faustino J. Gomez.\\nModel-based active exploration.\\nIn\\nInternational Conference on Machine Learning (ICML) , 2019.\\nDJ Strouse, Kate Baumli, David Warde-Farley, Vlad Mnih, and Steven Stenberg Hansen.\\nLearning\\nmore skills through optimistic exploration.\\nIn International Conference on Learning Representa-\\ntions (ICLR) , 2022.\\nSainbayar Sukhbaatar, Ilya Kostrikov, Arthur D. Szlam, and Rob Fergus.\\nIntrinsic motivation and\\nautomatic curricula via asymmetric self-play.\\nIn International Conference on Learning Represen-\\ntations (ICLR) , 2018.\\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,\\nFilip De Turck, and P. Abbeel.',\n",
       "  'category': 'deeplearning'},\n",
       " {'paper': 'metra_scalable_unsupervised_rl_with_metric_aware_abstraction.pdf',\n",
       "  'id': 'bf9d5038e3b94a5e4df68bd3140008123eb773a52547f06b71520d78e2029474-25',\n",
       "  'text': 'Representation learning with contrastive predic-\\ntive coding.\\nArXiv , abs/1807.03748, 2018.\\nC´edric Villani et al.\\nOptimal transport: old and new .\\nSpringer, 2009.\\nTongzhou Wang, Antonio Torralba, Phillip Isola, and Amy Zhang.\\nOptimal goal-reaching rein-\\nforcement learning via quasimetric learning.\\nIn International Conference on Machine Learning\\n(ICML) , 2023.\\nDavid Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and\\nV olodymyr Mnih.\\nUnsupervised control through non-parametric discriminative rewards.\\nIn In-\\nternational Conference on Learning Representations (ICLR) , 2019.\\nRushuai Yang, Chenjia Bai, Hongyi Guo, Siyuan Li, Bin Zhao, Zhen Wang, Peng Liu, and Xuelong\\nLi.\\nBehavior contrastive learning for unsupervised skill discovery.\\nIn International Conference\\non Machine Learning (ICML) , 2023.\\nDenis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto.\\nReinforcement learning with pro-\\ntotypical representations.\\nIn International Conference on Machine Learning (ICML) , 2021.\\nTom Zahavy, Yannick Schroecker, Feryal M. P. Behbahani, Kate Baumli, Sebastian Flennerhag,\\nShaobo Hou, and Satinder Singh.\\nDiscovering policies with domino: Diversity optimization\\nmaintaining near optimality.\\nIn International Conference on Learning Representations (ICLR) ,\\n2023a.\\nTom Zahavy, Vivek Veeriah, Shaobo Hou, Kevin Waugh, Matthew Lai, Edouard Leurent, Nenad\\nTomasev, Lisa Schut, Demis Hassabis, and Satinder Singh.\\nDiversifying ai: Towards creative\\nchess with alphazero.\\nArXiv , abs/2308.09175, 2023b.\\nJesse Zhang, Haonan Yu, and Wei Xu.',\n",
       "  'category': 'deeplearning'},\n",
       " {'paper': 'metra_scalable_unsupervised_rl_with_metric_aware_abstraction.pdf',\n",
       "  'id': 'bf9d5038e3b94a5e4df68bd3140008123eb773a52547f06b71520d78e2029474-14',\n",
       "  'text': 'OpenAI Gym.\\nArXiv , abs/1606.01540, 2016.\\nTom B.\\nBrown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\\nAriel Herbert-V oss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M.\\nZiegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\\nIlya Sutskever, and Dario Amodei.\\nLanguage models are few-shot learners.\\nIn Neural Informa-\\ntion Processing Systems (NeurIPS) , 2020.\\nYuri Burda, Harrison Edwards, Amos J. Storkey, and Oleg Klimov.\\nExploration by random network\\ndistillation.\\nIn International Conference on Learning Representations (ICLR) , 2019.\\nV´ıctor Campos Cam ´u˜nez, Alex Trott, Caiming Xiong, Richard Socher, Xavier Gir ´o Nieto, and Jordi\\nTorres Vi ˜nals.\\nExplore, discover and learn: unsupervised discovery of state-covering skills.\\nIn\\nInternational Conference on Machine Learning (ICML) , 2020.\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton.\\nA simple framework for\\ncontrastive learning of visual representations.\\nIn International Conference on Machine Learning\\n(ICML) , 2020.\\n10Published as a conference paper at ICLR 2024\\nWenze Chen, Shiyu Huang, Yuan Chiang, Tingling Chen, and Jun Zhu.\\nDgpo: Discovering multiple\\nstrategies with diversity-guided policy optimization.\\nIn AAAI Conference on Artificial Intelligence\\n(AAAI) , 2024.\\nXinyue Chen, Che Wang, Zijian Zhou, and Keith W. Ross.\\nRandomized ensembled double q-\\nlearning: Learning fast without a model.\\nIn International Conference on Learning Representa-\\ntions (ICLR) , 2021.\\nJongwook Choi, Archit Sharma, Honglak Lee, Sergey Levine, and Shixiang Gu.\\nVariational empow-\\nerment as representation learning for goal-conditioned reinforcement learning.\\nIn International\\nConference on Machine Learning (ICML) , 2021.\\nJohn D. Co-Reyes, Yuxuan Liu, Abhishek Gupta, Benjamin Eysenbach, P. Abbeel, and Sergey\\nLevine.\\nSelf-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajec-\\ntory embeddings.\\nIn International Conference on Machine Learning (ICML) , 2018.\\nIshan Durugkar, Mauricio Tec, Scott Niekum, and Peter Stone.\\nAdversarial intrinsic motivation for\\nreinforcement learning.\\nIn Neural Information Processing Systems (NeurIPS) , 2021.\\nAdrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, and Jeff Clune.',\n",
       "  'category': 'deeplearning'},\n",
       " {'paper': 'metra_scalable_unsupervised_rl_with_metric_aware_abstraction.pdf',\n",
       "  'id': 'bf9d5038e3b94a5e4df68bd3140008123eb773a52547f06b71520d78e2029474-19',\n",
       "  'text': 'Provably efficient maximum\\nentropy exploration.\\nIn International Conference on Machine Learning (ICML) , 2019.\\nShuncheng He, Yuhang Jiang, Hongchang Zhang, Jianzhun Shao, and Xiangyang Ji.\\nWasserstein\\nunsupervised reinforcement learning.\\nIn AAAI Conference on Artificial Intelligence (AAAI) , 2022.\\nTakuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, and Yoshimasa Tsuruoka.\\nDropout q-functions for doubly efficient reinforcement learning.\\nIn International Conference on\\nLearning Representations (ICLR) , 2022.\\nRein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and P. Abbeel.\\nVime: Varia-\\ntional information maximizing exploration.\\nIn Neural Information Processing Systems (NeurIPS) ,\\n2016.\\nEdward S. Hu, Richard Chang, Oleh Rybkin, and Dinesh Jayaraman.\\nPlanning goals for exploration.\\nInInternational Conference on Learning Representations (ICLR) , 2023.\\nZheyuan Jiang, Jingyue Gao, and Jianyu Chen.\\nUnsupervised skill discovery via recurrent skill\\ntraining.\\nIn Neural Information Processing Systems (NeurIPS) , 2022.\\nLeslie Pack Kaelbling.\\nLearning to achieve goals.\\nIn International Joint Conference on Artificial\\nIntelligence (IJCAI) , 1993.\\nPierre-Alexandre Kamienny, Jean Tarbouriech, Alessandro Lazaric, and Ludovic Denoyer.\\nDirect\\nthen diffuse: Incremental unsupervised skill discovery for state covering and goal reaching.\\nIn\\nInternational Conference on Learning Representations (ICLR) , 2022.\\nJaekyeom Kim, Seohong Park, and Gunhee Kim.\\nUnsupervised skill discovery with bottleneck\\noption learning.\\nIn International Conference on Machine Learning (ICML) , 2021.\\nSeongun Kim, Kyowoon Lee, and Jaesik Choi.\\nVariational curriculum reinforcement learning for\\nunsupervised discovery of skills.\\nIn International Conference on Machine Learning (ICML) ,\\n2023.\\nDiederik P. Kingma and Jimmy Ba.',\n",
       "  'category': 'deeplearning'},\n",
       " {'paper': 'attention_is_all_you_need.pdf',\n",
       "  'id': 'e1ccff07e5c99304d9674e3bb8b21a9f3ad63a708349704476b45c169163a8b4-8',\n",
       "  'text': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\\nLayer normalization.\\narXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\\nNeural machine translation by jointly\\nlearning to align and translate.\\nCoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V .\\nLe.\\nMassive exploration of neural\\nmachine translation architectures.\\nCoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata.\\nLong short-term memory-networks for machine\\nreading.\\narXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio.\\nLearning phrase representations using rnn encoder-decoder for statistical\\nmachine translation.\\nCoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet.\\nXception: Deep learning with depthwise separable convolutions.\\narXiv\\npreprint arXiv:1610.02357 , 2016.\\n10[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio.\\nEmpirical evaluation\\nof gated recurrent neural networks on sequence modeling.\\nCoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith.\\nRecurrent neural\\nnetwork grammars.\\nIn Proc.\\nof NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin.\\nConvolu-\\ntional sequence to sequence learning.\\narXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850 , 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for im-\\nage recognition.\\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber.',\n",
       "  'category': 'deeplearning'},\n",
       " {'paper': 'metra_scalable_unsupervised_rl_with_metric_aware_abstraction.pdf',\n",
       "  'id': 'bf9d5038e3b94a5e4df68bd3140008123eb773a52547f06b71520d78e2029474-20',\n",
       "  'text': 'Adam: A method for stochastic optimization.\\nIn International\\nConference on Learning Representations (ICLR) , 2015.\\nMartin Klissarov and Marlos C. Machado.\\nDeep laplacian-based options for temporally-extended\\nexploration.\\nIn International Conference on Machine Learning (ICML) , 2023.\\nIlya Kostrikov, Denis Yarats, and Rob Fergus.\\nImage augmentation is all you need: Regularizing\\ndeep reinforcement learning from pixels.\\nIn International Conference on Learning Representa-\\ntions (ICLR) , 2021.\\nSaurabh Kumar, Aviral Kumar, Sergey Levine, and Chelsea Finn.\\nOne solution is not all you\\nneed: Few-shot extrapolation via structured maxent rl.\\nIn Neural Information Processing Sys-\\ntems (NeurIPS) , 2020.\\nMichael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel\\nPinto, and P. Abbeel.\\nUrlb: Unsupervised reinforcement learning benchmark.\\nIn Neural Informa-\\ntion Processing Systems (NeurIPS) Datasets and Benchmarks Track , 2021.\\n12Published as a conference paper at ICLR 2024\\nMichael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, and P. Abbeel.\\nUn-\\nsupervised reinforcement learning with contrastive intrinsic control.\\nIn Neural Information Pro-\\ncessing Systems (NeurIPS) , 2022.\\nYann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne E.\\nHubbard, and Lawrence D. Jackel.\\nBackpropagation applied to handwritten zip code recognition.\\nNeural Computation , 1:541–551, 1989.\\nLisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric P. Xing, Sergey Levine, and Ruslan Salakhut-\\ndinov.\\nEfficient exploration via state marginal matching.\\nArXiv , abs/1906.05274, 2019.\\nMengdi Li, Xufeng Zhao, Jae Hee Lee, Cornelius Weber, and Stefan Wermter.\\nInternally rewarded\\nreinforcement learning.\\nIn International Conference on Machine Learning (ICML) , 2023.\\nHao Liu and Pieter Abbeel.\\nAPS: Active pretraining with successor features.\\nIn International\\nConference on Machine Learning (ICML) , 2021a.\\nHao Liu and Pieter Abbeel.\\nBehavior from the void: Unsupervised active pre-training.\\nIn Neural\\nInformation Processing Systems (NeurIPS) , 2021b.\\nMarlos C. Machado, Marc G. Bellemare, and Michael Bowling.\\nA laplacian framework for option\\ndiscovery in reinforcement learning.\\nIn International Conference on Machine Learning (ICML) ,\\n2017.\\nMarlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, and Murray\\nCampbell.\\nEigenoption discovery through the deep successor representation.\\nIn International\\nConference on Learning Representations (ICLR) , 2018.\\nPietro Mazzaglia, Ozan C ¸ atal, Tim Verbelen, and B. Dhoedt.\\nCuriosity-driven exploration via latent\\nbayesian surprise.\\nIn AAAI Conference on Artificial Intelligence (AAAI) , 2022.\\nPietro Mazzaglia, Tim Verbelen, B. Dhoedt, Alexandre Lacoste, and Sai Rajeswar.',\n",
       "  'category': 'deeplearning'},\n",
       " {'paper': 'an_overview_of_gradient_descent_optimization_algorithms.pdf',\n",
       "  'id': '25bce9725a38949a86c5c1f196bdd7e5ef9d13b610448f321a2d36913c7a88f9-9',\n",
       "  'text': 'TensorFlow: Large-Scale Machine\\nLearning on Heterogeneous Distributed Systems.\\n2015.\\n[2]Yoshua Bengio, Nicolas Boulanger-Lewandowski, and Razvan Pascanu.\\nAdvances in Optimiz-\\ning Recurrent Networks.\\n2012.\\n[3]Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston.\\nCurriculum learning.\\nProceedings of the 26th annual international conference on machine learning , pages 41–48,\\n2009.\\n[4]C. Darken, J. Chang, and J. Moody.\\nLearning rate schedules for faster stochastic gradient\\nsearch.\\nNeural Networks for Signal Processing II Proceedings of the 1992 IEEE Workshop ,\\n(September):1–11, 1992.\\n[5]Yann N. Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and\\nYoshua Bengio.\\nIdentifying and attacking the saddle point problem in high-dimensional non-\\nconvex optimization.\\narXiv , pages 1–14, 2014.',\n",
       "  'category': 'deeplearning'},\n",
       " {'paper': 'metra_scalable_unsupervised_rl_with_metric_aware_abstraction.pdf',\n",
       "  'id': 'bf9d5038e3b94a5e4df68bd3140008123eb773a52547f06b71520d78e2029474-17',\n",
       "  'text': 'Temporal difference learning for model predictive\\ncontrol.\\nIn International Conference on Machine Learning (ICML) , 2022.\\nS. Hansen, Will Dabney, Andr ´e Barreto, T. Wiele, David Warde-Farley, and V .',\n",
       "  'category': 'deeplearning'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elasticsearch_manager.hybrid_search(\n",
    "    index_name=INDEX_NAME,\n",
    "    query=\"machine learning\",\n",
    "    field_names=TEXT_FIELDS,\n",
    "    vector=emb_model.encode(\"machine learning\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
